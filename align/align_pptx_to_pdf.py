# ai/src/apps/asura/align/align_pptx_to_pdf.py
import argparse, pathlib, re
import numpy as np
import orjson
import hnswlib
from sentence_transformers import SentenceTransformer, CrossEncoder

NOISE_PATTERNS = [
    re.compile(r"Generated by Genspark", re.I),
]

def load_meta(meta_path: pathlib.Path):
    meta = {}
    with meta_path.open("rb") as f:
        for line in f:
            o = orjson.loads(line)
            meta[int(o["id"])] = o
    return meta

def is_noise_text(t: str) -> bool:
    s = t.strip()
    if not s:
        return True
    if s in ("[IMAGE]", "[BACKGROUND]", "[TABLE]"):
        return True
    if len(s) <= 2:
        return True
    if all(ch.isdigit() or ch in " .:/|-年月日" for ch in s):
        # ほぼ日付/数字だけ
        return True
    for p in NOISE_PATTERNS:
        if p.search(s):
            return True
    return False

def pptx_units(pptx_json: dict, drop_footer: bool = True):
    doc = pptx_json.get("document", {})
    page_wh = doc.get("page", {}) or {}
    H = page_wh.get("h_emu", None)

    for c in pptx_json.get("chunks", []):
        page = c.get("page")
        kind = c.get("kind")
        bbox = c.get("bbox") or {}
        y = bbox.get("y", None)
        h = bbox.get("h", None)

        # footer除外（yが下端近い）
        if drop_footer and H and y is not None and h is not None:
            if (y + h) > (H * 0.92):
                # 下端8%はフッター領域とみなす（雑に切る）
                # 必要なら後で閾値を引数化
                pass  # いったん落とさない（データ依存なので）
        if kind == "text":
            t = (c.get("text") or "").strip()
            if is_noise_text(t):
                continue
            yield {
                "unit_id": c.get("chunk_id"),
                "page": page,
                "kind": "text",
                "text": t,
            }

        elif kind == "table":
            tbl = c.get("table") or {}
            for cell in (tbl.get("cells") or []):
                t = (cell.get("text") or "").strip()
                if is_noise_text(t):
                    continue
                yield {
                    "unit_id": f"{c.get('chunk_id')}_r{cell.get('r')}_c{cell.get('c')}",
                    "page": page,
                    "kind": "table_cell",
                    "text": t,
                    "cell": {"r": cell.get("r"), "c": cell.get("c")},
                }

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--pdf_index_dir", required=True)
    ap.add_argument("--pptx_json", required=True)
    ap.add_argument("--out", required=True, help="jsonl")
    ap.add_argument("--embed_model", default="intfloat/multilingual-e5-base")
    ap.add_argument("--topk", type=int, default=10)
    ap.add_argument("--rerank", type=int, default=5)
    ap.add_argument("--rerank_model", default="hotchpotch/japanese-reranker-cross-encoder-small-v1")
    ap.add_argument("--drop_footer", action="store_true")
    args = ap.parse_args()

    idx_dir = pathlib.Path(args.pdf_index_dir)
    dim = int((idx_dir / "dim.txt").read_text().strip())
    meta = load_meta(idx_dir / "meta.jsonl")

    index = hnswlib.Index(space="cosine", dim=dim)
    index.load_index(str(idx_dir / "index.hnsw"))

    n = index.get_current_count()
    if n <= 0:
        raise RuntimeError("pdf index is empty")

    pptx = orjson.loads(pathlib.Path(args.pptx_json).read_bytes())

    emb_model = SentenceTransformer(args.embed_model)
    reranker = None
    if args.rerank > 0:
        reranker = CrossEncoder(args.rerank_model, max_length=512, device="cpu")

    out_path = pathlib.Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with out_path.open("wb") as f:
        for u in pptx_units(pptx, drop_footer=args.drop_footer):
            q = u["text"]
            qvec = emb_model.encode([f"query: {q}"], normalize_embeddings=True).astype(np.float32)

            k = min(args.topk, n)
            index.set_ef(max(64, k + 8))  # ef>=k
            ids, dists = index.knn_query(qvec, k=k)
            ids = ids[0].tolist()
            dists = dists[0].tolist()

            cand = []
            for i, dist in zip(ids, dists):
                m = meta[i]
                cand.append({
                    "pdf_id": int(i),
                    "pdf_chunk_id": m["chunk_id"],
                    "pdf_page_no": m.get("page_no"),
                    "pdf_text": m["text"],
                    "sim": float(1.0 - dist),
                })

            topN = cand[:min(args.rerank, len(cand))]
            if reranker and topN:
                scores = reranker.predict([[q, c["pdf_text"]] for c in topN])
                for c, s in zip(topN, scores):
                    c["rerank_score"] = float(s)
                topN = sorted(topN, key=lambda x: x.get("rerank_score", -1e9), reverse=True)
                rest = cand[len(topN):]
                cand = topN + rest

            rec = {
                "pptx_unit_id": u["unit_id"],
                "pptx_page": u["page"],
                "pptx_kind": u["kind"],
                "pptx_text": u["text"],
                "candidates": cand,
            }
            f.write(orjson.dumps(rec))
            f.write(b"\n")

    print(f"OK: wrote {out_path}")

if __name__ == "__main__":
    main()
<!DOCTYPE html><html><head><meta charset="utf-8"><title>Athra Debug: document</title><style>
body{font-family:sans-serif;background:#ecf0f1;margin:0;padding:16px}
h1{font-size:1.1em;color:#2c3e50;margin:0 0 4px}
.meta{font-size:11px;color:#7f8c8d;margin-bottom:12px}
.legend{margin-bottom:16px}
.legend span{display:inline-block;padding:2px 8px;margin:2px;border-radius:3px;
             font-size:11px;font-weight:bold;border:1px solid #999}
.page-block{margin-bottom:32px}
.page-block h2{font-size:.95em;color:#555;margin:0 0 4px}
.canvas{position:relative;background:white;border:1px solid #bbb;
        overflow:hidden;box-shadow:0 1px 4px rgba(0,0,0,.15)}
.bbox{position:absolute;border:2px solid;box-sizing:border-box;
      opacity:.65;cursor:default;transition:opacity .1s}
.bbox:hover{opacity:1;z-index:50}
.lbl{font-size:9px;font-weight:bold;padding:1px 4px;display:inline-block;white-space:nowrap}
</style></head><body><h1>Athra PDF Debug — document</h1><p class="meta">Source: input/テスト.pdf | Pages: 80 | Chunks: 322</p><div class="legend">Legend: <span style="background:#5dade2;">body</span><span style="background:#e74c3c;">h1</span><span style="background:#e67e22;">h2</span><span style="background:#f1c40f;">h3</span><span style="background:#27ae60;">header</span><span style="background:#8e44ad;">footer</span><span style="background:#1abc9c;">image</span><span style="background:#e91e63;">table</span><span style="background:#607d8b;">shape</span></div><div class="page-block"><h2>Page 1 (4 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:339.0px;top:447.9px;width:186.0px;height:10.1px;border-color:#5dade2;" title="#1 document_p001_c00001 [body]
2026 年 2 月 15 日 | Generated by Genspark AI Slides"><span class="lbl" style="background:#5dade2;">#1&nbsp;body</span></div><div class="bbox" style="left:183.9px;top:125.4px;width:496.2px;height:73.4px;border-color:#f1c40f;" title="#2 document_p001_c00002 [h3]
Technical Research Report ローカル AI 技術調査レポート"><span class="lbl" style="background:#f1c40f;">#2&nbsp;h3</span></div><div class="bbox" style="left:172.9px;top:212.9px;width:518.1px;height:76.4px;border-color:#f1c40f;" title="#3 document_p001_c00003 [h3]
オンデバイス/ローカル推論の現実解と推奨スタック
LLM VLM ASR TTS RAG Apple Silicon Windows GPU"><span class="lbl" style="background:#f1c40f;">#3&nbsp;h3</span></div><div class="bbox" style="left:179.6px;top:336.8px;width:481.2px;height:44.9px;border-color:#f1c40f;" title="#4 document_p001_c00004 [h3]
Report Purpose
Apple Silicon および Windows 環境におけるローカル AI の実運用ラインと推奨スタックを提示。 「意思決定のための要点」と「根拠となる完全な技術詳細」を二層構造で網羅。"><span class="lbl" style="background:#f1c40f;">#4&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 2 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:27.0px;top:17.7px;width:803.0px;height:107.6px;border-color:#f1c40f;" title="#5 document_p002_c00005 [h3]
このスライドの読み方 概要 → 詳細の二層構造ガイド
構造:二層設計"><span class="lbl" style="background:#f1c40f;">#5&nbsp;h3</span></div><div class="bbox" style="left:66.8px;top:166.0px;width:335.7px;height:42.8px;border-color:#f1c40f;" title="#6 document_p002_c00006 [h3]
1. 概要スライド( Executive Summary )
意思決定に必要な「結論」と「要点」を最初に提示します。時間がな い場合はここだけ読めば全体像が掴めます。"><span class="lbl" style="background:#f1c40f;">#6&nbsp;h3</span></div><div class="bbox" style="left:66.8px;top:111.0px;width:749.6px;height:367.2px;border-color:#f1c40f;" title="#7 document_p002_c00007 [h3]
2. 完全版スライド( Full Detail )
レポート内の表、数値、グラフ、注釈を省略せずに掲載します。エン ジニアや実装担当者が参照するための詳細情報です。
相互リンクと参照
概要から詳細へ、詳細から参考文献へ、論理的に接続されています 。
設計原則と表記
省略しない
元の技術レポートに含まれる情報は、脚注や URL に至るまで全てスラ イド内に保持します。
根拠の明示
「推測」と「事実("><span class="lbl" style="background:#f1c40f;">#7&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 3 (14 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:33.8px;top:23.7px;width:797.1px;height:45.5px;border-color:#f1c40f;" title="#8 document_p003_c00008 [h3]
Local AI Technical Report 目次( Agenda ) Total 13 Chapters"><span class="lbl" style="background:#f1c40f;">#8&nbsp;h3</span></div><div class="bbox" style="left:41.9px;top:105.5px;width:226.6px;height:26.0px;border-color:#f1c40f;" title="#9 document_p003_c00009 [h3]
01 タイトル/目的/読み方
レポートの目的と、概要 → 詳細の二層構造の活用方法。"><span class="lbl" style="background:#f1c40f;">#9&nbsp;h3</span></div><div class="bbox" style="left:41.9px;top:159.2px;width:211.4px;height:25.1px;border-color:#f1c40f;" title="#10 document_p003_c00010 [h3]
02 エグゼクティブサマリ
実務要点、中核技術、推奨スタックの概要を提示。"><span class="lbl" style="background:#f1c40f;">#10&nbsp;h3</span></div><div class="bbox" style="left:41.9px;top:212.9px;width:200.1px;height:26.0px;border-color:#f1c40f;" title="#11 document_p003_c00011 [h3]
03 ローカル AI の定義・前提
オンデバイス/ローカル LAN 推論の定義と範囲。"><span class="lbl" style="background:#f1c40f;">#11&nbsp;h3</span></div><div class="bbox" style="left:41.9px;top:266.6px;width:211.9px;height:26.0px;border-color:#f1c40f;" title="#12 document_p003_c00012 [h3]
04 ローカル AI ランドスケープ
LLM から Agent まで、技術スタックの全体像を図解。"><span class="lbl" style="background:#f1c40f;">#12&nbsp;h3</span></div><div class="bbox" style="left:41.9px;top:320.3px;width:220.1px;height:26.0px;border-color:#f1c40f;" title="#13 document_p003_c00013 [h3]
05 特徴軸(評価軸)の定義
指示追従、 JSON 堅牢性、メモリ効率などの評価基準。"><span class="lbl" style="background:#f1c40f;">#13&nbsp;h3</span></div><div class="bbox" style="left:41.9px;top:374.0px;width:192.5px;height:26.0px;border-color:#f1c40f;" title="#14 document_p003_c00014 [h3]
06 カテゴリ別モデルカタログ
LLM/VLM/ASR/TTS 等の代表モデルと採用判断。"><span class="lbl" style="background:#f1c40f;">#14&nbsp;h3</span></div><div class="bbox" style="left:41.9px;top:427.7px;width:203.8px;height:26.0px;border-color:#f1c40f;" title="#15 document_p003_c00015 [h3]
07 メモリ設計のコア
重み理論値、 KV キャッシュ、最適化技術の詳解。"><span class="lbl" style="background:#f1c40f;">#15&nbsp;h3</span></div><div class="bbox" style="left:453.8px;top:105.5px;width:214.2px;height:26.0px;border-color:#f1c40f;" title="#16 document_p003_c00016 [h3]
08 Tier 定義と “ 現実ライン ”
Apple Silicon / Windows GPU / CPU ごとの実用ライン。"><span class="lbl" style="background:#f1c40f;">#16&nbsp;h3</span></div><div class="bbox" style="left:453.8px;top:159.2px;width:231.3px;height:26.0px;border-color:#f1c40f;" title="#17 document_p003_c00017 [h3]
09 ユースケース別推奨スタック
低コスト/高品質/ハード制約別の構成案( 6 ケース)。"><span class="lbl" style="background:#f1c40f;">#17&nbsp;h3</span></div><div class="bbox" style="left:453.8px;top:212.9px;width:223.1px;height:26.0px;border-color:#f1c40f;" title="#18 document_p003_c00018 [h3]
10 測定と比較の方法
再現可能なベンチマーク手順と指標( tok/s, TTFT 等)。"><span class="lbl" style="background:#f1c40f;">#18&nbsp;h3</span></div><div class="bbox" style="left:453.8px;top:266.6px;width:211.4px;height:25.1px;border-color:#f1c40f;" title="#19 document_p003_c00019 [h3]
11 リスク/コンプライアンス
ライセンス条件、商用利用、セキュリティリスク。"><span class="lbl" style="background:#f1c40f;">#19&nbsp;h3</span></div><div class="bbox" style="left:453.8px;top:320.3px;width:211.4px;height:25.1px;border-color:#f1c40f;" title="#20 document_p003_c00020 [h3]
12 まとめ(意思決定フロー)
モデル選定と環境構築のための意思決定チャート。"><span class="lbl" style="background:#f1c40f;">#20&nbsp;h3</span></div><div class="bbox" style="left:59.4px;top:375.3px;width:782.1px;height:96.9px;border-color:#e67e22;" title="#21 document_p003_c00021 [h2]
13 References
一次情報源および参照 URL 一覧。
各章は「概要(要点)」 → 「詳細(完全版)」の順で構成されています
Page 3"><span class="lbl" style="background:#e67e22;">#21&nbsp;h2</span></div></div></div><div class="page-block"><h2>Page 4 (4 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:40.5px;top:32.9px;width:673.6px;height:271.9px;border-color:#f1c40f;" title="#22 document_p004_c00022 [h3]
エグゼクティブサマリ(実務要点)
ローカル AI 導入における意思決定の重要ポイント
結論:ローカル AI は「 4bit 量子化+ KV 管理」を前提に、 GGUF/llama.cpp 系と Ollama/LM Studio/MLX で実務化可能です。
定義とスコープ
推論が端末 or ローカル LAN 内で完結
入力データは外部へ送信されない
LAN サーブ含む( LM Studio, Oll"><span class="lbl" style="background:#f1c40f;">#22&nbsp;h3</span></div><div class="bbox" style="left:84.4px;top:272.3px;width:619.4px;height:99.7px;border-color:#f1c40f;" title="#23 document_p004_c00023 [h3]
Ollama / LM Studio : UI + API
MLX : Apple Silicon 特化
ハードウェア実用ライン (4bit 量子化前提 )"><span class="lbl" style="background:#f1c40f;">#23&nbsp;h3</span></div><div class="bbox" style="left:54.7px;top:386.0px;width:149.9px;height:52.6px;border-color:#f1c40f;" title="#24 document_p004_c00024 [h3]
Apple Silicon (Unified Memory)
16GB : 3B 〜 7/8B 級
24-32GB : 7B 〜 14B 級 ( 業務最小ライン )"><span class="lbl" style="background:#f1c40f;">#24&nbsp;h3</span></div><div class="bbox" style="left:292.4px;top:359.1px;width:531.9px;height:125.6px;border-color:#f1c40f;" title="#25 document_p004_c00025 [h3]
Windows (NVIDIA GPU VRAM)
8GB : 7B 級 / 12GB : 14B 級
24GB : 27B 〜 34B 級 ( 実務域 )
推奨スタック
低コスト : GGUF + llama.cpp
高品質 : GPU + TensorRT-LLM
ハード制約 : 3B 級 + RAG 工夫
補足:長文コンテキストは KV キャッシュがメモリを支配します。最終判断は再現ベンチマ"><span class="lbl" style="background:#f1c40f;">#25&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 5 (2 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:21.9px;width:772.0px;height:126.1px;border-color:#f1c40f;" title="#26 document_p005_c00026 [h3]
エグゼクティブサマリ(詳細 1 ) ローカル AI の定義と中核技術
結論:ローカル AI は「推論がユーザー管理下で完結する構成」と定義され、 重み量子化( 4bit ) と KV キャッシュ最適化 が実運用の技術的基盤です。"><span class="lbl" style="background:#f1c40f;">#26&nbsp;h3</span></div><div class="bbox" style="left:62.0px;top:217.1px;width:751.4px;height:230.4px;border-color:#f1c40f;" title="#27 document_p005_c00027 [h3]
ローカル AI の定義と射程
ユーザー端末(オンデバイス)、 PC 、またはローカル LAN 内サーバで完 結。
入力データ(文書・音声・画像)がデフォルトでクラウドへ送出されな い構成。
LM Studio や Ollama の「 localhost/network 公開」機能を含みます。
[1][2]
Localhost On-Premise LAN Privacy First
中核技術( E"><span class="lbl" style="background:#f1c40f;">#27&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 6 (4 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:50.0px;top:18.3px;width:782.6px;height:352.5px;border-color:#f1c40f;" title="#28 document_p006_c00028 [h3]
エグゼクティブサマリ(詳細 2 ) 実運用ランタイムとハードウェア実用ライン
結論:ランタイムは llama.cpp/GGUF 系・ MLX が第一選択となり、ハードウェアはメモリ容量で Tier 化されます(長文は KV 支配)。
実運用ランタイム(第一選択)
クロスプラットフォーム標準: llama.cpp
GGUF 形式必須。最小セットアップで CPU/GPU 推論が可能。
軽量・汎用で多く"><span class="lbl" style="background:#f1c40f;">#28&nbsp;h3</span></div><div class="bbox" style="left:468.7px;top:185.4px;width:227.4px;height:90.7px;border-color:#f1c40f;" title="#29 document_p006_c00029 [h3]
ハードウェア実用ライン( 4bit 中心概算)
Apple Silicon (Unified Memory):
3B 〜 7/8B 級(インタラクティブ) 16GB
7B 〜 14B 級(業務実用・ RAG 最小ライン) 24-32GB
14B 〜 32B 級以上(高品質・長文要約可) 64GB+"><span class="lbl" style="background:#f1c40f;">#29&nbsp;h3</span></div><div class="bbox" style="left:470.0px;top:288.3px;width:208.1px;height:60.5px;border-color:#f1c40f;" title="#30 document_p006_c00030 [h3]
Windows (Discrete GPU):
7B 級(最小ライン) VRAM 8GB
14B 級〜 27B 級(品質寄り) VRAM 12-16GB
32-34B 級・重い画像生成が実務域 VRAM 24GB"><span class="lbl" style="background:#f1c40f;">#30&nbsp;h3</span></div><div class="bbox" style="left:60.1px;top:361.0px;width:762.7px;height:96.6px;border-color:#f1c40f;" title="#31 document_p006_c00031 [h3]
Windows CPU-only:
7B 級が「使える」境界( AVX 活用)。 32GB RAM+
注意:長文コンテキストとメモリ
上記は重み( weight-only )の概算です。長文( Long Context )を扱う場合、 KV キャッシュがメモリを線形に圧迫するため、別途 KV 量子化やコンテキスト長制限( 4k/8k 等)の設計が必要です。"><span class="lbl" style="background:#f1c40f;">#31&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 7 (13 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:19.1px;width:767.1px;height:172.9px;border-color:#f1c40f;" title="#32 document_p007_c00032 [h3]
エグゼクティブサマリ(詳細 3 ) 推奨スタック 3 パターン( Tier 別構成案)
結論:用途とリソースに応じて「低コスト/高品質/ハード制約」の 3 構成を選択します。 ※各構成の具体的な失敗モードと回避策は、後述の「ユースケース別推奨スタック」章で詳細に展開します。
低コスト構成"><span class="lbl" style="background:#f1c40f;">#32&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:223.8px;width:198.5px;height:31.3px;border-color:#f1c40f;" title="#33 document_p007_c00033 [h3]
Target HW
CPU のみ / エントリー GPU / Apple 16GB"><span class="lbl" style="background:#f1c40f;">#33&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:272.8px;width:201.1px;height:37.7px;border-color:#f1c40f;" title="#34 document_p007_c00034 [h3]
LLM Runtime
llama.cpp (GGUF Q4/K) Ollama / LM Studio"><span class="lbl" style="background:#f1c40f;">#34&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:336.0px;width:201.6px;height:38.4px;border-color:#f1c40f;" title="#35 document_p007_c00035 [h3]
ASR / TTS
faster-whisper (INT8) Piper / Kokoro ( 軽量 )"><span class="lbl" style="background:#f1c40f;">#35&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:178.4px;width:433.5px;height:313.1px;border-color:#f1c40f;" title="#36 document_p007_c00036 [h3]
RAG / Embedding
BGE-M3 ( 多言語・多用途 ) bge-reranker-base
最低限の業務自動化
高品質構成"><span class="lbl" style="background:#f1c40f;">#36&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:223.8px;width:203.7px;height:38.4px;border-color:#f1c40f;" title="#37 document_p007_c00037 [h3]
Target HW
GPU 搭載機 (VRAM 16-24GB+) Apple 64GB+"><span class="lbl" style="background:#f1c40f;">#37&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:287.0px;width:222.6px;height:38.4px;border-color:#f1c40f;" title="#38 document_p007_c00038 [h3]
LLM Runtime
GPU 優先 (TensorRT-LLM / vLLM) 32B 〜 70B 級モデル"><span class="lbl" style="background:#f1c40f;">#38&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:350.2px;width:210.2px;height:38.4px;border-color:#f1c40f;" title="#39 document_p007_c00039 [h3]
ASR / TTS
faster-whisper (GPU) XTTS / StyleTTS2 ( 高品質 )"><span class="lbl" style="background:#f1c40f;">#39&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:178.4px;width:447.3px;height:313.1px;border-color:#f1c40f;" title="#40 document_p007_c00040 [h3]
RAG / VLM
bge-reranker-large Qwen2-VL 7B / InternVL2
リッチな体験・高精度
ハード制約構成"><span class="lbl" style="background:#f1c40f;">#40&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:223.8px;width:206.8px;height:38.4px;border-color:#f1c40f;" title="#41 document_p007_c00041 [h3]
Target HW
低スペック PC / 古い Mac メモリ 8GB 以下等"><span class="lbl" style="background:#f1c40f;">#41&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:287.0px;width:202.9px;height:38.4px;border-color:#f1c40f;" title="#42 document_p007_c00042 [h3]
LLM Runtime
Phi-3 mini (3.8B) 等 GGUF Q4 ( 極小モデル )"><span class="lbl" style="background:#f1c40f;">#42&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:350.2px;width:226.4px;height:43.1px;border-color:#f1c40f;" title="#43 document_p007_c00043 [h3]
Strategy
短いコンテキストで完結させる RAG は Embedding 検
索を重視"><span class="lbl" style="background:#f1c40f;">#43&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:413.4px;width:228.3px;height:78.1px;border-color:#f1c40f;" title="#44 document_p007_c00044 [h3]
Compromise
LLM 自体を軽くし、検索精度で補う生成タスクを限
定する
極限環境での動作"><span class="lbl" style="background:#f1c40f;">#44&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 8 (2 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:67.5px;top:19.8px;width:746.4px;height:23.7px;border-color:#f1c40f;" title="#45 document_p008_c00045 [h3]
ローカル AI の定義とスコープ Definition &amp; Scope"><span class="lbl" style="background:#f1c40f;">#45&nbsp;h3</span></div><div class="bbox" style="left:59.5px;top:107.8px;width:756.9px;height:386.5px;border-color:#f1c40f;" title="#46 document_p008_c00046 [h3]
DEFINITION
本資料の「ローカル AI 」とは、推論がユーザー管理下(端末/ローカル PC /ローカル LAN )で完結し、 入力データが外部へ送信されない構成を指します。
スコープの射程(範囲)
ローカル API サーバを含む : localhost だけでなく、 LAN 公開( network )された推論サーバも対象 。
LM Studio / Ollama
実装例 : Window"><span class="lbl" style="background:#f1c40f;">#46&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 9 (2 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:56.0px;top:22.9px;width:765.5px;height:121.1px;border-color:#5dade2;" title="#47 document_p009_c00047 [body]
未指定事項の扱い(前提条件) オフライン要件・対象 OS ・ライセンス
結論:「完全オフライン」は推論実行時のみを必須要件とし、 導入・更新時のネットワーク利用は許容する現実的な設計を前提とします。"><span class="lbl" style="background:#5dade2;">#47&nbsp;body</span></div><div class="bbox" style="left:32.4px;top:209.9px;width:798.5px;height:273.3px;border-color:#f1c40f;" title="#48 document_p009_c00048 [h3]
オフライン要件と対象 OS
“ 完全オフライン ” の定義: 「モデル推論自体はオフラインで成立する」レベルを基本とします。モ デルの初回ダウンロードや RAG 文書の取り込みプロセスにはネットワー ク接続が必要になり得る点を前提とします。
[19]
対象 OS のスコープ: macOS Ventura 以降および Windows 10/11 を主要ターゲットとします。 Linux は「 Wind"><span class="lbl" style="background:#f1c40f;">#48&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 10 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:54.0px;top:101.5px;width:89.8px;height:90.5px;border-color:#e74c3c;" title="#49 document_p010_c00049 [h1]
04"><span class="lbl" style="background:#e74c3c;">#49&nbsp;h1</span></div><div class="bbox" style="left:54.0px;top:236.2px;width:260.7px;height:132.8px;border-color:#e74c3c;" title="#50 document_p010_c00050 [h1]
ローカル AI ランドスケープ
オンデバイス実行環境の 全体像と技術スタック"><span class="lbl" style="background:#e74c3c;">#50&nbsp;h1</span></div><div class="bbox" style="left:443.0px;top:78.1px;width:351.8px;height:326.8px;border-color:#f1c40f;" title="#51 document_p010_c00051 [h3]
KE Y TAKEAWAYS
本章の要点
テキスト LLM の二大潮流:
GGUF ( llama.cpp 系)と GPU 量子化形式( GPTQ/AWQ/EXL2 等 )に大別されます。
プラットフォームの最適解:
macOS は Metal/Unified Memory 、 Windows は Ollama/LM Studio が実務的選択肢です。
構成要素の多様性:
LLM だけでなく、 A"><span class="lbl" style="background:#f1c40f;">#51&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 11 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:32.4px;top:17.4px;width:800.4px;height:461.2px;border-color:#f1c40f;" title="#52 document_p011_c00052 [h3]
ローカル AI ランドスケープ カテゴリ網羅・代表モデルとランタイムの関係図
Local AI Technical Survey Report 2026 11 / 80
結論:ローカル AI は 9 カテゴリ( LLM/VLM/ASR/TTS/Embedding/OCR/ 画像生成 /Agent/ 音声前処理)で構成され、 モデル系とランタイム系が実務上の結節点となります。
Core (AI Ca"><span class="lbl" style="background:#f1c40f;">#52&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 12 (8 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:20.8px;width:779.8px;height:95.3px;border-color:#f1c40f;" title="#53 document_p012_c00053 [h3]
ローカル AI ランドスケープ(詳細) カテゴリ別代表モデル・ランタイム・主用途
結論:実務上の収束点は「 LLM は GGUF (llama.cpp) か GPU 量子化系 」「 macOS は MLX/Unified Memory 最適化」「 Windows は Ollama/LM Studio + 必要に 応じ WSL2 」。"><span class="lbl" style="background:#f1c40f;">#53&nbsp;h3</span></div><div class="bbox" style="left:42.5px;top:165.4px;width:214.2px;height:78.9px;border-color:#f1c40f;" title="#54 document_p012_c00054 [h3]
テキスト LLM
形式 : GGUF (llama.cpp 必須 ), GPTQ/AWQ/EXL2 (GPU 向け ) [21]
代表 : Llama 3.1, Qwen2.5, Gemma 2, Phi-3/4
用途 : 汎用対話 , 要約 , RAG, 翻訳"><span class="lbl" style="background:#f1c40f;">#54&nbsp;h3</span></div><div class="bbox" style="left:312.7px;top:165.8px;width:187.9px;height:66.3px;border-color:#f1c40f;" title="#55 document_p012_c00055 [h3]
VLM / LMM
代表 : Qwen2-VL, Phi-3-Vision, LLaVA
実行 : MLX-VLM (Mac 最適化 ), Transformers, vLLM [28]
用途 : 画像理解 , OCR 代替 , UI 解析"><span class="lbl" style="background:#f1c40f;">#55&nbsp;h3</span></div><div class="bbox" style="left:582.8px;top:165.4px;width:223.4px;height:66.7px;border-color:#f1c40f;" title="#56 document_p012_c00056 [h3]
ASR (音声認識)
方式 : Whisper 系 (faster-whisper, whisper.cpp) [50] [51]
特徴 : CTranslate2 で高速・省メモリ , C++ 軽量実装
用途 : 議事録 , 音声コマンド , 文字起こし"><span class="lbl" style="background:#f1c40f;">#56&nbsp;h3</span></div><div class="bbox" style="left:42.5px;top:282.7px;width:211.9px;height:66.8px;border-color:#f1c40f;" title="#57 document_p012_c00057 [h3]
TTS (音声合成)
軽量 : Piper / Kokoro ( 高速・ CPU 向け ) [52][53]
高品質 : XTTS / StyleTTS2 ( クローン・ GPU 推奨 ) [54][55]
用途 : 読み上げ , 通知 , ボイスボット"><span class="lbl" style="background:#f1c40f;">#57&nbsp;h3</span></div><div class="bbox" style="left:312.7px;top:283.1px;width:210.1px;height:66.4px;border-color:#f1c40f;" title="#58 document_p012_c00058 [h3]
Embedding/Reranker
代表 : bge-m3, multilingual-e5, bge-reranker [58-62]
重要性 : RAG の検索精度( Recall/Precision )を決定
用途 : ベクトル検索 , 再ランキング"><span class="lbl" style="background:#f1c40f;">#58&nbsp;h3</span></div><div class="bbox" style="left:42.5px;top:283.1px;width:725.8px;height:171.5px;border-color:#f1c40f;" title="#59 document_p012_c00059 [h3]
OCR / Document AI
古典 : Tesseract / PaddleOCR ( 構造化 ) [64][66]
AI 型 : Donut / LayoutLMv3 ( 文書理解 ) [68][69]
用途 : PDF 解析 , 請求書読取 , スキャン処理
画像生成
代表 : SDXL, FLUX.1 ( 要 VRAM 確認 ) [71][75]
実行 : ComfyUI, SD We"><span class="lbl" style="background:#f1c40f;">#59&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:387.8px;width:797.3px;height:93.3px;border-color:#f1c40f;" title="#60 document_p012_c00060 [h3]
Agent / Tool-use
方式 : OpenAI 互換 API での Function Calling [24]
実行 : Ollama / LM Studio が実務的 [23][41]
用途 : タスク自動化 , 外部ツール連携
音声前処理
代表 : Silero VAD, pyannote, RNNoise [82][84]
役割 : 品質の下限担保(無音除去・話者分離)
用途 : "><span class="lbl" style="background:#f1c40f;">#60&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 13 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:54.0px;top:80.6px;width:89.8px;height:90.5px;border-color:#e74c3c;" title="#61 document_p013_c00061 [h1]
05"><span class="lbl" style="background:#e74c3c;">#61&nbsp;h1</span></div><div class="bbox" style="left:54.0px;top:217.9px;width:260.7px;height:171.9px;border-color:#f1c40f;" title="#62 document_p013_c00062 [h3]
特徴軸(評価軸 ) の定義
評価の共通物差しと 測定基準"><span class="lbl" style="background:#f1c40f;">#62&nbsp;h3</span></div><div class="bbox" style="left:443.0px;top:75.4px;width:352.0px;height:333.2px;border-color:#f1c40f;" title="#63 document_p013_c00063 [h3]
KE Y TAKEAWAYS
本章の要点
13 の評価軸:
指示追従、 JSON 堅牢性、長文耐性、速度、メモリなど、多 角的な視点でモデルを比較評価します。
KV キャッシュ最適化:
paged/quantized KV や reuse 機能が、長文コンテキストや多同 時リクエスト処理の鍵となります。
JSON 制約の重要性:
ツール利用において JSON 出力の安定性は必須ですが、機能 はラン"><span class="lbl" style="background:#f1c40f;">#63&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 14 (2 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:33.8px;top:20.2px;width:122.2px;height:9.1px;border-color:#5dade2;" title="#64 document_p014_c00064 [body]
Evaluation Metrics Overview"><span class="lbl" style="background:#5dade2;">#64&nbsp;body</span></div><div class="bbox" style="left:33.8px;top:40.8px;width:807.0px;height:434.8px;border-color:#f1c40f;" title="#65 document_p014_c00065 [h3]
特徴軸(評価軸)の定義 Total 13 Metrics
01 指示追従 (Instruction Following) プロンプトの制約・禁止事項・形式指定(箇条書き禁止等)を遵守する能力。
02 幻覚耐性 (Hallucination Resistance) 根拠のない固有名詞や数値を捏造せず、不確実性を提示できる性質。
03 JSON 堅牢性 (Structured Output) スキーマ"><span class="lbl" style="background:#f1c40f;">#65&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 15 (6 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:27.0px;top:17.7px;width:801.0px;height:18.8px;border-color:#5dade2;" title="#66 document_p015_c00066 [body]
特徴軸(詳細):測定と設計含意 13 の評価軸における計測手順と技術的背景"><span class="lbl" style="background:#5dade2;">#66&nbsp;body</span></div><div class="bbox" style="left:81.7px;top:97.9px;width:134.8px;height:13.6px;border-color:#f1c40f;" title="#67 document_p015_c00067 [h3]
機能・性能評価のコア"><span class="lbl" style="background:#f1c40f;">#67&nbsp;h3</span></div><div class="bbox" style="left:61.5px;top:143.8px;width:346.6px;height:181.7px;border-color:#f1c40f;" title="#68 document_p015_c00068 [h3]
JSON 堅牢性と指示追従
ツール呼び出し等の業務連携ではスキーマ破壊が致命的。 Ollama/LM Studio の OpenAI 互換 API を用いて同一条件で比較検証を行う。 llama-cpp-python 等のランタイム依存機 能( response_format )も活用。
[24]
[25]
長文耐性とメモリ管理
コンテキスト長に応じて KV キャッシュメモリは線形に増加し、支配的"><span class="lbl" style="background:#f1c40f;">#68&nbsp;h3</span></div><div class="bbox" style="left:61.5px;top:345.5px;width:347.1px;height:41.6px;border-color:#f1c40f;" title="#69 document_p015_c00069 [h3]
画像理解 (VLM)
視覚トークン数が増加すると速度・メモリ効率が急落するため、解像度や max-pixels 制 御が重要。 [28]"><span class="lbl" style="background:#f1c40f;">#69&nbsp;h3</span></div><div class="bbox" style="left:497.1px;top:97.9px;width:134.8px;height:13.6px;border-color:#f1c40f;" title="#70 document_p015_c00070 [h3]
技術的詳細と設計含意"><span class="lbl" style="background:#f1c40f;">#70&nbsp;h3</span></div><div class="bbox" style="left:362.8px;top:143.8px;width:461.2px;height:334.4px;border-color:#f1c40f;" title="#71 document_p015_c00071 [h3]
KV キャッシュ最適化技術
vLLM は Paged KV Cache 設計でメモリ断片化を防ぎ、 KV キャッシュを FP8 等に量子化して フットプリントを削減。 NVIDIA TensorRT-LLM も KV reuse (再利用)を最適化として強調 。長文・多同時接続時の鍵となる。
[5][6]
[7]
量子化耐性と相性
4bit 化による品質劣化はモデルと量子化方式( AWQ, GPT"><span class="lbl" style="background:#f1c40f;">#71&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 16 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:54.0px;top:101.5px;width:89.8px;height:90.5px;border-color:#e74c3c;" title="#72 document_p016_c00072 [h1]
06"><span class="lbl" style="background:#e74c3c;">#72&nbsp;h1</span></div><div class="bbox" style="left:54.0px;top:236.2px;width:260.7px;height:132.8px;border-color:#f1c40f;" title="#73 document_p016_c00073 [h3]
カテゴリ別 モデルカタログ
主要 9 カテゴリの 詳細データと採用判断"><span class="lbl" style="background:#f1c40f;">#73&nbsp;h3</span></div><div class="bbox" style="left:443.0px;top:71.4px;width:345.8px;height:341.3px;border-color:#f1c40f;" title="#74 document_p016_c00074 [h3]
SECTION OVERVIEW
提示方針:三層構造
概要( Overview ):
カテゴリごとの主要トレンドと代表モデルの要点を概説。
完全版表( Full Data ):
規模・量子化・得意不得意・ランタイム相性・リスク・参 考文献を列落ちなく全掲載。
採用判断( Decision ):
「どう選ぶか」の意思決定基準と失敗回避策を提示。
テキスト LLM VLM/LMM ASR TTS Em"><span class="lbl" style="background:#f1c40f;">#74&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 17 (2 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:55.3px;top:22.9px;width:770.5px;height:330.8px;border-color:#e74c3c;" title="#75 document_p017_c00075 [h1]
テキスト LLM (汎用)概要 主要 7 モデルと実運用環境
結論:ローカル実務のボリュームゾーンは 7B 〜 14B ( 4bit ) 。 形式は GGUF ( llama.cpp 系)か GPU 量子化( GPTQ/AWQ/EXL2 )の二択が現実解です。
代表モデル群(要点)
Llama 3.1 Instruct: 8B/70B/405B 系。多言語対話最適化を明記。 [29-31]
Qwe"><span class="lbl" style="background:#e74c3c;">#75&nbsp;h1</span></div><div class="bbox" style="left:32.4px;top:194.8px;width:797.9px;height:283.8px;border-color:#f1c40f;" title="#76 document_p017_c00076 [h3]
実行環境と形式( Runtime &amp; Format )
クロスプラットフォーム標準。 CPU/GPU ハイブリッド実行が可能。 GGUF 形式が必須。
[21][116]
MLX / MLX-LM による変換・実行、または GGUF (llama.cpp) が最適。 Unified Memory を活用 。 [22][120]
vLLM / ExLlamaV2 / AutoGPTQ 等。 GPU "><span class="lbl" style="background:#f1c40f;">#76&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 18 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:778.0px;height:275.2px;border-color:#f1c40f;" title="#77 document_p018_c00077 [h3]
テキスト LLM (完全版表 1 ) 主要モデル詳細比較: Llama 3.1 / Qwen2.5 / Gemma 2 / Mixtral
代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断
Llama 3.1 Instruct Meta [29-31]
8B 70B 405B
4bit 中心 (GGUF 等 ) ※方式は環境依存
得意 多言語対"><span class="lbl" style="background:#f1c40f;">#77&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:344.5px;width:776.6px;height:30.9px;border-color:#f1c40f;" title="#78 document_p018_c00078 [h3]
Gemma 2 Google [35]
2B 9B 27B
2B/9B はローカル向き 27B は VRAM/ メモリ要求 増
得意 モデルカード更新日が明示されて おり、責任ある利用を前提に整理さ れている [35]
Mac: MLX / GGUF 変換 Win: GGUF / 各 GPU
Risk “ 事実質問用途での誤生成 成 ” は一般に起こり得る(モ モデル一般リスク)"><span class="lbl" style="background:#f1c40f;">#78&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:412.6px;width:792.4px;height:65.7px;border-color:#f1c40f;" title="#79 document_p018_c00079 [h3]
Mixtral 8x7B Mistral [36-37]
MoE (8×7B)
GPU 向けで真価が出やす い (量子化 / 実装依存)
得意 「 Llama2 70B を多くのベンチで上 上回る」等をモデル説明で明記 [37]
Win GPU: 適合ランタイム( vLLM 等)前提 Mac: 難易度高め
Risk MoE は実装・ VRAM ・スル ループットのブレが大きい
採用判断の要点:ロー"><span class="lbl" style="background:#f1c40f;">#79&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 19 (4 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:771.6px;height:107.0px;border-color:#f1c40f;" title="#80 document_p019_c00080 [h3]
テキスト LLM (完全版表 2 ) 主要モデル詳細比較: Phi-3 Mini / Phi-4 / gpt-oss
代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断"><span class="lbl" style="background:#f1c40f;">#80&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:176.4px;width:776.5px;height:29.9px;border-color:#f1c40f;" title="#81 document_p019_c00081 [h3]
Phi-3 Mini Microsoft [38]
3.8B 4bit でローカル適性高い 長文版 (128K) 運用は KV が 支配
得意 「 128K 文脈」をうたうが長文はメ メモリ設計前提 [38]
Mac/Win: 比較的回しやすい( 形式依存)
Risk “ 小さい=万能 ” ではなく く、専門領域は RAG 前提で補 補う判断"><span class="lbl" style="background:#f1c40f;">#81&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:246.0px;width:776.8px;height:30.8px;border-color:#f1c40f;" title="#82 document_p019_c00082 [h3]
Phi-4 Microsoft [39-40]
14B 4bit でローカル上位ライ ン ( 32GB 級以上推奨)
得意 データ品質重視・合成データ活用 を明示 [40]
Win GPU: 最適化次第 Mac: MLX / 変換次第
Risk 14B は “ 重いが現実的 ” の境界。
の境 KV/ 長文設計が重要"><span class="lbl" style="background:#f1c40f;">#82&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:315.6px;width:793.6px;height:162.6px;border-color:#f1c40f;" title="#83 document_p019_c00083 [h3]
gpt-oss OpenAI [41]
20B 120B
MXFP4 量子化で出荷を明 明記 (他量子化なし)
得意 ツール呼び出し・ローカル API 利 用までガイドあり [27]
Mac/Win: LM Studio / Ollama で 手順が提示される [41]
Risk 20B は 16GB 以上、 120B は
は 60GB 以上推奨を明記 [41]
採用判断の要点: 14B クラス("><span class="lbl" style="background:#f1c40f;">#83&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 20 (6 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:17.4px;width:779.8px;height:460.9px;border-color:#f1c40f;" title="#84 document_p020_c00084 [h3]
採用判断基準(テキスト LLM ) 選定フローと実務上の重要ポイント
結論:モデル選定は「必要品質 → 許容レイテンシ → 許容メモリ → 形式 → ランタイム」の順に行うのが、実務上最も破綻しにくい フローです。
実務の現実ライン
7B 〜 14B がボリュームゾーン
一般業務や RAG において、品質と速度のバランスが良いスイー トスポット。
3B 級はハード制約モード
メモリ不足時のフォール"><span class="lbl" style="background:#f1c40f;">#84&nbsp;h3</span></div><div class="bbox" style="left:75.8px;top:173.8px;width:79.1px;height:72.4px;border-color:#e74c3c;" title="#85 document_p020_c00085 [h1]
1
必要品質
タスク難易度でサイズ決
定 ( 例 : 70B vs 8B)"><span class="lbl" style="background:#e74c3c;">#85&nbsp;h1</span></div><div class="bbox" style="left:242.4px;top:178.6px;width:62.9px;height:63.2px;border-color:#e74c3c;" title="#86 document_p020_c00086 [h1]
2
許容レイテンシ
リアルタイム性か
バッチ処理か"><span class="lbl" style="background:#e74c3c;">#86&nbsp;h1</span></div><div class="bbox" style="left:396.6px;top:178.6px;width:71.1px;height:63.9px;border-color:#e74c3c;" title="#87 document_p020_c00087 [h1]
3
許容メモリ
4bit 量子化前提で VRAM/RAM に収まるか"><span class="lbl" style="background:#e74c3c;">#87&nbsp;h1</span></div><div class="bbox" style="left:560.2px;top:178.6px;width:61.1px;height:63.2px;border-color:#e74c3c;" title="#88 document_p020_c00088 [h1]
4
形式選定
GGUF or GPTQ/AWQ
要件先行で決定"><span class="lbl" style="background:#e74c3c;">#88&nbsp;h1</span></div><div class="bbox" style="left:713.1px;top:178.6px;width:71.9px;height:63.9px;border-color:#e74c3c;" title="#89 document_p020_c00089 [h1]
5
ランタイム
形式に合うものを選択
(llama.cpp / vLLM 等 )"><span class="lbl" style="background:#e74c3c;">#89&nbsp;h1</span></div></div></div><div class="page-block"><h2>Page 21 (5 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:56.0px;top:21.9px;width:771.2px;height:183.3px;border-color:#f1c40f;" title="#90 document_p021_c00090 [h3]
VLM/LMM 概要(画像理解・マルチモーダル) カテゴリ別モデルカタログ
結論:ローカル運用の現実ラインは 2B/7B 級 が中心。 画像トークン化の前処理依存が強く、ランタイムの明示サポートが重要です。
代表モデルと特徴"><span class="lbl" style="background:#f1c40f;">#90&nbsp;h3</span></div><div class="bbox" style="left:65.5px;top:224.6px;width:322.6px;height:25.9px;border-color:#f1c40f;" title="#91 document_p021_c00091 [h3]
Qwen2-VL (2B/7B/72B) [42]
ローカル現実ラインの主力。 2B/7B が実用的。 72B は上位ハード( 96GB+ )前提。"><span class="lbl" style="background:#f1c40f;">#91&nbsp;h3</span></div><div class="bbox" style="left:65.5px;top:193.5px;width:552.2px;height:214.8px;border-color:#f1c40f;" title="#92 document_p021_c00092 [h3]
Phi-3-Vision (128K) [43]
軽量マルチモーダル+長文対応を標榜。テキスト重視の画像理解に適正。
InternVL2 (4B 等 ) [44-45]
画像理解の系列として設計。モデル群が複数サイズ展開。
LLaVA (7B/13B 系 ) [46-47]
画像チャット用途の先駆者。 4bit 運用で 12GB VRAM でも動作可能。
ランタイム相性と技術課題"><span class="lbl" style="background:#f1c40f;">#92&nbsp;h3</span></div><div class="bbox" style="left:473.4px;top:224.6px;width:277.6px;height:36.9px;border-color:#f1c40f;" title="#93 document_p021_c00093 [h3]
macOS / Apple Silicon
MLX-VLM: Qwen2-VL 等の利用例を具体コマンドで提示。
Unified Memory を活用し、システムメモリで大型モデルを実行可能。
[28]"><span class="lbl" style="background:#f1c40f;">#93&nbsp;h3</span></div><div class="bbox" style="left:61.5px;top:286.0px;width:760.3px;height:197.5px;border-color:#f1c40f;" title="#94 document_p021_c00094 [h3]
Windows / GPU
transformers / vLLM: GPU 推論が主流。モデル形式( Safetensors/AWQ 等)とランタイム の対応に依存。
技術的ボトルネック
画像トークンが増えるとメモリ / 速度が急落。解像度制御や max-pixels 設定が運用の肝と なる。
Visual Tokens Multi-modal RAG Resolution Control
設計"><span class="lbl" style="background:#f1c40f;">#94&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 22 (5 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:760.5px;height:109.0px;border-color:#f1c40f;" title="#95 document_p022_c00095 [h3]
VLM/LMM (完全版表) Qwen2-VL / Phi-3-Vision / InternVL2 / LLaVA 詳細比較
代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断"><span class="lbl" style="background:#f1c40f;">#95&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:180.3px;width:772.6px;height:30.8px;border-color:#f1c40f;" title="#96 document_p022_c00096 [h3]
Qwen2-VL Qwen [42]
2B 7B 72B
2B/7B がローカル現実ライン 72B は上位ハード前提
得意 画像+テキストの汎用 画像トークン増でメモリ / 速度急 落 → 解像度制御が肝
Mac: MLX-VLM が利用例を明 明示 [28] Win: transformers / vLLM
Risk 視覚トークン肥大による リソース枯渇"><span class="lbl" style="background:#f1c40f;">#96&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:253.9px;width:773.5px;height:30.8px;border-color:#f1c40f;" title="#97 document_p022_c00097 [h3]
Phi-3-Vision Microsoft [43]
4.2B (128K)
形式依存でローカル適性あり 128K 長文対応
得意 軽量マルチモーダル+長文を 標榜 “ 実務的な OCR 読取 ” に強み
Mac: MLX 変換次第 Win: GPU 推論
Risk 長文は KV が支配しやすく 、メモリ管理が必須 [43]"><span class="lbl" style="background:#f1c40f;">#97&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:327.4px;width:772.6px;height:30.8px;border-color:#f1c40f;" title="#98 document_p022_c00098 [h3]
InternVL2 OpenGVLab [44-45]
1B/2B 4B/8B 等
4B 等がローカル向き モデル群が複数サイズ展開
得意 画像理解の系列として設計 構成要素をモデルカードで詳細説 明 [45]
Win: GPU / 形式次第 Mac: 実装依存
Risk エコシステム差(変換・ プロセッサ依存)が運用の 難所"><span class="lbl" style="background:#f1c40f;">#98&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:401.0px;width:792.8px;height:77.3px;border-color:#f1c40f;" title="#99 document_p022_c00099 [h3]
LLaVA Community [46-47]
7B 13B
4bit での運用示唆 13B で 12GB VRAM 動作可 [47]
得意 画像チャット用途 Win GPU で構築しやすい
Win: GPU 推論で実績多 Mac: llama.cpp 等
Risk ベース LLM や実装差が大 きく、モデルカードとの整 合確認が必要 採用判断の要点:画像理解はテキスト LLM より前処理・プロセ"><span class="lbl" style="background:#f1c40f;">#99&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 23 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:17.4px;width:779.7px;height:425.4px;border-color:#f1c40f;" title="#100 document_p023_c00100 [h3]
採用判断基準( VLM/LMM ) ランタイム適合性と運用設計
結論:画像理解は前処理・プロセッサ依存が強いため、「ランタイムの明示サポート確認」が最優先です。 2B/7B 級を基本とし 、不足分を RAG/OCR で補うのが安全策です。
ランタイム適合性
一次情報の確認
ランタイムがモデルを明示サポートしているか確認が安全( 例 : MLX-VLM の Qwen2-VL 対応コマンド提示)。
プ"><span class="lbl" style="background:#f1c40f;">#100&nbsp;h3</span></div><div class="bbox" style="left:324.4px;top:311.1px;width:227.5px;height:131.7px;border-color:#f1c40f;" title="#101 document_p023_c00101 [h3]
運用設計( 2B/7B 級)
現実的なライン
72B 級は 96GB+ メモリや複数 GPU が必要なため、実務では 2B/7B 級での運用設計に寄せる。
視覚トークン制御
長文・多画像時は KV 支配とトークン肥大に注意。 max-pixels/ マ ルチ画像数を制御する。"><span class="lbl" style="background:#f1c40f;">#101&nbsp;h3</span></div><div class="bbox" style="left:97.2px;top:209.4px;width:722.1px;height:268.9px;border-color:#f1c40f;" title="#102 document_p023_c00102 [h3]
補完戦略( OCR/RAG )
不得意のカバー
微細な文字認識や構造化は OCR ( PaddleOCR 等)に任せ、 VLM は「意味理解」に集中させる。
ハイブリッド構成
画像から情報をテキスト抽出し、 Embedding 化して RAG で検索 可能にする設計が堅牢。
Page 23 | ローカル AI 技術調査レポート
サポート確認
README 等でランタイム (MLX/vLLM) の対"><span class="lbl" style="background:#f1c40f;">#102&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 24 (2 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:21.2px;width:774.4px;height:119.6px;border-color:#e74c3c;" title="#103 document_p024_c00103 [h1]
ASR (音声認識)概要 カテゴリ別モデルカタログ
結論:ローカル ASR は「 RTF ・メモリ・誤転記」の同時最適化が必須。 faster-whisper を基準実装とし、速度と精度(幻覚抑制)のバランスを図ります。"><span class="lbl" style="background:#e74c3c;">#103&nbsp;h1</span></div><div class="bbox" style="left:49.3px;top:198.9px;width:769.8px;height:275.1px;border-color:#f1c40f;" title="#104 document_p024_c00104 [h3]
Whisper 系実装の比較
実装名 特徴・最適化 推奨用途
Whisper (Original) [48] 研究 / 公開ベース、 PyTorch 依存 精度検証・基準
faster-whisper [50] CTranslate2 、 8bit 量子化、最大 4 倍速 実務の第一選択
whisper.cpp [51] C/C+ + 軽量、 g gml 、 Apple Silicon 最 組込み"><span class="lbl" style="background:#f1c40f;">#104&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 25 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:761.7px;height:108.7px;border-color:#f1c40f;" title="#105 document_p025_c00105 [h3]
ASR (完全版表) 主要モデル・方式比較: Whisper / faster-whisper / whisper.cpp
代表モデル / 方式 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断"><span class="lbl" style="background:#f1c40f;">#105&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:179.7px;width:776.6px;height:103.9px;border-color:#f1c40f;" title="#106 document_p025_c00106 [h3]
Whisper OpenAI [48-49]
Tiny 〜 Large-v3
実装多数 (下記派生に分岐)
得意 68 万時間規模の多言語データで頑 健性を示し、翻訳も可能 [48]
汎用 : 直接実装 / 各派生 Risk 高リスク領域では “ 誤転記 記(幻覚) ” が実害になる可 可能性が報道されており、 検証プロセスが必須 [49]
faster-whisper CTranslate2 ["><span class="lbl" style="background:#f1c40f;">#106&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:325.8px;width:790.3px;height:152.5px;border-color:#f1c40f;" title="#107 document_p025_c00107 [h3]
whisper.cpp ggml [51]
Whisper 互換
ggml 系で ローカル最適化
得意 C/C++ で軽量運用、各種最適化例 例が豊富
Mac/Win: 共に導入可能 Info バッチ向きに設計し、ス トリーミング要件は別設計 が必要になるケースがある (一般論) [51]
採用判断の要点:ローカル ASR は「 RTF (実時間比)」「メモリ」「誤転記リスク」の 3 要素を同時に"><span class="lbl" style="background:#f1c40f;">#107&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 26 (5 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:17.4px;width:775.6px;height:460.9px;border-color:#f1c40f;" title="#108 document_p026_c00108 [h3]
採用判断基準( ASR ) RTF 要件と誤転記リスクに基づく選定フロー
結論: faster-whisper を基準実装とし、 RTF 要件・誤転記リスクに応じて GPU 化やモデル拡大を検討する段階的最適化が合理的で す。
前処理の固定
VAD 設定の統一
無音区間の除去は認識精度と速度に直結するため、 Silero VAD 等の設定を固定し再現性を担保する。
ベンチマーク条件
同一音声・同一"><span class="lbl" style="background:#f1c40f;">#108&nbsp;h3</span></div><div class="bbox" style="left:113.3px;top:173.8px;width:54.2px;height:64.0px;border-color:#e74c3c;" title="#109 document_p026_c00109 [h1]
1
基準実装
faster-whisper (CPU/INT8) で検証"><span class="lbl" style="background:#e74c3c;">#109&nbsp;h1</span></div><div class="bbox" style="left:309.7px;top:173.8px;width:51.2px;height:63.2px;border-color:#e74c3c;" title="#110 document_p026_c00110 [h1]
2
RTF 評価
実時間比 (RTF)&lt;1 を満たすか確認"><span class="lbl" style="background:#e74c3c;">#110&nbsp;h1</span></div><div class="bbox" style="left:506.0px;top:173.8px;width:47.4px;height:63.2px;border-color:#e74c3c;" title="#111 document_p026_c00111 [h1]
3
品質評価
誤転記 ( 幻覚 ) の
許容範囲内か"><span class="lbl" style="background:#e74c3c;">#111&nbsp;h1</span></div><div class="bbox" style="left:695.4px;top:173.8px;width:57.7px;height:63.2px;border-color:#e74c3c;" title="#112 document_p026_c00112 [h1]
4
最適化
不足なら GPU 化 またはモデル拡大"><span class="lbl" style="background:#e74c3c;">#112&nbsp;h1</span></div></div></div><div class="page-block"><h2>Page 27 (2 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:56.4px;top:21.9px;width:769.3px;height:334.0px;border-color:#e74c3c;" title="#113 document_p027_c00113 [h1]
TTS (音声合成)概要 読み上げと音声クローンの分離設計
結論:「読み上げ」は 軽量モデル( Piper/Kokoro ) で常駐化し、 「音声クローン」は 高品質モデル( XTTS/StyleTTS2 ) で分離設計します。
代表システムと特徴
Piper: &quot;fast, local neural TTS&quot; を標榜。省リソース・常駐向き。 [52]
Kokoro: 82M パラメータで高品質・"><span class="lbl" style="background:#e74c3c;">#113&nbsp;h1</span></div><div class="bbox" style="left:63.3px;top:217.1px;width:754.9px;height:256.9px;border-color:#f1c40f;" title="#114 document_p027_c00114 [h3]
Text-to-Speech Voice Cloning
運用の分離とリスク管理
軽量 TTS を採用し、システム負荷を最小化。バックグラウンドでの安定 動作を優先。
GPU リソースを割り当て品質を最大化。ただし、法務・権利リスク管理 (許諾・ログ監査)を必須要件とします。 [56]
Risk Management Resource Optimization
日本語品質への注意( Quality"><span class="lbl" style="background:#f1c40f;">#114&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 28 (5 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:762.5px;height:106.7px;border-color:#f1c40f;" title="#115 document_p028_c00115 [h3]
TTS (完全版表) 主要モデル詳細比較: Piper / Kokoro / XTTS-v2 / StyleTTS2
代表モデル / 方式 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断"><span class="lbl" style="background:#f1c40f;">#115&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:175.7px;width:773.2px;height:30.8px;border-color:#f1c40f;" title="#116 document_p028_c00116 [h3]
Piper Fast/Local [52]
軽量 ローカル常駐 TTS 向き
得意 &quot;fast, local neural TTS&quot; を明示 得意 省リソース環境で有利
Win/Mac: 導入容易 Risk リポジトリ移転あり(運 用はフォーク / 移転先を確認 ) [52]"><span class="lbl" style="background:#f1c40f;">#116&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:244.7px;width:775.6px;height:29.9px;border-color:#f1c40f;" title="#117 document_p028_c00117 [h3]
Kokoro Open Weight [53]
82M 軽量 TTS として 使い分け
得意 82M で高速・コスト効率を主張 得意 Apache ライセンス重みをうたう [53]
CPU: 成立しやすい Note 日本語品質は声・辞書・ 前処理に依存(事前確認必 須)"><span class="lbl" style="background:#f1c40f;">#117&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:313.7px;width:772.6px;height:28.2px;border-color:#f1c40f;" title="#118 document_p028_c00118 [h3]
Coqui XTTS-v2 Clone [54]
大きめ GPU 推奨 音声クローン寄り
得意 数秒の参照音声で多言語クローン をうたう [54]
GPU 環境 : 実務的 Risk 法務・倫理・権利リスク 大(同意・監査が必須) [54,56]"><span class="lbl" style="background:#f1c40f;">#118&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:382.6px;width:784.1px;height:95.7px;border-color:#f1c40f;" title="#119 document_p028_c00119 [h3]
StyleTTS2 Research [55]
研究系 高品質 TTS 志向 得意 &quot;human-level&quot; を目標とする研究系 [55]
GPU: 望ましい Risk 実運用は依存関係・再現 性の確認が必須
採用判断の要点:「読み上げ(通知・要約)」と「クローン(本人声)」は別物として分離設計します。前者は Piper/Kokoro のような軽量系、後者は XTTS 等で、法務・倫理リスク管理"><span class="lbl" style="background:#f1c40f;">#119&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 29 (7 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:17.4px;width:779.2px;height:94.9px;border-color:#e74c3c;" title="#120 document_p029_c00120 [h1]
採用判断基準( TTS ) 品質・コスト・リスクの分離管理
結論:「読み上げ(通知・要約)」と「クローン(本人声)」は別レイヤで運用すべきです。前者は軽量化し、後者は法務・ 倫理リスクを管理します。"><span class="lbl" style="background:#e74c3c;">#120&nbsp;h1</span></div><div class="bbox" style="left:53.4px;top:295.0px;width:491.9px;height:140.7px;border-color:#f1c40f;" title="#121 document_p029_c00121 [h3]
分離運用( Layered )
軽量 TTS の常駐
通知・要約読み上げは Piper/Kokoro 等で省リソース化し、シス テムに常駐させる。
クローンは別系統
XTTS 等は GPU リソースを消費するため、必要な時のみ呼び出す 別サービスとして切り出す。
リスク・コンプライアンス
権利同意の必須化
音声クローンは法務・倫理リスクが高い。業務利用時は対象 者の書面同意をプロセス化する。
ログ"><span class="lbl" style="background:#f1c40f;">#121&nbsp;h3</span></div><div class="bbox" style="left:362.8px;top:295.0px;width:453.5px;height:183.3px;border-color:#f1c40f;" title="#122 document_p029_c00122 [h3]
品質保証( QA )
日本語品質の事前試験
モデルによりアクセントや読み間違い(数値・固有名詞)の 差が大きい。
辞書・前処理の標準化
業務特有の用語はユーザー辞書や前処理ルール(正規化)で カバーする義務を課す。
Page 29 | ローカル AI 技術調査レポート"><span class="lbl" style="background:#f1c40f;">#122&nbsp;h3</span></div><div class="bbox" style="left:106.4px;top:173.8px;width:57.7px;height:63.2px;border-color:#e74c3c;" title="#123 document_p029_c00123 [h1]
1
用途定義
単なる読み上げか
本人性の再現か"><span class="lbl" style="background:#e74c3c;">#123&nbsp;h1</span></div><div class="bbox" style="left:310.7px;top:173.8px;width:45.4px;height:64.0px;border-color:#e74c3c;" title="#124 document_p029_c00124 [h1]
2
モデル選定
軽量 (Piper) vs
高品質 (XTTS)"><span class="lbl" style="background:#e74c3c;">#124&nbsp;h1</span></div><div class="bbox" style="left:506.1px;top:173.8px;width:50.6px;height:63.2px;border-color:#e74c3c;" title="#125 document_p029_c00125 [h1]
3
リスク判定
声の権利許諾と
ログ監査設計"><span class="lbl" style="background:#e74c3c;">#125&nbsp;h1</span></div><div class="bbox" style="left:699.2px;top:173.8px;width:60.3px;height:64.0px;border-color:#e74c3c;" title="#126 document_p029_c00126 [h1]
4
実装・運用
常駐 (CPU) or オンデマンド (GPU)"><span class="lbl" style="background:#e74c3c;">#126&nbsp;h1</span></div></div></div><div class="page-block"><h2>Page 30 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:56.0px;top:21.9px;width:768.9px;height:447.6px;border-color:#f1c40f;" title="#127 document_p030_c00127 [h3]
Embedding / Reranker 概要 検索・ RAG 基盤モデル
結論: RAG システムの品質上限は Embedding の検索精度 で決まります。 まず BGE-M3 等で Recall を確保し、 Reranker で Precision を向上させる二段構えが定石です。
Embedding ( ベクトル検索 )
Retrieval Vector DB
Multi-Function"><span class="lbl" style="background:#f1c40f;">#127&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 31 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:758.1px;height:105.3px;border-color:#f1c40f;" title="#128 document_p031_c00128 [h3]
Embedding/Reranker (完全版表) 検索・ RAG 基盤: BGE-M3 / multilingual-e5 / bge-reranker
代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断"><span class="lbl" style="background:#f1c40f;">#128&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:173.0px;width:775.4px;height:86.7px;border-color:#f1c40f;" title="#129 document_p031_c00129 [h3]
BGE-M3 BAAI [58]
Model Card 参照
FP16/INT8 で安定運用 必要なら量子化
特徴 Multi-Functionality Multi-Linguality Multi-Granularity を明示 [58]
CPU/GPU/ONNX 等で運用可能 (環境依存)
推奨 RAG は Embedding 品質が が上限を決めるため、まず ここを堅くする
multili"><span class="lbl" style="background:#f1c40f;">#129&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:305.5px;width:791.0px;height:172.8px;border-color:#f1c40f;" title="#130 document_p031_c00130 [h3]
bge-reranker BAAI [61-62]
278M 〜 560M 等
large/v2 等 RAG の “ 再ランキング ” で 精度向上
機能 「クエリ+文書を入力しスコ アを直接出す」とモデルカードで 説明 [62]
CPU/GPU (遅延要件次第)
Risk reranker はレイテンシを増 増やすため、 p95 要件で採否 を判断
採用判断の要点: RAG の失敗の多くは「検索"><span class="lbl" style="background:#f1c40f;">#130&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 32 (7 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:17.4px;width:779.6px;height:460.9px;border-color:#f1c40f;" title="#131 document_p032_c00131 [h3]
採用判断基準( Embedding / Reranker ) 検索精度と運用安定化のための二段構え設計
結論: Embedding で網羅性 (Recall) を確保し、 Reranker で精度 (Precision) を上げる「二段構え」が実務的な最適解です。
運用の安定化
前計算(夜間バッチ)
検索対象文書の Embedding 化や OCR 処理は夜間にバッチ実行し 、日中の計算資源を L"><span class="lbl" style="background:#f1c40f;">#131&nbsp;h3</span></div><div class="bbox" style="left:168.1px;top:156.0px;width:8.2px;height:16.2px;border-color:#e74c3c;" title="#132 document_p032_c00132 [h1]
1"><span class="lbl" style="background:#e74c3c;">#132&nbsp;h1</span></div><div class="bbox" style="left:123.9px;top:191.0px;width:96.7px;height:47.3px;border-color:#e67e22;" title="#133 document_p032_c00133 [h2]
Embedding
広く候補を取得 (Recall 重視
) BGE-M3 / E5"><span class="lbl" style="background:#e67e22;">#133&nbsp;h2</span></div><div class="bbox" style="left:428.1px;top:156.0px;width:8.2px;height:16.2px;border-color:#e74c3c;" title="#134 document_p032_c00134 [h1]
2"><span class="lbl" style="background:#e74c3c;">#134&nbsp;h1</span></div><div class="bbox" style="left:385.5px;top:191.0px;width:93.5px;height:37.6px;border-color:#e67e22;" title="#135 document_p032_c00135 [h2]
Reranker
上位を厳選 (Precision 重視 )
bge-reranker"><span class="lbl" style="background:#e67e22;">#135&nbsp;h2</span></div><div class="bbox" style="left:688.1px;top:161.3px;width:8.2px;height:16.2px;border-color:#e74c3c;" title="#136 document_p032_c00136 [h1]
3"><span class="lbl" style="background:#e74c3c;">#136&nbsp;h1</span></div><div class="bbox" style="left:652.1px;top:195.1px;width:81.0px;height:38.7px;border-color:#f1c40f;" title="#137 document_p032_c00137 [h3]
LLM 生成
精度の高い文脈で回答
( 幻覚抑制 )"><span class="lbl" style="background:#f1c40f;">#137&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 33 (5 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:21.9px;width:770.1px;height:208.9px;border-color:#f1c40f;" title="#138 document_p033_c00138 [h3]
OCR / Document AI 概要 スキャン品質と構造化要件に基づく手法選択
結論:スキャン品質やレイアウトの複雑度に応じて、 古典的 OCR と OCR-free ( Doc 理解) を使い分けるハイブリッド戦略が推奨されます。
代表的なシステム・モデル"><span class="lbl" style="background:#f1c40f;">#138&nbsp;h3</span></div><div class="bbox" style="left:65.5px;top:254.0px;width:270.3px;height:27.7px;border-color:#f1c40f;" title="#139 document_p033_c00139 [h3]
Tesseract OCR [64]
古典的 OCR + LSTM 。単純なテキスト化のデファクトスタンダード。"><span class="lbl" style="background:#f1c40f;">#139&nbsp;h3</span></div><div class="bbox" style="left:65.5px;top:298.4px;width:270.3px;height:26.6px;border-color:#f1c40f;" title="#140 document_p033_c00140 [h3]
PaddleOCR [66]
高精度かつ多機能。表構造や縦書きなど複雑なレイアウトに強い。"><span class="lbl" style="background:#f1c40f;">#140&nbsp;h3</span></div><div class="bbox" style="left:65.5px;top:342.7px;width:225.6px;height:27.7px;border-color:#f1c40f;" title="#141 document_p033_c00141 [h3]
Donut (OCR-free) [68]
画像を直接 Transformer で処理し、構造化データを抽出。"><span class="lbl" style="background:#f1c40f;">#141&nbsp;h3</span></div><div class="bbox" style="left:61.5px;top:217.1px;width:756.5px;height:276.7px;border-color:#f1c40f;" title="#142 document_p033_c00142 [h3]
LayoutLMv3 [69]
テキスト、画像、レイアウト情報を統合して文書理解を行う。
運用指針と使い分け
高品質スキャン・単純文書:
Tesseract を採用。高速かつ軽量で、単純なテキスト化に最適。
低品質・複雑レイアウト・構造化:
PaddleOCR 、 Donut 、 LayoutLMv3 を採用。傾きやノイズに強く、フォームや表の理解が 可能。
Hybrid Strategy Lay"><span class="lbl" style="background:#f1c40f;">#142&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 34 (5 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:756.7px;height:107.2px;border-color:#f1c40f;" title="#143 document_p034_c00143 [h3]
OCR / Document AI (完全版表) 主要モデル詳細比較: Tesseract / PaddleOCR / Donut / LayoutLMv3
代表モデル / 方式 方式 推奨実行形態 得意 / 不得意(要点) ランタイム相性"><span class="lbl" style="background:#f1c40f;">#143&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:176.8px;width:742.5px;height:30.9px;border-color:#f1c40f;" title="#144 document_p034_c00144 [h3]
Tesseract OCR Google [64]
古典 OCR + LSTM
テキスト化基盤 スキャン品質高なら CPU で十分
得意 長年の実績と安定性 不得意 複雑レイアウトや手書き文字は苦手
Cross-Platform: Win/Mac/Linux 問わず導入容易"><span class="lbl" style="background:#f1c40f;">#144&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:246.9px;width:738.9px;height:20.4px;border-color:#f1c40f;" title="#145 document_p034_c00145 [h3]
PaddleOCR Baidu [66]
Deep Learning OCR
高精度・多機能 構造化出力向け
得意 軽量かつ高精度な認識 特徴 表認識やレイアウト解析機能も充実
Python / ONNX: Python 環境で容易に実装可能"><span class="lbl" style="background:#f1c40f;">#145&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:316.9px;width:741.2px;height:30.9px;border-color:#f1c40f;" title="#146 document_p034_c00146 [h3]
Donut Clova AI [68]
OCR-free Transformer
Doc 理解・抽出 画像から直接 JSON 等へ
得意 OCR 誤りを経由せず直接情報抽出 特徴 フォームや請求書等の定型文書に強み
Transformers: Hugging Face Transformers 対応"><span class="lbl" style="background:#f1c40f;">#146&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:386.9px;width:790.1px;height:91.4px;border-color:#f1c40f;" title="#147 document_p034_c00147 [h3]
LayoutLMv3 Microsoft [69]
Multimodal Transformer
レイアウト保持 文脈理解統合
得意 視覚情報とテキスト情報を統合して理解 特徴 表 / 図 / レイアウト由来の欠落を補完 [69]
Transformers / ONNX: 推論エンジンでの最適化が可能
レイアウト保持の重要性: RAG において、単なるテキスト化では図表やレイアウト構造に含まれる情"><span class="lbl" style="background:#f1c40f;">#147&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 35 (7 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:17.4px;width:778.3px;height:403.6px;border-color:#f1c40f;" title="#148 document_p035_c00148 [h3]
OCR/Document AI 採用判断基準 リスク緩和のための UX 要件とハイブリッド構成
結論:誤読リスクをゼロにはできない前提で、前処理の標準化と「原文引用」による人間系確認の UX を要件化します。
前処理の標準化
品質の下限を担保
傾き補正、ノイズ除去、二値化を OCR 前段に固定的に組み込む ことで、認識精度のベースラインを確保。
画像の正規化
解像度やコントラストのバラつきを抑え、"><span class="lbl" style="background:#f1c40f;">#148&nbsp;h3</span></div><div class="bbox" style="left:362.8px;top:276.1px;width:460.3px;height:202.2px;border-color:#f1c40f;" title="#149 document_p035_c00149 [h3]
UX 要件と運用
原文スニペット引用
AI 回答の根拠となった原文箇所(画像切り出し等)を提示し、 ユーザーが誤読を検証できる UX を必須化。
夜間バッチへのオフロード
重い OCR/Embedding 処理は夜間に回し、日中のリソースは検索 と短い回答生成に集中させる。
Page 35 | ローカル AI 技術調査レポート"><span class="lbl" style="background:#f1c40f;">#149&nbsp;h3</span></div><div class="bbox" style="left:75.8px;top:154.9px;width:79.0px;height:63.2px;border-color:#e74c3c;" title="#150 document_p035_c00150 [h1]
1
品質評価
スキャン品質と レイアウト複雑度を確認"><span class="lbl" style="background:#e74c3c;">#150&nbsp;h1</span></div><div class="bbox" style="left:241.4px;top:154.9px;width:64.8px;height:63.2px;border-color:#e74c3c;" title="#151 document_p035_c00151 [h1]
2
前処理
傾き補正・二値化の 標準パイプライン化"><span class="lbl" style="background:#e74c3c;">#151&nbsp;h1</span></div><div class="bbox" style="left:401.4px;top:154.9px;width:62.2px;height:63.9px;border-color:#e74c3c;" title="#152 document_p035_c00152 [h1]
3
モデル選択
PaddleOCR( 構造化 ) +Donut( レイアウト )"><span class="lbl" style="background:#e74c3c;">#152&nbsp;h1</span></div><div class="bbox" style="left:554.6px;top:154.9px;width:71.9px;height:63.2px;border-color:#e74c3c;" title="#153 document_p035_c00153 [h1]
4
UX 要件化
原文スニペット表示で
誤読リスクを緩和"><span class="lbl" style="background:#e74c3c;">#153&nbsp;h1</span></div><div class="bbox" style="left:720.4px;top:154.9px;width:57.8px;height:63.2px;border-color:#e74c3c;" title="#154 document_p035_c00154 [h1]
5
運用設計
OCR/Embedding は 夜間バッチ処理へ"><span class="lbl" style="background:#e74c3c;">#154&nbsp;h1</span></div></div></div><div class="page-block"><h2>Page 36 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:56.4px;top:21.9px;width:768.6px;height:468.4px;border-color:#f1c40f;" title="#155 document_p036_c00155 [h3]
画像生成( Diffusion )概要 モデル・ランタイム・実務ポイント
結論:画像生成は SDXL を軸に、 ComfyUI/SD WebUI で運用。 FLUX 等はライセンス精査が必須であり、 VRAM 要件はワークフローに依存します。
主要モデルと UI ( Runtime )
高品質生成の標準モデル。ライセンス確認の上、低コスト構成の軸に。
[71]
ノードベース UI 。 Window"><span class="lbl" style="background:#f1c40f;">#155&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 37 (4 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:771.8px;height:196.3px;border-color:#5dade2;" title="#156 document_p037_c00156 [body]
画像生成(完全版表) 主要モデル・ UI 詳細比較: SDXL / ComfyUI / SD WebUI / FLUX
モデル / UI 方式 / 特徴 VRAM 要件 得意 / 不得意(要点) リスク / 採用判断
SDXL base 1.0 Stability AI [71]
Diffusion ワークフロー依存 (解像度・バッチ・ステップ数 に大きく左右される)
得意 高品質生成(ベースモデ"><span class="lbl" style="background:#5dade2;">#156&nbsp;body</span></div><div class="bbox" style="left:43.9px;top:266.6px;width:776.1px;height:30.9px;border-color:#f1c40f;" title="#157 document_p037_c00157 [h3]
ComfyUI UI [72]
ノード型 UI Win/Linux/macOS 対応
構成次第 (効率的なメモリ管理が可能)
得意 再利用性・資産化(ワークフロー 保存) 得意 seed 固定による再現性担保
判断 ワークフローのブラックボックス化を 防ぐため管理が必要"><span class="lbl" style="background:#f1c40f;">#157&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:346.5px;width:776.1px;height:29.8px;border-color:#f1c40f;" title="#158 document_p037_c00158 [h3]
Stable Diffusion WebUI UI [73]
プラグイン豊富 設定・拡張依存 得意 導入が比較的容易
注意 設定差による品質ブレが発生しや すい
判断 簡易利用には適するが、厳密な再現性 には注意"><span class="lbl" style="background:#f1c40f;">#158&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:412.6px;width:789.0px;height:65.7px;border-color:#f1c40f;" title="#159 document_p037_c00159 [h3]
FLUX.1 Black Forest Labs [75]
上位モデル系 高め (高品質ゆえのリソース要求)
得意 高い品質ポテンシャル Risk ライセンス(特に dev 版の条件)の精査 が必須 [76] 商用利用可否を必ず確認
採用判断の要点: VRAM 容量が最大の制約となります。解像度、バッチサイズ、ステップ数を固定したプロファイルを作成し、 VRAM 不足を防ぐ運用設計が重要です。特に"><span class="lbl" style="background:#f1c40f;">#159&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 38 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:5.4px;top:5.2px;width:572.0px;height:490.2px;border-color:#5dade2;" title="#160 document_p038_c00160 [body]
\\n\\n 採用判断基準(画像生成) \\n
VRAM 制約と再現性の担保
\\n 結論: VRAM 制約に合わせて解像度 / バッチ / ステップを固定し、ワークフローを資産化して再現性を担保します。 \\n
1
VRAM 制約
ハードウェア上限を まず確認
2
解像度固定
VRAM に収まる 最大サイズを決定
3
\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\"><span class="lbl" style="background:#5dade2;">#160&nbsp;body</span></div></div></div><div class="page-block"><h2>Page 39 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:22.9px;width:771.3px;height:113.0px;border-color:#f1c40f;" title="#161 document_p039_c00161 [h3]
Agent / Tool-use 概要 ローカル環境でのエージェント構築と実行
結論: Agent は「 LLM +ツール呼び出し(関数)+実行環境 」で構成され、 Ollama/LM Studio の OpenAI 互換 API を用いる いることで、ローカルでの実務的な構築が現実化しています。"><span class="lbl" style="background:#f1c40f;">#161&nbsp;h3</span></div><div class="bbox" style="left:61.9px;top:199.2px;width:347.6px;height:264.6px;border-color:#f1c40f;" title="#162 document_p039_c00162 [h3]
構成要素と API 基盤
エージェントの基本構成。 LLM が判断し、定義されたツール(関数)を 呼び出し、ローカル環境で実行して結果を返すループ構造。
Ollama や LM Studio は、ローカルで動作しながら OpenAI 互換のエンドポイ ポイントを提供。これにより、既存の Agent フレームワークやクライア ントツールをそのまま利用可能。
[23][24][41]
OpenHand"><span class="lbl" style="background:#f1c40f;">#162&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:200.6px;width:797.9px;height:278.0px;border-color:#e67e22;" title="#163 document_p039_c00163 [h2]
Function Calling (Tool Use)
ローカル API でも tool_choice や functions パラメータを利用可能。モデ ルが JSON 形式で引数を生成し、システム側で実行します。
[24][27]
ローカルモデル(特に小型)では JSON スキーマの破壊が発生しやすいた め、型定義の厳格化と、パース失敗時の再試行ロジックが不可欠です。
生成されたコードやコマン"><span class="lbl" style="background:#e67e22;">#163&nbsp;h2</span></div></div></div><div class="page-block"><h2>Page 40 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:32.4px;top:31.2px;width:793.8px;height:464.2px;border-color:#f1c40f;" title="#164 document_p040_c00164 [h3]
Agent/Tool-use (完全版表) OpenAI 互換 API / Function Calling / 実装基盤の詳細比較
項目・方式 詳細仕様・提供形態 利点・特徴 注意点・リスク
OpenAI 互換 API インターフェース [24,41]
提供: Ollama / LM Studio ローカル LLM を標準的な REST API として公開 Chat Completions AP"><span class="lbl" style="background:#f1c40f;">#164&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 41 (4 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:17.4px;width:774.5px;height:425.6px;border-color:#f1c40f;" title="#165 document_p041_c00165 [h3]
採用判断基準( Agent / Tool-use ) JSON スキーマ固定と検証ループによる安全運用
結論: JSON スキーマ固定+検証ループ(自動チェック)による安全運用を基本とし、 Ollama/LM Studio の OpenAI 互換 API で実装 を統一します。
スキーマと検証
JSON スキーマの厳格化 必須引数、型定義、 Enum 制約を厳密に 記述し、モデルの構造化出力能力を"><span class="lbl" style="background:#f1c40f;">#165&nbsp;h3</span></div><div class="bbox" style="left:324.4px;top:356.9px;width:222.7px;height:97.0px;border-color:#f1c40f;" title="#166 document_p041_c00166 [h3]
[24,41]
ランタイム機能の活用 必要に応じて、 llama-cpp-python 等のラ ンタイム固有機能(文法制約機能など)を補助的に活用。
[25]"><span class="lbl" style="background:#f1c40f;">#166&nbsp;h3</span></div><div class="bbox" style="left:102.1px;top:212.1px;width:720.1px;height:266.1px;border-color:#f1c40f;" title="#167 document_p041_c00167 [h3]
幻覚 API 対策
ホワイトリスト化 実行可能な関数セットを厳密にホワイトリ スト化し、存在しない API の呼び出し(幻覚)をシステム側で 遮断。
タイムアウトとリトライ 無限ループや応答遅延を防ぐため、 標準でタイムアウト設定とリトライ上限を設ける設計を義務化 。
Page 41 | ローカル AI 技術調査レポート
スキーマ定義
必須引数・型・制約を
厳格化して固定
ツール実行
Ollama"><span class="lbl" style="background:#f1c40f;">#167&nbsp;h3</span></div><div class="bbox" style="left:690.4px;top:212.1px;width:63.7px;height:36.1px;border-color:#f1c40f;" title="#168 document_p041_c00168 [h3]
完了 / リトライ
結果確認または フォールバック"><span class="lbl" style="background:#f1c40f;">#168&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 42 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:22.9px;width:774.4px;height:290.3px;border-color:#f1c40f;" title="#169 document_p042_c00169 [h3]
音声周辺( VAD/ 話者分離 / ウェイクワード / ノイズ除去)概要 カテゴリ別モデルカタログ
結論:前処理( VAD/ 話者分離 / ウェイクワード / ノイズ除去)が音声 UX の品質下限を決定します。 Silero VAD による無音除去と pyannote.audio による話者分離が実務上の標準構成です。
主要コンポーネント( Primary )
VAD ( Voice Activi"><span class="lbl" style="background:#f1c40f;">#169&nbsp;h3</span></div><div class="bbox" style="left:55.7px;top:194.8px;width:765.8px;height:152.7px;border-color:#f1c40f;" title="#170 document_p042_c00170 [h3]
Silero VAD pyannote.audio
補助コンポーネント( Secondary )
ウェイクワード( Wake Word ): openWakeWord [85]
「 Hey Siri 」等の起動語検出。オンデバイスで動作し、誤検知( False Positive )とのトレー ドオフ設計が鍵。
ノイズ除去( Noise Suppression ): RNNoise [14]
RNN"><span class="lbl" style="background:#f1c40f;">#170&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:339.1px;width:797.9px;height:139.5px;border-color:#f1c40f;" title="#171 document_p042_c00171 [h3]
openWakeWord RNNoise
設計含意( Design Implication )
これら前処理の設定差(閾値、モデルバージョン)で最終的な認識精度が大きく変動します。ベンチマーク時は「同一前処理条件」を固定することが再現性の担保に不可欠です。
Local AI Technical Survey Report 2026 42 / 80"><span class="lbl" style="background:#f1c40f;">#171&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 43 (4 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:757.7px;height:105.7px;border-color:#5dade2;" title="#172 document_p043_c00172 [body]
音声周辺(完全版表) Silero VAD / pyannote / openWakeWord / RNNoise 詳細比較
代表モデル / ツール 役割 実行要件 / 形式 利点 / 特徴 リスク / 注意点"><span class="lbl" style="background:#5dade2;">#172&nbsp;body</span></div><div class="bbox" style="left:43.9px;top:173.8px;width:774.3px;height:87.5px;border-color:#f1c40f;" title="#173 document_p043_c00173 [h3]
Silero VAD Snakers4 [82]
無音 / 雑音除去 (Voice Activity Detection)
軽量・ CPU 動作可 ONNX / PyTorch
標準 前処理のデファクト ASR 負荷を大幅に削減
Attention 閾値設定に依存(誤って語 語頭を切るリスクあり)
pyannote.audio HuggingFace [84]
話者分離 (Speaker Diar"><span class="lbl" style="background:#f1c40f;">#173&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:307.9px;width:769.9px;height:29.9px;border-color:#f1c40f;" title="#174 document_p043_c00174 [h3]
openWakeWord dscripka [85]
ウェイクワード検出 (Wake Word Detection)
ローカル動作 CPU / tflite 等
実用 カスタムウェイクワード作成が可 能 非常に軽量
Attention 誤検知( False Positive )の の調整が必要"><span class="lbl" style="background:#f1c40f;">#174&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:375.0px;width:786.9px;height:103.3px;border-color:#f1c40f;" title="#175 document_p043_c00175 [h3]
RNNoise Xiph.Org [14]
ノイズ除去 (Noise Suppression)
極めて軽量 C/C++ / WebAssembly
高速 リアルタイム処理向き RNN ベースで背景雑音を抑制
Risk 音質変化(声がロボットっぽく なる等)のリスク
設計の推奨:会議系ワークフローでは「 VAD → ASR → 要約」の直列処理を基本とし、負荷の高い話者分離( pyannote 等)"><span class="lbl" style="background:#f1c40f;">#175&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 44 (6 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:17.4px;width:779.7px;height:460.9px;border-color:#f1c40f;" title="#176 document_p044_c00176 [h3]
採用判断基準(音声周辺) 前処理パイプライン標準化と品質管理
結論:前処理パイプラインを標準化し、閾値・モデルバージョン・ RTF 目標を固定することで、 UX の品質下限を担保します。
パイプラインの標準化
基準設定のプリセット化
VAD 閾値、話者分離の有無、ノイズ除去の有無をユースケース ごとに固定セットとして定義。
再現性の担保
モデルバージョンとパラメータをコードで固定し、環境によ る挙"><span class="lbl" style="background:#f1c40f;">#176&nbsp;h3</span></div><div class="bbox" style="left:79.5px;top:154.9px;width:71.9px;height:63.2px;border-color:#e74c3c;" title="#177 document_p044_c00177 [h1]
1
ユースケース定義
会議録音か ウェイクワードか"><span class="lbl" style="background:#e74c3c;">#177&nbsp;h1</span></div><div class="bbox" style="left:237.8px;top:154.9px;width:71.9px;height:63.2px;border-color:#e74c3c;" title="#178 document_p044_c00178 [h1]
2
パイプライン構成
VAD+ASR+ 分離など
プリセット化"><span class="lbl" style="background:#e74c3c;">#178&nbsp;h1</span></div><div class="bbox" style="left:400.9px;top:154.9px;width:63.2px;height:63.2px;border-color:#e74c3c;" title="#179 document_p044_c00179 [h1]
3
閾値固定
VAD 感度・分離閾値
を数値で管理"><span class="lbl" style="background:#e74c3c;">#179&nbsp;h1</span></div><div class="bbox" style="left:559.3px;top:154.9px;width:63.0px;height:63.2px;border-color:#e74c3c;" title="#180 document_p044_c00180 [h1]
4
バージョン固定
Silero/pyannote の
バージョン統一"><span class="lbl" style="background:#e74c3c;">#180&nbsp;h1</span></div><div class="bbox" style="left:720.3px;top:154.9px;width:57.7px;height:63.2px;border-color:#e74c3c;" title="#181 document_p044_c00181 [h1]
5
ログ監視
RTF ・エラー率の 継続モニタリング"><span class="lbl" style="background:#e74c3c;">#181&nbsp;h1</span></div></div></div><div class="page-block"><h2>Page 45 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:54.0px;top:101.5px;width:89.8px;height:90.5px;border-color:#e74c3c;" title="#182 document_p045_c00182 [h1]
07"><span class="lbl" style="background:#e74c3c;">#182&nbsp;h1</span></div><div class="bbox" style="left:54.0px;top:236.2px;width:244.7px;height:132.8px;border-color:#f1c40f;" title="#183 document_p045_c00183 [h3]
メモリ設計の コア
重み量子化・ KV キャッシュ管理と 最適化技術"><span class="lbl" style="background:#f1c40f;">#183&nbsp;h3</span></div><div class="bbox" style="left:443.0px;top:68.9px;width:352.0px;height:346.1px;border-color:#f1c40f;" title="#184 document_p045_c00184 [h3]
KE Y TAKEAWAYS
本章の要点
重みメモリの現実解:
4bit 量子化( weight-only )が基本。理論値+ 10% オーバーヘ ッドで設計します。
KV キャッシュの支配性:
長文コンテキストでは KV がメモリを圧迫。 vLLM の paged/quantized KV [5,6] や TensorRT-LLM の KV reuse [7] が必須で す。
最適化技術の活用:"><span class="lbl" style="background:#f1c40f;">#184&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 46 (4 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:21.9px;width:769.9px;height:124.3px;border-color:#5dade2;" title="#185 document_p046_c00185 [body]
メモリ設計のコア(概要) 重み量子化と KV キャッシュの支配性
結論:ローカル AI のメモリ制約は、 「重み 4bit 量子化」 と 「 KV キャッシュ最適化」 の 2 点によって決定さ れる支配的な要因です。"><span class="lbl" style="background:#5dade2;">#185&nbsp;body</span></div><div class="bbox" style="left:62.1px;top:217.1px;width:342.5px;height:171.4px;border-color:#f1c40f;" title="#186 document_p046_c00186 [h3]
重みメモリ( Weights )
パラメータ数と bit 幅で物理的な下限が決まります。
メモリ ≈ パラメータ数 × bit 幅 ÷ 8 ( + 約 10% ラ
ンタイム /CUDA オーバーヘッド)
FP16 ( 16bit )と比較して約 1/4 のサイズで、品質劣化を最小限に抑えつ つコンシューマ GPU に載せるための必須技術です。"><span class="lbl" style="background:#f1c40f;">#186&nbsp;h3</span></div><div class="bbox" style="left:62.4px;top:413.8px;width:169.2px;height:9.1px;border-color:#f1c40f;" title="#187 document_p046_c00187 [h3]
GGUF Q4_K_M AWQ 4bit GPTQ"><span class="lbl" style="background:#f1c40f;">#187&nbsp;h3</span></div><div class="bbox" style="left:62.8px;top:217.1px;width:748.3px;height:277.1px;border-color:#f1c40f;" title="#188 document_p046_c00188 [h3]
KV キャッシュと最適化
KV キャッシュは「コンテキスト長に比例」して増加します。長文・多 同時接続では重み以上にメモリを圧迫します。
例: Llama3 8B (n_layer=32, n_head_kv=8, head_dim=128)
Paged KV: メモリ断片化を防ぐ( vLLM 等) [5]
Quantized KV: FP8/INT4 化で容量削減 [6]
KV Reuse: 計"><span class="lbl" style="background:#f1c40f;">#188&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 47 (6 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:784.0px;height:96.2px;border-color:#f1c40f;" title="#189 document_p047_c00189 [h3]
重みメモリ理論値(完全版表 1 : 0.5B 〜 14B ) モデル規模別推奨メモリ容量( +10% オーバーヘッド込み概算)
モデル規模 (Parameters) 4bit (Q4_K_M 等 )
ローカル推奨
INT8 (8bit) 標準的量子化
FP16 (16bit) 元精度 / 学習時"><span class="lbl" style="background:#f1c40f;">#189&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:151.6px;width:701.6px;height:11.0px;border-color:#f1c40f;" title="#190 document_p047_c00190 [h3]
0.5B (Qwen2.5-0.5B 等 ) 0.3 GiB 0.5 GiB 1.0 GiB"><span class="lbl" style="background:#f1c40f;">#190&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:189.0px;width:701.6px;height:11.0px;border-color:#f1c40f;" title="#191 document_p047_c00191 [h3]
1.5B (Qwen2.5-1.5B 等 ) 0.8 GiB 1.5 GiB 3.1 GiB"><span class="lbl" style="background:#f1c40f;">#191&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:226.3px;width:701.6px;height:11.0px;border-color:#f1c40f;" title="#192 document_p047_c00192 [h3]
3B (Phi-3 Mini 等 ) 1.5 GiB 3.1 GiB 6.1 GiB"><span class="lbl" style="background:#f1c40f;">#192&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:263.7px;width:704.4px;height:11.0px;border-color:#f1c40f;" title="#193 document_p047_c00193 [h3]
7B (Qwen2.5-7B 等 ) 3.6 GiB 7.2 GiB 14.3 GiB"><span class="lbl" style="background:#f1c40f;">#193&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:301.1px;width:793.7px;height:177.2px;border-color:#f1c40f;" title="#194 document_p047_c00194 [h3]
8B (Llama 3.1 8B 等 ) 4.1 GiB 8.2 GiB 16.4 GiB
14B (Qwen2.5-14B/Phi-4 等 ) 7.2 GiB 14.3 GiB 28.7 GiB
計算根拠:上記数値は「パラメータ数 × bit 幅 ÷ 8 」に、ランタイムオーバヘッドとして約 10% を加算した概算理論値です。これらは重みのみのメモリ消費であり、実運用ではこれに加えて KV キャ"><span class="lbl" style="background:#f1c40f;">#194&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 48 (5 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:49.3px;top:31.2px;width:772.3px;height:98.9px;border-color:#f1c40f;" title="#195 document_p048_c00195 [h3]
重みメモリ理論値(完全版表 2 : 27B 〜 70B ) 大規模モデル( 4bit/INT8/FP16 )のメモリ要件
モデル規模 4bit (Q4) 推奨 IN T8 (Q8) FP16 (Half)"><span class="lbl" style="background:#f1c40f;">#195&nbsp;h3</span></div><div class="bbox" style="left:49.3px;top:166.7px;width:565.8px;height:20.3px;border-color:#f1c40f;" title="#196 document_p048_c00196 [h3]
27B Gemma 2 27B 等 13.8 GiB 27.7 GiB 55.3 GiB"><span class="lbl" style="background:#f1c40f;">#196&nbsp;h3</span></div><div class="bbox" style="left:49.3px;top:218.5px;width:565.8px;height:20.3px;border-color:#f1c40f;" title="#197 document_p048_c00197 [h3]
32B Qwen2.5 32B 等 16.4 GiB 32.8 GiB 65.6 GiB"><span class="lbl" style="background:#f1c40f;">#197&nbsp;h3</span></div><div class="bbox" style="left:49.3px;top:270.3px;width:565.8px;height:20.3px;border-color:#f1c40f;" title="#198 document_p048_c00198 [h3]
34B Yi-34B 等 17.4 GiB 34.8 GiB 69.7 GiB"><span class="lbl" style="background:#f1c40f;">#198&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:322.1px;width:788.8px;height:156.2px;border-color:#f1c40f;" title="#199 document_p048_c00199 [h3]
70B Llama 3 70B 等 35.9 GiB 71.7 GiB 143.4 GiB
計算前提:パラメータ数 × bit 幅 ÷ 8 で基本容量を算出後、実運用におけるランタイムオーバヘッド等を考慮して約 +10% を加算した概算値です。
設計含意: 30B 級モデルの実運用には、 4bit 量子化でも約 16-18GiB の VRAM/ 統合メモリが必要です。 70B 級では 4bit で"><span class="lbl" style="background:#f1c40f;">#199&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 49 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:32.4px;top:31.2px;width:786.1px;height:447.0px;border-color:#f1c40f;" title="#200 document_p049_c00200 [h3]
KV キャッシュの支配性(完全版表 1 ) 2k 〜 32k tokens におけるメモリ消費量比較( Llama3 8B 相当)
コンテキスト長 (Tokens)
FP16 ( 標準 ) (16-bit)
FP8 ( 量子化 ) (8-bit)
INT4 ( 量子化 ) (4-bit)
2,048 tokens 標準的な短文対話 0.25 GiB 0.12 GiB 0.06 GiB
8,192 "><span class="lbl" style="background:#f1c40f;">#200&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 50 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:46.5px;top:31.2px;width:772.2px;height:139.9px;border-color:#f1c40f;" title="#201 document_p050_c00201 [h3]
KV キャッシュの支配性(完全版表 2 :超長文域) コンテキスト長 131k tokens におけるメモリ消費量比較( Llama3 8B 相当)
コンテキスト長 FP16 (Base) FP8 (Optimized) INT4 (Highly Optimized)
131,072 tokens 128k context 16.00 GiB Critical モデル本体 (8B) と合わせると "><span class="lbl" style="background:#f1c40f;">#201&nbsp;h3</span></div><div class="bbox" style="left:446.3px;top:148.7px;width:71.9px;height:22.4px;border-color:#f1c40f;" title="#202 document_p050_c00202 [h3]
8.00 GiB Heavy 50% 削減(実用域へ)"><span class="lbl" style="background:#f1c40f;">#202&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:148.7px;width:785.4px;height:329.6px;border-color:#f1c40f;" title="#203 document_p050_c00203 [h3]
4.00 GiB Manageable 75% 削減(余裕あり)
メモリ内訳の逆転現象
超長文( 128k 等)では、 KV キャッシュのメモリ消費量がモデル本体(重み)のメモリ消費 量を上回る現象が発生します。
例: Llama3 8B ( 4bit 重み ≒ 4.1GiB )に対し、 131k tokens の KV ( FP16 )は 16.00GiB に達し、総 メモリの約 80% を K"><span class="lbl" style="background:#f1c40f;">#203&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 51 (4 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:27.0px;top:17.7px;width:770.7px;height:18.8px;border-color:#5dade2;" title="#204 document_p051_c00204 [body]
メモリ最適化技術 paged KV / quantized KV / KV reuse / prompt cache"><span class="lbl" style="background:#5dade2;">#204&nbsp;body</span></div><div class="bbox" style="left:91.1px;top:111.0px;width:146.7px;height:15.8px;border-color:#f1c40f;" title="#205 document_p051_c00205 [h3]
KV 構造と管理の最適化"><span class="lbl" style="background:#f1c40f;">#205&nbsp;h3</span></div><div class="bbox" style="left:66.8px;top:167.3px;width:335.7px;height:55.8px;border-color:#f1c40f;" title="#206 document_p051_c00206 [h3]
Paged KV Cache (vLLM) [5]
メモリを固定サイズのページ単位で管理し、断片化を抑制する技術。 OS の仮想メモリと同様の仕組みで、 GPU メモリの利用効率を劇的に向 上させ、スループットを高めます。"><span class="lbl" style="background:#f1c40f;">#206&nbsp;h3</span></div><div class="bbox" style="left:66.8px;top:111.0px;width:755.0px;height:367.2px;border-color:#f1c40f;" title="#207 document_p051_c00207 [h3]
Quantized KV Cache [6]
KV キャッシュを標準の FP16 から FP8 や INT4 へ量子化。精度劣化を最小 限に抑えつつメモリフットプリントを 50 〜 75% 削減し、より長いコン テキストや大きなバッチサイズでの推論を可能にします。
再利用とレイテンシ短縮
KV Cache Reuse (TensorRT-LLM) [7]
共通のプレフィックス(システムプロンプトや"><span class="lbl" style="background:#f1c40f;">#207&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 52 (4 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:22.9px;width:772.6px;height:113.0px;border-color:#f1c40f;" title="#208 document_p052_c00208 [h3]
Tier 定義と現実ライン(概要) ハードウェア別の実用ライン定義
結論: Tier は「重み( 4bit )+ KV ( 4k 〜 8k )+オーバーヘッド」を前提に定義。 理論計算値 をベースとしつつ、最終判断は 実機再現ベンチ で行う設計です。"><span class="lbl" style="background:#f1c40f;">#208&nbsp;h3</span></div><div class="bbox" style="left:74.9px;top:196.8px;width:163.6px;height:223.0px;border-color:#f1c40f;" title="#209 document_p052_c00209 [h3]
Apple Silicon (UMA)
16GB (A-16) 3B 〜 7/8B (4bit)
24-32GB (A-24/32) 7B 〜 14B (4bit)
64GB (A-64) 14B 〜 32B (4bit)
96-128GB (A-96+) 32B 〜 70B (4bit)"><span class="lbl" style="background:#f1c40f;">#209&nbsp;h3</span></div><div class="bbox" style="left:346.9px;top:196.8px;width:170.4px;height:223.0px;border-color:#f1c40f;" title="#210 document_p052_c00210 [h3]
Windows GPU (VRAM)
VRAM 8GB (G-8) 7B (4bit) ※短文中心
VRAM 12GB (G-12) 7B 〜 14B (4bit)
VRAM 16GB (G-16) 14B 〜 27B (4bit)
VRAM 24GB (G-24) 27B 〜 34B (4bit)"><span class="lbl" style="background:#f1c40f;">#210&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:196.8px;width:797.9px;height:291.3px;border-color:#f1c40f;" title="#211 document_p052_c00211 [h3]
Windows CPU-only
RAM 32GB (W-CPU1) 3B 〜 7B (4bit)
7B 級が「実用的に動く」境界線。 AVX 命令等の最適化が必須。 速度は GPU 比で大幅劣後。
実務上の注意点
Local AI Technical Survey Report 2026 52 / 80"><span class="lbl" style="background:#f1c40f;">#211&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 53 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:761.6px;height:130.3px;border-color:#f1c40f;" title="#212 document_p053_c00212 [h3]
Apple Silicon Tier (完全版表 1 ) 実用ライン定義: A-16 / A-24/32 (Unified Memory Architecture)
Tier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針"><span class="lbl" style="background:#f1c40f;">#212&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:227.4px;width:775.2px;height:56.4px;border-color:#e67e22;" title="#213 document_p053_c00213 [h2]
A-16 Entry
Apple Silicon 16GB (M1/M2/M3/M4 等 ) [1,11,92]
3B 〜 7/8B (4bit 量子化 )
条件付 “ 短文中心 ” なら可
軽量構成 ASR/TTS は小さめなら可
文脈長を抑え、 RAG は Embedding 先計算 算。 Apple 公式の Unified Memory Architecture (UMA) 特性を理解し、シ"><span class="lbl" style="background:#e67e22;">#213&nbsp;h2</span></div><div class="bbox" style="left:32.4px;top:342.8px;width:788.8px;height:135.5px;border-color:#e67e22;" title="#214 document_p053_c00214 [h2]
A-24/32 Standard
Apple Silicon 24 〜 32GB (M3/M4 Pro/Max 等 ) [93]
7B 〜 14B (4bit 量子化 )
可 (業務最小ライン)
成立 多くのユースケースで 実用可
14B 級を軸に、 VLM は 2B 〜小型を併用。 。 「 RAG 含め業務用途の最小実用ライン」 になりやすい。 M4 世代でも統合メモリ 前提で構成されるため、 "><span class="lbl" style="background:#e67e22;">#214&nbsp;h2</span></div></div></div><div class="page-block"><h2>Page 54 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:781.9px;height:135.0px;border-color:#f1c40f;" title="#215 document_p054_c00215 [h3]
Apple Silicon Tier (完全版表 2 ) A-64 / A-96/128 :高品質ローカルの到達点と運用指針
Tier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針"><span class="lbl" style="background:#f1c40f;">#215&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:233.6px;width:777.8px;height:83.1px;border-color:#f1c40f;" title="#216 document_p054_c00216 [h3]
A-64 High-End
Apple 64GB (M1 Max 等 ) [1, 92]
14B 〜 32B (4bit)
可 余裕あり LLM+VLM 等
長文会議は要約で圧縮し、 KV 最適化を活用。 M1 Max 等で 64GB 構成が公式に提示されており、 32B 級モデルの実用的なスイートスポット。 32B モデル( 4bit )で約 16-18GB 消費、残りで VLM/ASR/OS "><span class="lbl" style="background:#f1c40f;">#216&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:359.2px;width:789.6px;height:119.1px;border-color:#f1c40f;" title="#217 document_p054_c00217 [h3]
A-96/128 Ultra
Apple 96 〜 128GB (M2/M3/M4 Max/Ultra) [93, 94]
32B 〜 70B (4bit)
可 ( 重い )
構成次第 高品質 RAG 等
“ 高品質ローカル ” の現実ライン。 70B 級( 4bit で約 40GB )をロードしても、まだ 50GB 以上の余裕がある圧倒的なメモリ空間。 M3/M4 で最大 128GB 構成が公式"><span class="lbl" style="background:#f1c40f;">#217&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 55 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:43.9px;top:31.2px;width:777.0px;height:225.2px;border-color:#f1c40f;" title="#218 document_p055_c00218 [h3]
Windows Tier (完全版表 1 ) W-CPU1 / W-CPU2 / G-8 / G-12 の現実ライン
Tier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針
W-CPU1 CPU-only 8C/16T 級 + 32GB RAM
3B 〜 7B (4bit) 条件付き 負荷高
厳しい オンデマンド推奨
CPU 最適化命令( AVX-512/VNNI/AMX 等)"><span class="lbl" style="background:#f1c40f;">#218&nbsp;h3</span></div><div class="bbox" style="left:43.9px;top:300.7px;width:771.4px;height:20.5px;border-color:#f1c40f;" title="#219 document_p055_c00219 [h3]
G-8 NVIDIA GPU VRAM 8GB
7B 中心 (4bit) 可 短文に限る
軽量なら可 画像生成 SDXL 等は設定依存で厳しいため ベンチ。 7B 級 LLM 単体なら快適に動作。"><span class="lbl" style="background:#f1c40f;">#219&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:365.3px;width:791.0px;height:113.0px;border-color:#f1c40f;" title="#220 document_p055_c00220 [h3]
G-12 NVIDIA GPU VRAM 12GB
7B 〜 14B (4bit) 可 現実化 14B 級が視野に入る。 VLM 2B 〜 7B 級や GPU 版を同時に載せやすく、実用性が高い 。
最適化のポイント: Windows の CPU 推論は Intel 拡張( IPEX )や OpenVINO 等の最適化が効く場合があります。 GPU ( G-8/12 )では、 VRAM 不足時にシ"><span class="lbl" style="background:#f1c40f;">#220&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 56 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:32.4px;top:31.2px;width:786.0px;height:447.0px;border-color:#f1c40f;" title="#221 document_p056_c00221 [h3]
Windows Tier (完全版表 2 ) 高品質・実務用: G-16 (16GB) / G-24 (24GB) 定義
Tier 想定ハード 現実的な LLM 規模 常駐可否 / 同時実行 設計指針
G-16 VRAM 16GB GeForce 4080 Laptop RTX 4070 Ti Super 等
14B 〜 27B (4bit 量子化 )
常駐 : 可 同時 : 高品質寄り成立 AS"><span class="lbl" style="background:#f1c40f;">#221&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 57 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:21.9px;width:773.2px;height:124.3px;border-color:#f1c40f;" title="#222 document_p057_c00222 [h3]
補足: AMD/Intel GPU の現実ライン 公式最適化とエコシステムの現状
結論: AMD/Intel 環境でも公式最適化による選択肢が存在しますが、運用は実装・ドライバ依存となり、 個別検証が必要です。"><span class="lbl" style="background:#f1c40f;">#222&nbsp;h3</span></div><div class="bbox" style="left:57.4px;top:218.6px;width:347.8px;height:253.2px;border-color:#e67e22;" title="#223 document_p057_c00223 [h2]
AMD Radeon GPU (ROCm)
ROCm HIP SDK ZLUDA (Deprecated)
ROCm の Windows サポートが進展しており、一部の WSL2 環境やネイティ ブ Windows での動作が可能になりつつあります。 [97]
llama.cpp の HIP BLAS バックエンドや、 MLC LLM などが AMD GPU をサポー ト。 VRAM 容量あたりの"><span class="lbl" style="background:#e67e22;">#223&nbsp;h2</span></div><div class="bbox" style="left:472.7px;top:218.6px;width:348.8px;height:235.9px;border-color:#f1c40f;" title="#224 document_p057_c00224 [h3]
Intel Arc / iGPU (OpenVINO/SYCL)
OpenVINO IPEX SYCL / oneAPI
PyTorch 拡張として XPU ( Intel GPU )サポートを提供。 Arc GPU での推論 加速が可能。 [13]
llama.cpp の SYCL バックエンドや OpenVINO 最適化により、 iGPU ( Core Ultra 等)を含めた幅広いハードウェ"><span class="lbl" style="background:#f1c40f;">#224&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 58 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:27.0px;top:17.7px;width:801.3px;height:460.6px;border-color:#f1c40f;" title="#225 document_p058_c00225 [h3]
常駐運用 vs オンデマンド運用 TTFT (初速)、安定性、リソース効率のトレードオフ
基本運用モデルの比較
常駐( Always-on ) 推奨 : チャット Bot
モデルをメモリに保持し、 Prompt Cache を維持。
利点: TTFT が最短、応答が安定。
欠点:メモリを常時占有(他アプリと競合)。
オンデマンド( On-demand ) 推奨 : 翻訳 / 要約
リクエスト時のみ"><span class="lbl" style="background:#f1c40f;">#225&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 59 (10 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:6.5px;width:782.0px;height:184.9px;border-color:#f1c40f;" title="#226 document_p059_c00226 [h3]
ユースケース別推奨スタック:音声メモ → 整文化 → タスク抽出
用途に応じて「低コスト/高品質/ハード制約」の 3 構 成を選択
結論: ASR 精度と LLM 推論能力のバランスで構成を決定します。 ※誤転記防止には VAD (無音除去)が必須。タスク抽出には JSON スキーマ固定 が有効です。
低コスト構成"><span class="lbl" style="background:#f1c40f;">#226&nbsp;h3</span></div><div class="bbox" style="left:54.2px;top:223.1px;width:216.5px;height:46.5px;border-color:#f1c40f;" title="#227 document_p059_c00227 [h3]
Target HW
A-16 / W-CPU1 / G-8 想定 (Apple 16GB / CPU / VRAM
8GB)"><span class="lbl" style="background:#f1c40f;">#227&nbsp;h3</span></div><div class="bbox" style="left:61.5px;top:291.0px;width:198.9px;height:108.7px;border-color:#f1c40f;" title="#228 document_p059_c00228 [h3]
VAD &amp; ASR
Silero VAD ( 無音除去 ) faster-whisper (CPU/INT8)
LLM 整形・抽出
7B 級 (Qwen2.5-7B 等 ) 4bit (GGUF 系 )"><span class="lbl" style="background:#f1c40f;">#228&nbsp;h3</span></div><div class="bbox" style="left:49.9px;top:177.8px;width:430.2px;height:289.9px;border-color:#f1c40f;" title="#229 document_p059_c00229 [h3]
Output Interface
JSON (タスク配列) Ollama/LM Studio (OpenAI 互換 )
高品質構成"><span class="lbl" style="background:#f1c40f;">#229&nbsp;h3</span></div><div class="bbox" style="left:318.7px;top:223.1px;width:227.2px;height:40.8px;border-color:#f1c40f;" title="#230 document_p059_c00230 [h3]
Target HW
A-64+ / G-16+ / G-24 想定 (Apple 64GB+ / VRAM 16GB+)"><span class="lbl" style="background:#f1c40f;">#230&nbsp;h3</span></div><div class="bbox" style="left:332.5px;top:291.0px;width:199.3px;height:108.7px;border-color:#f1c40f;" title="#231 document_p059_c00231 [h3]
VAD &amp; ASR
faster-whisper (GPU) 必要なら大型 Whisper 系列
LLM 整形・抽出
14B 〜 32B 級 (Phi-4/Qwen 上位 ) 4bit / GPU 実行"><span class="lbl" style="background:#f1c40f;">#231&nbsp;h3</span></div><div class="bbox" style="left:323.9px;top:177.8px;width:441.0px;height:289.9px;border-color:#f1c40f;" title="#232 document_p059_c00232 [h3]
Output Interface
Function Calling 前提 Schema 固定で JSON 破壊率低減
ハード制約構成"><span class="lbl" style="background:#f1c40f;">#232&nbsp;h3</span></div><div class="bbox" style="left:592.6px;top:223.1px;width:221.8px;height:46.5px;border-color:#f1c40f;" title="#233 document_p059_c00233 [h3]
Target HW
W-CPU1 / A-16 ( ギリギリ ) ( 低スペック PC / メモリ不
足 )"><span class="lbl" style="background:#f1c40f;">#233&nbsp;h3</span></div><div class="bbox" style="left:591.8px;top:291.0px;width:223.1px;height:108.7px;border-color:#f1c40f;" title="#234 document_p059_c00234 [h3]
VAD &amp; ASR
短音声限定 or 夜間バッチ処理リアルタイム性は犠
牲にする
LLM 整形・抽出
3B 〜 4B 級 (Phi-3 mini 等 ) 「抽出のみ」に機能限定"><span class="lbl" style="background:#f1c40f;">#234&nbsp;h3</span></div><div class="bbox" style="left:603.3px;top:426.9px;width:200.0px;height:39.7px;border-color:#f1c40f;" title="#235 document_p059_c00235 [h3]
Output Interface
抽出テンプレート固定自由生成を極力減らす"><span class="lbl" style="background:#f1c40f;">#235&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 60 (6 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:56.4px;top:21.9px;width:771.1px;height:118.0px;border-color:#f1c40f;" title="#236 document_p060_c00236 [h3]
音声メモ → 整文化 → タスク抽出(失敗モードと回避策) ユースケース別推奨スタック
結論:誤転記・要点漏れ・推論不足は「設計」で抑え込む。 VAD による区間分割、逐次要約、テンプレ固定が鍵となります。"><span class="lbl" style="background:#f1c40f;">#236&nbsp;h3</span></div><div class="bbox" style="left:62.6px;top:202.7px;width:215.7px;height:67.0px;border-color:#f1c40f;" title="#237 document_p060_c00237 [h3]
ASR 誤転記 → 抽出ミス
失敗モード
ASR が固有名詞や数値を誤認識し、そのままタスクと して抽出されてしまう。"><span class="lbl" style="background:#f1c40f;">#237&nbsp;h3</span></div><div class="bbox" style="left:54.6px;top:202.7px;width:489.0px;height:190.2px;border-color:#f1c40f;" title="#238 document_p060_c00238 [h3]
回避策( Design )
1. VAD で区間分割:無音除去で認識精度向上。 2. 重要箇所再確認 UI :抽出されたタスクの元音声を ワンクリック再生できる UI を提供。
Silero VAD Playback UI
長文での要点漏れ
失敗モード
コンテキスト長超過や Attention の分散により、後半 の重要事項が無視される。"><span class="lbl" style="background:#f1c40f;">#238&nbsp;h3</span></div><div class="bbox" style="left:328.6px;top:289.8px;width:220.4px;height:103.0px;border-color:#f1c40f;" title="#239 document_p060_c00239 [h3]
回避策( Design )
1. 逐次要約( Rolling Summary ):一定区間ごとに要 約し、次区間の入力に含める。
2. KV 節約:量子化 KV やウインドウ制限でメモリ枯渇 を防ぐ。
Rolling Context Quantized KV"><span class="lbl" style="background:#f1c40f;">#239&nbsp;h3</span></div><div class="bbox" style="left:606.5px;top:202.7px;width:213.6px;height:67.0px;border-color:#f1c40f;" title="#240 document_p060_c00240 [h3]
LLM の推論不足
失敗モード
小型モデル( 3B-7B )が指示に従わず、タスク以外の 雑談や幻覚を出力する。"><span class="lbl" style="background:#f1c40f;">#240&nbsp;h3</span></div><div class="bbox" style="left:67.5px;top:289.8px;width:754.6px;height:185.0px;border-color:#f1c40f;" title="#241 document_p060_c00241 [h3]
回避策( Design )
1. テンプレ固定:自由生成を禁止し、抽出テンプレ ート( JSON Schema 等)を強制。
2. Few-shot 提示:プロンプトに抽出成功例を含める 。
JSON Schema Few-shot
システム連携のポイント
Ollama/LM Studio の OpenAI 互換 API を活用して業務アプリと接続する場合、受け取った JSON 出力の型チェック("><span class="lbl" style="background:#f1c40f;">#241&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 61 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:32.4px;top:19.5px;width:797.9px;height:459.1px;border-color:#5dade2;" title="#242 document_p061_c00242 [body]
ユースケース:会議議事録(話者分離+要約+アクション) 推奨スタック 3 構成( Tier 別)
結論:規模と要件に応じて 3 構成を選択。無音除去( VAD )と発話ターン統合ルールが品質下限を決定します。 ※評価は p50/p95 、 RTF 、意味改変率で監視することを推奨します。
低コスト構成
前処理 (VAD)
Silero VAD [82] 無音区間を確実に除去
[82]
ASR ( "><span class="lbl" style="background:#5dade2;">#242&nbsp;body</span></div></div></div><div class="page-block"><h2>Page 62 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:21.9px;width:775.8px;height:116.9px;border-color:#f1c40f;" title="#243 document_p062_c00243 [h3]
会議議事録:失敗モードと回避策 ユースケース詳細分析
結論:話者誤割当・ ASR 幻覚・長時間処理の負荷を設計で抑止。 VAD (無音除去)の厳格化とタイムスタンプベースの検証が品質の防波堤となります。"><span class="lbl" style="background:#f1c40f;">#243&nbsp;h3</span></div><div class="bbox" style="left:66.6px;top:205.2px;width:344.6px;height:200.7px;border-color:#f1c40f;" title="#244 document_p062_c00244 [h3]
主な失敗モード( Failure Modes )
話者誤割当( Diarization Error ) 高頻度
発話の切れ目が不明確で、 A さんの発言が B さんとして記録される。特に割り 込み発話で多発。
ASR 幻覚( Hallucination ) 致命的
無音区間やノイズに対して、存在しない文章(「ご視聴ありがとうございま した」等)を生成してしまう現象。
長文による要点漏れ
コンテキス"><span class="lbl" style="background:#f1c40f;">#244&nbsp;h3</span></div><div class="bbox" style="left:64.2px;top:205.2px;width:750.7px;height:282.2px;border-color:#f1c40f;" title="#245 document_p062_c00245 [h3]
回避策と設計( Mitigation )
短い切れ目のマージ+手動マッピング 設計
極端に短い発話区間を前後の発話者に統合するルールを適用。参加者名と ID の紐付け UI を用意。
VAD 厳格化+無音区間破棄 前処理
Silero VAD 等の閾値を調整し、確実に音声がある区間のみ ASR へ渡す。「要審 査フラグ」で怪しい出力をマーク。
逐次要約( Rolling Summary )+ KV"><span class="lbl" style="background:#f1c40f;">#245&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 63 (10 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:19.1px;width:766.5px;height:172.9px;border-color:#f1c40f;" title="#246 document_p063_c00246 [h3]
ユースケース:文書 RAG PDF/ スキャン → OCR → 検索 → 回答
結論: RAG 品質は Embedding で上限が決まるため、「 Embedding で recall 確保 →Reranker で precision 向上」の二段構えが基本戦略で す。
低コスト構成"><span class="lbl" style="background:#f1c40f;">#246&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:223.8px;width:204.0px;height:38.4px;border-color:#f1c40f;" title="#247 document_p063_c00247 [h3]
OCR / Pre-process
Tesseract OCR ※スキャン品質が高い場合"><span class="lbl" style="background:#f1c40f;">#247&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:287.0px;width:202.6px;height:38.4px;border-color:#f1c40f;" title="#248 document_p063_c00248 [h3]
Embedding / Reranker
BGE-M3 ( 多用途 ) bge-reranker-base ( 軽量 )"><span class="lbl" style="background:#f1c40f;">#248&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:178.4px;width:433.5px;height:265.1px;border-color:#f1c40f;" title="#249 document_p063_c00249 [h3]
LLM Runtime
7B 級 4bit (CPU) llama.cpp / Ollama
テキスト中心の標準 PDF
高品質構成"><span class="lbl" style="background:#f1c40f;">#249&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:223.8px;width:214.4px;height:51.2px;border-color:#f1c40f;" title="#250 document_p063_c00250 [h3]
OCR / DocAI
PaddleOCR ( 構造化 ) Donut (OCR-free 情報抽出 )
LayoutLMv3 ( レイアウト保持 )"><span class="lbl" style="background:#f1c40f;">#250&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:301.2px;width:196.1px;height:38.3px;border-color:#f1c40f;" title="#251 document_p063_c00251 [h3]
Embedding / Reranker
BGE-M3 + 大型 reranker (GPU 推論推奨 )"><span class="lbl" style="background:#f1c40f;">#251&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:178.4px;width:447.3px;height:264.1px;border-color:#f1c40f;" title="#252 document_p063_c00252 [h3]
LLM Runtime
14B 〜 32B 級 (GPU) 図表・レイアウト情報を加味
図表・帳票を含む文書
ハード制約構成"><span class="lbl" style="background:#f1c40f;">#252&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:223.8px;width:216.1px;height:38.4px;border-color:#f1c40f;" title="#253 document_p063_c00253 [h3]
Processing Strategy
夜間バッチ処理 OCR ・ Embedding を夜間に生成"><span class="lbl" style="background:#f1c40f;">#253&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:287.0px;width:218.3px;height:38.4px;border-color:#f1c40f;" title="#254 document_p063_c00254 [h3]
Daytime Operation
検索と短い回答のみ実行 LLM 推論負荷を最小化"><span class="lbl" style="background:#f1c40f;">#254&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:350.2px;width:797.9px;height:143.3px;border-color:#f1c40f;" title="#255 document_p063_c00255 [h3]
LLM Runtime
3B 〜 7B 級 (4bit/INT8) コンテキスト長を制限
リソース極小環境
Local AI Technical Survey Report 2026 63 / 80"><span class="lbl" style="background:#f1c40f;">#255&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 64 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:21.9px;width:767.9px;height:436.7px;border-color:#f1c40f;" title="#256 document_p064_c00256 [h3]
文書 RAG :失敗モードと回避策 OCR 誤読・検索失敗・幻覚への対策設計
結論:文書 RAG の失敗は「 OCR 誤読」「検索精度不足」「幻覚引用」に大別され、 前処理の標準化 と 原文スニペット引用 の UX 要件化で回避します。
主な失敗モード( Failure Modes )
スキャン品質低下や傾きにより、固有名詞や数値が誤認識され、正しい文書が ヒットしない。 Recall 低下
表組"><span class="lbl" style="background:#f1c40f;">#256&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 65 (13 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:19.1px;width:769.6px;height:172.9px;border-color:#f1c40f;" title="#257 document_p065_c00257 [h3]
ユースケース:画像理解( VLM ) — 3 構成 スクショ / 写真の説明・抽出:推奨スタック比較
結論: 2B 〜 7B 級 VLM がローカルの現実ライン。 72B 級は上位機前提となります。 ※視覚トークン肥大による速度 / メモリ急落を防ぐため、解像度・ max-pixels の制御が必須です。
低コスト構成"><span class="lbl" style="background:#f1c40f;">#257&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:223.8px;width:169.6px;height:31.3px;border-color:#f1c40f;" title="#258 document_p065_c00258 [h3]
Target HW
Apple 16GB / VRAM 8GB 級"><span class="lbl" style="background:#f1c40f;">#258&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:272.8px;width:192.2px;height:38.4px;border-color:#f1c40f;" title="#259 document_p065_c00259 [h3]
VLM Model
Qwen2-VL 2B 軽量・高速な視覚理解"><span class="lbl" style="background:#f1c40f;">#259&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:336.0px;width:204.3px;height:44.1px;border-color:#f1c40f;" title="#260 document_p065_c00260 [h3]
Runtime
mac: MLX-VLM ( 公式例あり ) Win: GPU 推論
(transformers 等 )"><span class="lbl" style="background:#f1c40f;">#260&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:178.4px;width:433.5px;height:299.8px;border-color:#f1c40f;" title="#261 document_p065_c00261 [h3]
Key Point
解像度を制限しメモリ圧迫を回避
スクショ説明・簡易 OCR
高品質構成"><span class="lbl" style="background:#f1c40f;">#261&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:223.8px;width:178.9px;height:30.6px;border-color:#f1c40f;" title="#262 document_p065_c00262 [h3]
Target HW
Apple 32GB+ / VRAM 16-24GB+"><span class="lbl" style="background:#f1c40f;">#262&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:272.8px;width:224.5px;height:38.4px;border-color:#f1c40f;" title="#263 document_p065_c00263 [h3]
VLM Model
Qwen2-VL 7B / InternVL2 ( 必要に応じて上位モデル )"><span class="lbl" style="background:#f1c40f;">#263&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:336.0px;width:228.0px;height:44.1px;border-color:#f1c40f;" title="#264 document_p065_c00264 [h3]
Strategy
画像 → テキスト抽出 →Embedding →RAG 融合 ( ハイブ
リッド検索 )"><span class="lbl" style="background:#f1c40f;">#264&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:178.4px;width:447.3px;height:298.9px;border-color:#f1c40f;" title="#265 document_p065_c00265 [h3]
Key Point
OCR-free の図表理解能力を活用
図表理解・情報抽出
ハード制約構成"><span class="lbl" style="background:#f1c40f;">#265&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:223.8px;width:170.8px;height:31.3px;border-color:#f1c40f;" title="#266 document_p065_c00266 [h3]
Target HW
VRAM 不足 / メモリ制約大"><span class="lbl" style="background:#f1c40f;">#266&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:272.8px;width:227.3px;height:43.1px;border-color:#f1c40f;" title="#267 document_p065_c00267 [h3]
Substitute Strategy
OCR (PaddleOCR 等 ) + テキスト LLM VLM モデルを使用
しない"><span class="lbl" style="background:#f1c40f;">#267&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:336.0px;width:226.9px;height:43.1px;border-color:#f1c40f;" title="#268 document_p065_c00268 [h3]
Operation
OCR でテキスト化し、 LLM で整形画像入力自体を避
ける"><span class="lbl" style="background:#f1c40f;">#268&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:399.2px;width:224.6px;height:78.1px;border-color:#f1c40f;" title="#269 document_p065_c00269 [h3]
Compromise
空間認識・文脈理解を諦め文字情報の抽出・整理
に限定
文字主体の処理"><span class="lbl" style="background:#f1c40f;">#269&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 66 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:21.9px;width:772.8px;height:438.5px;border-color:#5dade2;" title="#270 document_p066_c00270 [body]
画像理解:失敗モードと回避策 ユースケース別推奨スタック(詳細)
結論: UI 文字の誤読と視覚トークン肥大による速度低下を、 OCR 併用 と 解像度・ max-pixels 制御 で回避します。
主な失敗モード( Failure Modes )
2B/7B 級 VLM では、スクリーンショット内の小さなフォントや密集した 情報を正確に読み取れないケースが頻発。
高解像度画像をそのまま入力すると"><span class="lbl" style="background:#5dade2;">#270&nbsp;body</span></div></div></div><div class="page-block"><h2>Page 67 (10 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:19.1px;width:765.8px;height:172.9px;border-color:#5dade2;" title="#271 document_p067_c00271 [body]
ユースケース:コーディング補助 推奨スタック 3 パターン( IDE 支援・リポジトリ理解)
結論:補完は小型モデル、設計レビューや長距離依存解決は中〜大型モデルで役割分担します。 ※生成コードは自動コンパイル / テスト実行を “ ツール ” 化し、検証ループに組み込むことが重要です。
低コスト構成"><span class="lbl" style="background:#5dade2;">#271&nbsp;body</span></div><div class="bbox" style="left:46.6px;top:223.8px;width:208.5px;height:38.4px;border-color:#f1c40f;" title="#272 document_p067_c00272 [h3]
Code LLM
Qwen2.5-Coder 7B サイズ展開が豊富で軽量"><span class="lbl" style="background:#f1c40f;">#272&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:287.0px;width:222.1px;height:44.1px;border-color:#f1c40f;" title="#273 document_p067_c00273 [h3]
RAG / Embedding
リポジトリ Embedding (BGE/E5) 関連ファイル抽出
→LLM 回答"><span class="lbl" style="background:#f1c40f;">#273&nbsp;h3</span></div><div class="bbox" style="left:46.6px;top:178.4px;width:433.5px;height:250.8px;border-color:#f1c40f;" title="#274 document_p067_c00274 [h3]
Connection
Ollama / LM Studio OpenAI 互換 API で IDE 拡張接続
一般的な IDE 補完・ Q&amp;A
高品質構成"><span class="lbl" style="background:#f1c40f;">#274&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:223.8px;width:201.8px;height:38.4px;border-color:#f1c40f;" title="#275 document_p067_c00275 [h3]
Code LLM
StarCoder2 (7B/15B) + 32B 級汎用 LLM 併用"><span class="lbl" style="background:#f1c40f;">#275&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:287.0px;width:227.5px;height:43.1px;border-color:#f1c40f;" title="#276 document_p067_c00276 [h3]
Optimization
prefix caching (vLLM 等 ) 長文コンテキストでの遅延抑
制"><span class="lbl" style="background:#f1c40f;">#276&nbsp;h3</span></div><div class="bbox" style="left:317.6px;top:178.4px;width:447.3px;height:249.9px;border-color:#f1c40f;" title="#277 document_p067_c00277 [h3]
Capability
設計レビュー / 長距離依存複雑なリファクタリング
提案
大規模開発・設計支援
ハード制約構成"><span class="lbl" style="background:#f1c40f;">#277&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:223.8px;width:202.3px;height:38.4px;border-color:#f1c40f;" title="#278 document_p067_c00278 [h3]
Target HW
メモリ 8GB / エントリー機古い開発環境"><span class="lbl" style="background:#f1c40f;">#278&nbsp;h3</span></div><div class="bbox" style="left:588.7px;top:287.0px;width:194.7px;height:38.4px;border-color:#f1c40f;" title="#279 document_p067_c00279 [h3]
Code LLM
3B 級モデル Phi-3 mini / StarCoder2-3B"><span class="lbl" style="background:#f1c40f;">#279&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:350.2px;width:797.9px;height:132.3px;border-color:#f1c40f;" title="#280 document_p067_c00280 [h3]
Strategy
短い支援に限定「行補完」+「関数リファクタ提
案」
スニペット補完のみ
Local AI Technical Survey Report 2026 67 / 80"><span class="lbl" style="background:#f1c40f;">#280&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 68 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:56.4px;top:21.9px;width:764.0px;height:472.9px;border-color:#5dade2;" title="#281 document_p068_c00281 [body]
コーディング支援:失敗モードと回避策 コーディング補助( IDE 支援・リポジトリ理解)
結論:幻覚 API 生成と長文遅延を「検証ループ」と「キャッシュ活用」で抑止し、 コード品質とセキュリティを自動化プロセスで担保します。
主な失敗モード( Failure Modes )
ライブラリのバージョン不一致や、もっともらしいが実在しない関数 ( Hallucination )を生成。
リポジトリ全体"><span class="lbl" style="background:#5dade2;">#281&nbsp;body</span></div></div></div><div class="page-block"><h2>Page 69 (13 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:19.1px;width:770.3px;height:174.3px;border-color:#f1c40f;" title="#282 document_p069_c00282 [h3]
ユースケース:画像生成( Diffusion ) — 3 構成 推奨スタックと VRAM 要件の最適化
結論: SDXL を軸に ComfyUI でワークフローを資産化。 FLUX 等はライセンス精査が必須です。 ※ VRAM 要件は解像度・バッチ・ステップ数に依存するため、プロファイル固定が重要です。
低コスト構成"><span class="lbl" style="background:#f1c40f;">#282&nbsp;h3</span></div><div class="bbox" style="left:56.1px;top:226.5px;width:81.3px;height:44.4px;border-color:#f1c40f;" title="#283 document_p069_c00283 [h3]
Target VRAM
VRAM 8GB 〜 12GB (Apple Silicon 16GB)"><span class="lbl" style="background:#f1c40f;">#283&nbsp;h3</span></div><div class="bbox" style="left:56.1px;top:297.1px;width:115.6px;height:44.5px;border-color:#f1c40f;" title="#284 document_p069_c00284 [h3]
Model / UI
SDXL base 1.0 + Refiner なし ComfyUI (Win/Linux/macOS)"><span class="lbl" style="background:#f1c40f;">#284&nbsp;h3</span></div><div class="bbox" style="left:56.1px;top:367.8px;width:95.5px;height:44.4px;border-color:#f1c40f;" title="#285 document_p069_c00285 [h3]
Optimization
Attention Slicing 有効化 FP16 / Tiled VAE 活用"><span class="lbl" style="background:#f1c40f;">#285&nbsp;h3</span></div><div class="bbox" style="left:56.1px;top:179.8px;width:425.3px;height:303.1px;border-color:#f1c40f;" title="#286 document_p069_c00286 [h3]
Resolution
1024x1024 固定 Batch size = 1
高品質構成"><span class="lbl" style="background:#f1c40f;">#286&nbsp;h3</span></div><div class="bbox" style="left:327.1px;top:226.5px;width:86.4px;height:44.4px;border-color:#f1c40f;" title="#287 document_p069_c00287 [h3]
Target VRAM
VRAM 24GB 以上 (Apple Silicon 64GB+)"><span class="lbl" style="background:#f1c40f;">#287&nbsp;h3</span></div><div class="bbox" style="left:327.1px;top:297.1px;width:132.1px;height:44.5px;border-color:#f1c40f;" title="#288 document_p069_c00288 [h3]
Model / UI
FLUX.1 [dev/pro] 等の上位系 ComfyUI ( ワークフロー資産化 )"><span class="lbl" style="background:#f1c40f;">#288&nbsp;h3</span></div><div class="bbox" style="left:327.1px;top:367.8px;width:98.7px;height:44.4px;border-color:#f1c40f;" title="#289 document_p069_c00289 [h3]
Optimization
Seed 固定で再現性担保 Refiner / LoRA 多重適用"><span class="lbl" style="background:#f1c40f;">#289&nbsp;h3</span></div><div class="bbox" style="left:327.1px;top:179.8px;width:439.2px;height:302.1px;border-color:#f1c40f;" title="#290 document_p069_c00290 [h3]
License Check
FLUX.1-dev 等の商用条件 ライセンス精査が必須
ハード制約構成"><span class="lbl" style="background:#f1c40f;">#290&nbsp;h3</span></div><div class="bbox" style="left:598.1px;top:226.5px;width:95.1px;height:44.4px;border-color:#f1c40f;" title="#291 document_p069_c00291 [h3]
Target VRAM
VRAM 8GB 未満 ( メインメモリ共有等 )"><span class="lbl" style="background:#f1c40f;">#291&nbsp;h3</span></div><div class="bbox" style="left:598.1px;top:297.1px;width:124.0px;height:44.5px;border-color:#f1c40f;" title="#292 document_p069_c00292 [h3]
Model / UI
SD 1.5 系 / LCM-LoRA ( 高速化 ) Stable Diffusion WebUI (Forge)"><span class="lbl" style="background:#f1c40f;">#292&nbsp;h3</span></div><div class="bbox" style="left:598.1px;top:367.8px;width:144.7px;height:44.4px;border-color:#f1c40f;" title="#293 document_p069_c00293 [h3]
Offloading strategy
夜間バッチ処理 または LAN 内別筐体へオフロード"><span class="lbl" style="background:#f1c40f;">#293&nbsp;h3</span></div><div class="bbox" style="left:598.1px;top:438.4px;width:115.9px;height:44.5px;border-color:#f1c40f;" title="#294 document_p069_c00294 [h3]
Limitation
解像度 512x512 等に制限 日中は LLM にリソース集中"><span class="lbl" style="background:#f1c40f;">#294&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 70 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:56.4px;top:21.9px;width:767.1px;height:467.5px;border-color:#5dade2;" title="#295 document_p070_c00295 [body]
画像生成:失敗モードと回避策 VRAM 制約と品質・ライセンス管理
結論: VRAM 不足と生成結果のブレを 「標準プロファイル」 で抑制し、 ライセンス違反リスクを 「事前精査と監査」 で排除します。
主な失敗モード( Failure Modes )
高解像度や大バッチ指定時にプロセスがクラッシュ。特に SDXL/FLUX 等 の大型モデルで頻発。
同じプロンプトでも Seed や設定の違いで出"><span class="lbl" style="background:#5dade2;">#295&nbsp;body</span></div></div></div><div class="page-block"><h2>Page 71 (3 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:54.0px;top:103.1px;width:89.8px;height:90.5px;border-color:#e74c3c;" title="#296 document_p071_c00296 [h1]
10"><span class="lbl" style="background:#e74c3c;">#296&nbsp;h1</span></div><div class="bbox" style="left:54.0px;top:237.8px;width:223.5px;height:132.8px;border-color:#f1c40f;" title="#297 document_p071_c00297 [h3]
測定と比較の 方法
ベンチマーク設計と 再現性の担保"><span class="lbl" style="background:#f1c40f;">#297&nbsp;h3</span></div><div class="bbox" style="left:443.0px;top:75.4px;width:348.5px;height:332.2px;border-color:#f1c40f;" title="#298 document_p071_c00298 [h3]
KE Y TAKEAWAYS
本章の要点
指標の厳密定義:
p50/p95 レイテンシ、 TTFT 、 tok/s 、 RTF 等を定義し、 JSON 破 壊率や意味改変率も定量化します。
測定手順の標準化:
条件固定(温度・量子化等)、ウォームアップ分離、 KV 影 響の切り分け(短文 / 長文)を徹底します。
再現性のコア要素:
ASR/TTS の前処理固定と、最小テストセット(短文 / 長文"><span class="lbl" style="background:#f1c40f;">#298&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 72 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:53.2px;top:18.3px;width:764.1px;height:470.4px;border-color:#5dade2;" title="#299 document_p072_c00299 [body]
測定指標の定義 p50/p95 ・ TTFT ・ tok/s ・ RTF ・品質指標
結論: LLM の「初速( TTFT )」と「生成速度( tok/s )」を分離して計測し、 品質( JSON 破壊率等)とリソース消費( RAM/VRAM )を定量化します。
速度・リソース指標( Latency &amp; Resource )
TTFT ( Time To First Token )と生成完了時間を"><span class="lbl" style="background:#5dade2;">#299&nbsp;body</span></div></div></div><div class="page-block"><h2>Page 73 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:27.0px;top:17.7px;width:803.2px;height:461.0px;border-color:#f1c40f;" title="#300 document_p073_c00300 [h3]
測定手順(条件固定・ウォームアップ・ KV 切り分け) 再現可能なベンチマーク設計の要件
再現条件の固定
生成パラメータの統一
同一プロンプト、最大トークン数、温度( temperature )、 top_p を固 定し、ランダム性を排除または制御します。
モデル環境の固定
同一の量子化形式( GGUF Q4_K_M など)、コンテキスト長設定を使 用します。
音声・画像系の前提
ASR/TTS "><span class="lbl" style="background:#f1c40f;">#300&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 74 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:52.7px;top:17.4px;width:780.5px;height:460.9px;border-color:#5dade2;" title="#301 document_p074_c00301 [body]
リスクとコンプライアンス ライセンス管理・プライバシー・安全性対策
結論:「コードの OSS ライセンス」と「モデル重みの利用条件」を分離管理し、ローカル完結の利点を活かしつつ、検証ループ による安全性確保が必須です。
ライセンス管理
コードと重みの分離
ランタイム (MIT/Apache) とモデル重み (Community/ 非商用 ) は別 条件。利用範囲を台帳化する。
個別確認の義務化
Q"><span class="lbl" style="background:#5dade2;">#301&nbsp;body</span></div></div></div><div class="page-block"><h2>Page 75 (8 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:64.8px;top:17.4px;width:765.4px;height:18.8px;border-color:#5dade2;" title="#302 document_p075_c00302 [body]
まとめ(意思決定フロー) ローカル AI 導入の 5 ステップと判断基準"><span class="lbl" style="background:#5dade2;">#302&nbsp;body</span></div><div class="bbox" style="left:43.9px;top:86.8px;width:113.7px;height:239.3px;border-color:#f1c40f;" title="#303 document_p075_c00303 [h3]
1 要件定義 Requirements Requirements
業務課題から技術要件へ変換す る始点。
判断基準 (Criteria)
品質 : 日本語、 JSON 、専門性
速度 : p95 レイテンシ、 tok/s
機能 : Tool-use 、 RAG 有無
アクション
許容レイテンシ目標(例 : 10 tok/s )と JSON スキーマの厳格 さを決定。"><span class="lbl" style="background:#f1c40f;">#303&nbsp;h3</span></div><div class="bbox" style="left:209.2px;top:86.8px;width:113.4px;height:226.2px;border-color:#f1c40f;" title="#304 document_p075_c00304 [h3]
2 Tier 選定 Hardware Tier Hardware Tier
メモリ制約に基づく現実ライン の把握。
判断基準 (Criteria)
Apple: 16 〜 128GB (UMA)
Windows: VRAM 8 〜 24GB
CPU: AVX/RAM 容量
アクション
「重み 4bit + KV キャッシュ」 の理論値で Tier (A-16 〜 G-24) を特定。"><span class="lbl" style="background:#f1c40f;">#304&nbsp;h3</span></div><div class="bbox" style="left:374.5px;top:86.8px;width:118.4px;height:213.0px;border-color:#f1c40f;" title="#305 document_p075_c00305 [h3]
3 モデル選定 Model Select Model Select
Tier 内で動く最適モデルの選定。
判断基準 (Criteria)
規模 : 7B-14B ( 汎用 ), 32B+ ( 高品 質 )
形式 : GGUF, EXL2, AWQ
権利 : ライセンス条件
アクション
Llama/Qwen/Phi 等からサイズ 適合候補を選び、ライセンス を確認。"><span class="lbl" style="background:#f1c40f;">#305&nbsp;h3</span></div><div class="bbox" style="left:539.9px;top:86.8px;width:113.4px;height:88.8px;border-color:#f1c40f;" title="#306 document_p075_c00306 [h3]
4 ランタイム Runtime Runtime
ハードウェア性能を引き出す実 行環境。
判断基準 (Criteria)"><span class="lbl" style="background:#f1c40f;">#306&nbsp;h3</span></div><div class="bbox" style="left:545.9px;top:182.4px;width:72.4px;height:8.5px;border-color:#f1c40f;" title="#307 document_p075_c00307 [h3]
Mac: MLX / llama.cpp"><span class="lbl" style="background:#f1c40f;">#307&nbsp;h3</span></div><div class="bbox" style="left:545.9px;top:196.4px;width:106.6px;height:103.4px;border-color:#f1c40f;" title="#308 document_p075_c00308 [h3]
Win: Ollama / LM Studio / vLLM
API: OpenAI 互換性
アクション
OS とモデル形式に合ったラン タイムを選択。 Server 機能の 有無を確認。"><span class="lbl" style="background:#f1c40f;">#308&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:86.8px;width:797.9px;height:394.5px;border-color:#f1c40f;" title="#309 document_p075_c00309 [h3]
5 検証・最適化 Verify Verify
実機ベンチによる実用性の確定 。
判断基準 (Criteria)
指標 : TTFT, p95, JSON 破壊率
負荷 : メモリピーク , KV 推移
品質 : 幻覚 , 誤転記
アクション
条件固定ベンチで測定。 KV 量 子化やプロンプトキャッシュ で調整。
意思決定の要点 (Key Takeaways)
「ランタイム先行」ではなく「要件 → "><span class="lbl" style="background:#f1c40f;">#309&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 76 (8 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:56.4px;top:22.9px;width:770.6px;height:248.3px;border-color:#f1c40f;" title="#310 document_p076_c00310 [h3]
総括(実務への適用) 実装の原則と次のステップ
結論:ローカル AI は「 4bit 量子化+ KV 最適化」を前提に、 失敗モードを設計で抑え込むことで実務運用が可能です。
実装の原則( Principles )
重みは 4bit ( GGUF/AWQ/GPTQ )でメモリ理論値を計算し、 Tier に合わせ る。"><span class="lbl" style="background:#f1c40f;">#310&nbsp;h3</span></div><div class="bbox" style="left:62.8px;top:271.2px;width:351.1px;height:63.2px;border-color:#f1c40f;" title="#311 document_p076_c00311 [h3]
Cost-Efficiency
長文・多同時接続時は、 paged KV 、 KV 量子化、 prompt cache を活用してメ モリ爆発を防ぐ。"><span class="lbl" style="background:#f1c40f;">#311&nbsp;h3</span></div><div class="bbox" style="left:62.8px;top:334.4px;width:345.9px;height:63.2px;border-color:#f1c40f;" title="#312 document_p076_c00312 [h3]
Scalability
幻覚・誤転記・ JSON 破壊は「起きるもの」とし、 UI 確認・検証ループ・ 再生成でカバーする。"><span class="lbl" style="background:#f1c40f;">#312&nbsp;h3</span></div><div class="bbox" style="left:62.8px;top:199.5px;width:594.2px;height:261.3px;border-color:#f1c40f;" title="#313 document_p076_c00313 [h3]
Robustness
p50/p95 レイテンシと品質( tok/s, RTF, JSON 破壊率)を定量ベンチマーク で測定し確定させる。
次のステップ( Action Items )"><span class="lbl" style="background:#f1c40f;">#313&nbsp;h3</span></div><div class="bbox" style="left:473.8px;top:240.6px;width:333.3px;height:34.5px;border-color:#f1c40f;" title="#314 document_p076_c00314 [h3]
1
Tier の確定: 手持ちハードウェア( Apple/Win )と Tier 表を照合し、現実的なモデル規模を把 握。"><span class="lbl" style="background:#f1c40f;">#314&nbsp;h3</span></div><div class="bbox" style="left:473.8px;top:308.2px;width:332.2px;height:34.5px;border-color:#f1c40f;" title="#315 document_p076_c00315 [h3]
2
パイロット構築: 推奨スタック(低コスト / 高品質)に基づき、 llama.cpp/Ollama/faster-whisper 等 でプロトタイプ作成。"><span class="lbl" style="background:#f1c40f;">#315&nbsp;h3</span></div><div class="bbox" style="left:473.8px;top:375.8px;width:333.3px;height:24.4px;border-color:#f1c40f;" title="#316 document_p076_c00316 [h3]
3
ベンチマーク実施: 条件固定(温度 / トークン数)でログを取り、 p95 レイテンシと実用性を計測。"><span class="lbl" style="background:#f1c40f;">#316&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:443.5px;width:797.9px;height:35.1px;border-color:#f1c40f;" title="#317 document_p076_c00317 [h3]
4
リスク監査: ライセンス(特に画像生成 /TTS )とプライバシー要件(外部送信なし)を最終 確認。 Local AI Technical Survey Report 2026 76 / 80"><span class="lbl" style="background:#f1c40f;">#317&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 77 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:32.4px;top:31.2px;width:777.2px;height:447.0px;border-color:#f1c40f;" title="#318 document_p077_c00318 [h3]
References ( 1/4 ) Technical Research Report Sources [1-34]
[1] https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/
[2] https://docs.openhands.dev/openhands/usage/llms/local-llms
[3] https://lm"><span class="lbl" style="background:#f1c40f;">#318&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 78 (2 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:41.2px;top:31.2px;width:753.9px;height:197.0px;border-color:#f1c40f;" title="#319 document_p078_c00319 [h3]
References (参考文献リスト 2/4 ) [35] ~ [68] Llama / Qwen / Gemma / Phi / VLM / Whisper / TTS / OCR
NO. S OU RC E U R L / D ES CR IPTION
[35] G em m a 2 M odel C ard https://ai.google.dev/gemma/docs/core/mod"><span class="lbl" style="background:#f1c40f;">#319&nbsp;h3</span></div><div class="bbox" style="left:32.4px;top:112.2px;width:790.7px;height:382.6px;border-color:#f1c40f;" title="#320 document_p078_c00320 [h3]
[41]
Op en AI C ook bo ok (R un locally LM S tu dio) https://developers.openai.com/cookbook/articles/gpt-oss/run-locally- lmstudio/
[42] Qw en2- VL 72B https://huggingface.co/Qwen/Qwen2-VL-72B
[43] Ph"><span class="lbl" style="background:#f1c40f;">#320&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 79 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:32.4px;top:31.2px;width:793.9px;height:447.0px;border-color:#f1c40f;" title="#321 document_p079_c00321 [h3]
References ( 3/4 ) 参考文献リスト [69] - [102]
N o . R eference D etails (Layout / Im ag e Gen / Voice / H W )
[69] LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking https://arxiv."><span class="lbl" style="background:#f1c40f;">#321&nbsp;h3</span></div></div></div><div class="page-block"><h2>Page 80 (1 chunks)</h2><div class="canvas" style="width:536px;height:758px;"><div class="bbox" style="left:32.4px;top:31.2px;width:793.5px;height:461.1px;border-color:#f1c40f;" title="#322 document_p080_c00322 [h3]
References (参考文献リスト 4/4 ) 音声周辺・ベンチマーク・最適化技術・ハードウェア関連 [103] - [136]
[103
]
Phi-3-m ini-128k-instruct M odel C ard https://huggingface.co/microsoft/Phi-3-mini-128k-instruct
[104
]
vLLM: Qua ntized KV Ca"><span class="lbl" style="background:#f1c40f;">#322&nbsp;h3</span></div></div></div></body></html>
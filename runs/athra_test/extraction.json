{
  "schema_version": "0.1",
  "document": {
    "document_id": "document",
    "source_type": "pdf",
    "source_path": "input/テスト.pdf",
    "page_count": 80
  },
  "chunks": [
    {
      "chunk_id": "document_p001_c00001",
      "block_type": "text",
      "page_no": 1,
      "order": 1,
      "bbox": [
        376.68,
        497.63,
        583.38,
        508.84
      ],
      "text": "2026 年 2 月 15 日 | Generated by Genspark AI Slides",
      "normalized_text": "2026 年 2 月 15 日 | Generated by Genspark AI Slides",
      "heading_level": 0,
      "numbers": [
        "2026 ",
        "2 ",
        "15 "
      ],
      "hash": "490329eef887e14c",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p001_c00002",
      "block_type": "text",
      "page_no": 1,
      "order": 2,
      "bbox": [
        204.28,
        139.37,
        755.57,
        220.95
      ],
      "text": "Technical Research Report ローカル AI 技術調査レポート",
      "normalized_text": "Technical Research Report ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [],
      "hash": "f56f4ad489f58a65",
      "meta": {
        "body_font_size": 10.1
      }
    },
    {
      "chunk_id": "document_p001_c00003",
      "block_type": "text",
      "page_no": 1,
      "order": 3,
      "bbox": [
        192.12,
        236.59,
        767.79,
        321.47
      ],
      "text": "オンデバイス/ローカル推論の現実解と推奨スタック\nLLM VLM ASR TTS RAG Apple Silicon Windows GPU",
      "normalized_text": "オンデバイス/ローカル推論の現実解と推奨スタック\nLLM VLM ASR TTS RAG Apple Silicon Windows GPU",
      "heading_level": 3,
      "numbers": [],
      "hash": "07a68ae17dfd28d4",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p001_c00004",
      "block_type": "text",
      "page_no": 1,
      "order": 4,
      "bbox": [
        199.61,
        374.18,
        734.28,
        424.05
      ],
      "text": "Report Purpose\nApple Silicon および Windows 環境におけるローカル AI の実運用ラインと推奨スタックを提示。 「意思決定のための要点」と「根拠となる完全な技術詳細」を二層構造で網羅。",
      "normalized_text": "Report Purpose\nApple Silicon および Windows 環境におけるローカル AI の実運用ラインと推奨スタックを提示。 「意思決定のための要点」と「根拠となる完全な技術詳細」を二層構造で網羅。",
      "heading_level": 3,
      "numbers": [],
      "hash": "7c6f7cb90460f09e",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p002_c00005",
      "block_type": "text",
      "page_no": 2,
      "order": 5,
      "bbox": [
        30.04,
        19.66,
        922.28,
        139.24
      ],
      "text": "このスライドの読み方 概要 → 詳細の二層構造ガイド\n構造:二層設計",
      "normalized_text": "このスライドの読み方 概要 → 詳細の二層構造ガイド\n構造:二層設計",
      "heading_level": 3,
      "numbers": [],
      "hash": "b3f2eb3a702707e1",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p002_c00006",
      "block_type": "text",
      "page_no": 2,
      "order": 6,
      "bbox": [
        74.26,
        184.47,
        447.24,
        231.98
      ],
      "text": "1. 概要スライド( Executive Summary )\n意思決定に必要な「結論」と「要点」を最初に提示します。時間がな い場合はここだけ読めば全体像が掴めます。",
      "normalized_text": "1. 概要スライド( Executive Summary )\n意思決定に必要な「結論」と「要点」を最初に提示します。時間がな い場合はここだけ読めば全体像が掴めます。",
      "heading_level": 3,
      "numbers": [
        "1. "
      ],
      "hash": "ff18cc1da57f6b79",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p002_c00007",
      "block_type": "text",
      "page_no": 2,
      "order": 7,
      "bbox": [
        74.26,
        123.37,
        907.14,
        531.4
      ],
      "text": "2. 完全版スライド( Full Detail )\nレポート内の表、数値、グラフ、注釈を省略せずに掲載します。エン ジニアや実装担当者が参照するための詳細情報です。\n相互リンクと参照\n概要から詳細へ、詳細から参考文献へ、論理的に接続されています 。\n設計原則と表記\n省略しない\n元の技術レポートに含まれる情報は、脚注や URL に至るまで全てスラ イド内に保持します。\n根拠の明示\n「推測」と「事実(計算値 / 公式発表)」を明確に区別し、一次ソー スに基づきます。\n推奨構成 注意・失敗モード 補足情報\nKey Takeaways: 各章の冒頭で「持ち帰るべき要点」を 3 〜 5 点で提示します。\nPage 2 | ローカル AI 技術調査レポート",
      "normalized_text": "2. 完全版スライド( Full Detail )\nレポート内の表、数値、グラフ、注釈を省略せずに掲載します。エン ジニアや実装担当者が参照するための詳細情報です。\n相互リンクと参照\n概要から詳細へ、詳細から参考文献へ、論理的に接続されています 。\n設計原則と表記\n省略しない\n元の技術レポートに含まれる情報は、脚注や URL に至るまで全てスラ イド内に保持します。\n根拠の明示\n「推測」と「事実(計算値 / 公式発表)」を明確に区別し、一次ソー スに基づきます。\n推奨構成 注意・失敗モード 補足情報\nKey Takeaways: 各章の冒頭で「持ち帰るべき要点」を 3 〜 5 点で提示します。\nPage 2 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "2. ",
        "3 ",
        "5 ",
        "2 "
      ],
      "hash": "eec65ef843bd789f",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p003_c00008",
      "block_type": "text",
      "page_no": 3,
      "order": 8,
      "bbox": [
        37.54,
        26.28,
        923.25,
        76.89
      ],
      "text": "Local AI Technical Report 目次( Agenda ) Total 13 Chapters",
      "normalized_text": "Local AI Technical Report 目次( Agenda ) Total 13 Chapters",
      "heading_level": 3,
      "numbers": [
        "13 "
      ],
      "hash": "bb4322436b166bbb",
      "meta": {
        "body_font_size": 10.1
      }
    },
    {
      "chunk_id": "document_p003_c00009",
      "block_type": "text",
      "page_no": 3,
      "order": 9,
      "bbox": [
        46.53,
        117.26,
        298.34,
        146.14
      ],
      "text": "01 タイトル/目的/読み方\nレポートの目的と、概要 → 詳細の二層構造の活用方法。",
      "normalized_text": "01 タイトル/目的/読み方\nレポートの目的と、概要 → 詳細の二層構造の活用方法。",
      "heading_level": 3,
      "numbers": [
        "01 "
      ],
      "hash": "851b139e6037b04b",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00010",
      "block_type": "text",
      "page_no": 3,
      "order": 10,
      "bbox": [
        46.53,
        176.89,
        281.39,
        204.76
      ],
      "text": "02 エグゼクティブサマリ\n実務要点、中核技術、推奨スタックの概要を提示。",
      "normalized_text": "02 エグゼクティブサマリ\n実務要点、中核技術、推奨スタックの概要を提示。",
      "heading_level": 3,
      "numbers": [
        "02 "
      ],
      "hash": "ba2e67315a8bcf15",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00011",
      "block_type": "text",
      "page_no": 3,
      "order": 11,
      "bbox": [
        46.53,
        236.6,
        268.88,
        265.49
      ],
      "text": "03 ローカル AI の定義・前提\nオンデバイス/ローカル LAN 推論の定義と範囲。",
      "normalized_text": "03 ローカル AI の定義・前提\nオンデバイス/ローカル LAN 推論の定義と範囲。",
      "heading_level": 3,
      "numbers": [
        "03 "
      ],
      "hash": "1fde4b0e6e030757",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00012",
      "block_type": "text",
      "page_no": 3,
      "order": 12,
      "bbox": [
        46.53,
        296.24,
        281.93,
        325.12
      ],
      "text": "04 ローカル AI ランドスケープ\nLLM から Agent まで、技術スタックの全体像を図解。",
      "normalized_text": "04 ローカル AI ランドスケープ\nLLM から Agent まで、技術スタックの全体像を図解。",
      "heading_level": 3,
      "numbers": [
        "04 "
      ],
      "hash": "bc11c587bb709879",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00013",
      "block_type": "text",
      "page_no": 3,
      "order": 13,
      "bbox": [
        46.53,
        355.88,
        291.08,
        384.77
      ],
      "text": "05 特徴軸(評価軸)の定義\n指示追従、 JSON 堅牢性、メモリ効率などの評価基準。",
      "normalized_text": "05 特徴軸(評価軸)の定義\n指示追従、 JSON 堅牢性、メモリ効率などの評価基準。",
      "heading_level": 3,
      "numbers": [
        "05 "
      ],
      "hash": "fa3fcf5f40510839",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00014",
      "block_type": "text",
      "page_no": 3,
      "order": 14,
      "bbox": [
        46.53,
        415.59,
        260.44,
        444.47
      ],
      "text": "06 カテゴリ別モデルカタログ\nLLM/VLM/ASR/TTS 等の代表モデルと採用判断。",
      "normalized_text": "06 カテゴリ別モデルカタログ\nLLM/VLM/ASR/TTS 等の代表モデルと採用判断。",
      "heading_level": 3,
      "numbers": [
        "06 "
      ],
      "hash": "6358e956b78fe585",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00015",
      "block_type": "text",
      "page_no": 3,
      "order": 15,
      "bbox": [
        46.53,
        475.22,
        273.03,
        504.11
      ],
      "text": "07 メモリ設計のコア\n重み理論値、 KV キャッシュ、最適化技術の詳解。",
      "normalized_text": "07 メモリ設計のコア\n重み理論値、 KV キャッシュ、最適化技術の詳解。",
      "heading_level": 3,
      "numbers": [
        "07 "
      ],
      "hash": "01c046e3d13b0ac7",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00016",
      "block_type": "text",
      "page_no": 3,
      "order": 16,
      "bbox": [
        504.25,
        117.26,
        742.2,
        146.14
      ],
      "text": "08 Tier 定義と “ 現実ライン ”\nApple Silicon / Windows GPU / CPU ごとの実用ライン。",
      "normalized_text": "08 Tier 定義と “ 現実ライン ”\nApple Silicon / Windows GPU / CPU ごとの実用ライン。",
      "heading_level": 3,
      "numbers": [
        "08 "
      ],
      "hash": "388a34ce41016930",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00017",
      "block_type": "text",
      "page_no": 3,
      "order": 17,
      "bbox": [
        504.25,
        176.89,
        761.28,
        205.78
      ],
      "text": "09 ユースケース別推奨スタック\n低コスト/高品質/ハード制約別の構成案( 6 ケース)。",
      "normalized_text": "09 ユースケース別推奨スタック\n低コスト/高品質/ハード制約別の構成案( 6 ケース)。",
      "heading_level": 3,
      "numbers": [
        "09 ",
        "6 "
      ],
      "hash": "50f97abc37d8b9cb",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00018",
      "block_type": "text",
      "page_no": 3,
      "order": 18,
      "bbox": [
        504.25,
        236.6,
        752.1,
        265.49
      ],
      "text": "10 測定と比較の方法\n再現可能なベンチマーク手順と指標( tok/s, TTFT 等)。",
      "normalized_text": "10 測定と比較の方法\n再現可能なベンチマーク手順と指標( tok/s, TTFT 等)。",
      "heading_level": 3,
      "numbers": [
        "10 "
      ],
      "hash": "a3ab82c6d48651b8",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00019",
      "block_type": "text",
      "page_no": 3,
      "order": 19,
      "bbox": [
        504.25,
        296.24,
        739.11,
        324.1
      ],
      "text": "11 リスク/コンプライアンス\nライセンス条件、商用利用、セキュリティリスク。",
      "normalized_text": "11 リスク/コンプライアンス\nライセンス条件、商用利用、セキュリティリスク。",
      "heading_level": 3,
      "numbers": [
        "11 "
      ],
      "hash": "d38e64c389dfdefe",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00020",
      "block_type": "text",
      "page_no": 3,
      "order": 20,
      "bbox": [
        504.25,
        355.88,
        739.16,
        383.74
      ],
      "text": "12 まとめ(意思決定フロー)\nモデル選定と環境構築のための意思決定チャート。",
      "normalized_text": "12 まとめ(意思決定フロー)\nモデル選定と環境構築のための意思決定チャート。",
      "heading_level": 3,
      "numbers": [
        "12 "
      ],
      "hash": "876d0150cd674ae1",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p003_c00021",
      "block_type": "text",
      "page_no": 3,
      "order": 21,
      "bbox": [
        65.99,
        417.0,
        934.95,
        524.66
      ],
      "text": "13 References\n一次情報源および参照 URL 一覧。\n各章は「概要(要点)」 → 「詳細(完全版)」の順で構成されています\nPage 3",
      "normalized_text": "13 References\n一次情報源および参照 URL 一覧。\n各章は「概要(要点)」 → 「詳細(完全版)」の順で構成されています\nPage 3",
      "heading_level": 2,
      "numbers": [
        "13 ",
        "3"
      ],
      "hash": "5b613588252fca77",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p004_c00022",
      "block_type": "text",
      "page_no": 4,
      "order": 22,
      "bbox": [
        45.01,
        36.56,
        793.42,
        338.63
      ],
      "text": "エグゼクティブサマリ(実務要点)\nローカル AI 導入における意思決定の重要ポイント\n結論:ローカル AI は「 4bit 量子化+ KV 管理」を前提に、 GGUF/llama.cpp 系と Ollama/LM Studio/MLX で実務化可能です。\n定義とスコープ\n推論が端末 or ローカル LAN 内で完結\n入力データは外部へ送信されない\nLAN サーブ含む( LM Studio, Ollama )\n中核技術\nWeight-only 4bit (AWQ 等 ) 量子化\nKV キャッシュ管理最適化 メモリ\nPaged Attention / Quantized KV\n主要ランタイム\nllama.cpp (GGUF) : 汎用・軽量",
      "normalized_text": "エグゼクティブサマリ(実務要点)\nローカル AI 導入における意思決定の重要ポイント\n結論:ローカル AI は「 4bit 量子化+ KV 管理」を前提に、 GGUF/llama.cpp 系と Ollama/LM Studio/MLX で実務化可能です。\n定義とスコープ\n推論が端末 or ローカル LAN 内で完結\n入力データは外部へ送信されない\nLAN サーブ含む( LM Studio, Ollama )\n中核技術\nWeight-only 4bit (AWQ 等 ) 量子化\nKV キャッシュ管理最適化 メモリ\nPaged Attention / Quantized KV\n主要ランタイム\nllama.cpp (GGUF) : 汎用・軽量",
      "heading_level": 3,
      "numbers": [
        "4",
        "4"
      ],
      "hash": "615ba15742adee80",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 12
      }
    },
    {
      "chunk_id": "document_p004_c00023",
      "block_type": "text",
      "page_no": 4,
      "order": 23,
      "bbox": [
        93.8,
        302.57,
        782.0,
        413.4
      ],
      "text": "Ollama / LM Studio : UI + API\nMLX : Apple Silicon 特化\nハードウェア実用ライン (4bit 量子化前提 )",
      "normalized_text": "Ollama / LM Studio : UI + API\nMLX : Apple Silicon 特化\nハードウェア実用ライン (4bit 量子化前提 )",
      "heading_level": 3,
      "numbers": [
        "4"
      ],
      "hash": "54f0bd06e147c3e6",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p004_c00024",
      "block_type": "text",
      "page_no": 4,
      "order": 24,
      "bbox": [
        60.8,
        428.87,
        227.36,
        487.33
      ],
      "text": "Apple Silicon (Unified Memory)\n16GB : 3B 〜 7/8B 級\n24-32GB : 7B 〜 14B 級 ( 業務最小ライン )",
      "normalized_text": "Apple Silicon (Unified Memory)\n16GB : 3B 〜 7/8B 級\n24-32GB : 7B 〜 14B 級 ( 業務最小ライン )",
      "heading_level": 3,
      "numbers": [
        "16",
        "3",
        "7",
        "8",
        "24",
        "32",
        "7",
        "14"
      ],
      "hash": "8ecc5f9899a0aaca",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p004_c00025",
      "block_type": "text",
      "page_no": 4,
      "order": 25,
      "bbox": [
        324.92,
        399.0,
        915.97,
        538.55
      ],
      "text": "Windows (NVIDIA GPU VRAM)\n8GB : 7B 級 / 12GB : 14B 級\n24GB : 27B 〜 34B 級 ( 実務域 )\n推奨スタック\n低コスト : GGUF + llama.cpp\n高品質 : GPU + TensorRT-LLM\nハード制約 : 3B 級 + RAG 工夫\n補足:長文コンテキストは KV キャッシュがメモリを支配します。最終判断は再現ベンチマーク(後述の手順)で確定してください。",
      "normalized_text": "Windows (NVIDIA GPU VRAM)\n8GB : 7B 級 / 12GB : 14B 級\n24GB : 27B 〜 34B 級 ( 実務域 )\n推奨スタック\n低コスト : GGUF + llama.cpp\n高品質 : GPU + TensorRT-LLM\nハード制約 : 3B 級 + RAG 工夫\n補足:長文コンテキストは KV キャッシュがメモリを支配します。最終判断は再現ベンチマーク(後述の手順)で確定してください。",
      "heading_level": 3,
      "numbers": [
        "8",
        "7",
        "12",
        "14",
        "24",
        "27",
        "34",
        "3"
      ],
      "hash": "9ba2bc50f397f8ed",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p005_c00026",
      "block_type": "text",
      "page_no": 5,
      "order": 26,
      "bbox": [
        58.57,
        24.28,
        916.32,
        164.37
      ],
      "text": "エグゼクティブサマリ(詳細 1 ) ローカル AI の定義と中核技術\n結論:ローカル AI は「推論がユーザー管理下で完結する構成」と定義され、 重み量子化( 4bit ) と KV キャッシュ最適化 が実運用の技術的基盤です。",
      "normalized_text": "エグゼクティブサマリ(詳細 1 ) ローカル AI の定義と中核技術\n結論:ローカル AI は「推論がユーザー管理下で完結する構成」と定義され、 重み量子化( 4bit ) と KV キャッシュ最適化 が実運用の技術的基盤です。",
      "heading_level": 3,
      "numbers": [
        "1 ",
        "4"
      ],
      "hash": "58c3be947a9998a8",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p005_c00027",
      "block_type": "text",
      "page_no": 5,
      "order": 27,
      "bbox": [
        68.85,
        241.22,
        903.79,
        497.23
      ],
      "text": "ローカル AI の定義と射程\nユーザー端末(オンデバイス)、 PC 、またはローカル LAN 内サーバで完 結。\n入力データ(文書・音声・画像)がデフォルトでクラウドへ送出されな い構成。\nLM Studio や Ollama の「 localhost/network 公開」機能を含みます。\n[1][2]\nLocalhost On-Premise LAN Privacy First\n中核技術( Enabling Technologies )\n4bit 前後の weight-only 量子化( AWQ/GGUF 等)が主流。メモリ消費を劇 的に削減し、コンシューマ機での実行を可能に。 [4]\n長文・多同時接続のメモリ管理が鍵。\nPaged KV Cache: メモリ断片化を防ぐ( vLLM 等) [5]\nQuantized KV: KV を FP8/INT4 化して容量削減 [6]\nKV Reuse / Prompt Cache: 計算再利用で高速化 [7]",
      "normalized_text": "ローカル AI の定義と射程\nユーザー端末(オンデバイス)、 PC 、またはローカル LAN 内サーバで完 結。\n入力データ(文書・音声・画像)がデフォルトでクラウドへ送出されな い構成。\nLM Studio や Ollama の「 localhost/network 公開」機能を含みます。\n[1][2]\nLocalhost On-Premise LAN Privacy First\n中核技術( Enabling Technologies )\n4bit 前後の weight-only 量子化( AWQ/GGUF 等)が主流。メモリ消費を劇 的に削減し、コンシューマ機での実行を可能に。 [4]\n長文・多同時接続のメモリ管理が鍵。\nPaged KV Cache: メモリ断片化を防ぐ( vLLM 等) [5]\nQuantized KV: KV を FP8/INT4 化して容量削減 [6]\nKV Reuse / Prompt Cache: 計算再利用で高速化 [7]",
      "heading_level": 3,
      "numbers": [
        "1",
        "2",
        "4",
        "4",
        "5",
        "8",
        "4 ",
        "6",
        "7"
      ],
      "hash": "e08a8bc19e343929",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p006_c00028",
      "block_type": "text",
      "page_no": 6,
      "order": 28,
      "bbox": [
        55.54,
        20.38,
        925.06,
        412.0
      ],
      "text": "エグゼクティブサマリ(詳細 2 ) 実運用ランタイムとハードウェア実用ライン\n結論:ランタイムは llama.cpp/GGUF 系・ MLX が第一選択となり、ハードウェアはメモリ容量で Tier 化されます(長文は KV 支配)。\n実運用ランタイム(第一選択)\nクロスプラットフォーム標準: llama.cpp\nGGUF 形式必須。最小セットアップで CPU/GPU 推論が可能。\n軽量・汎用で多くのフロントエンドの基盤。 [8]\nUI +ローカル API : Ollama / LM Studio\nOpenAI 互換 API サーバとして動作( LAN 公開可)。\nWindows/macOS/Linux 対応で導入が容易。 [23]\nApple Silicon 特化: MLX\nUnified Memory 前提で設計された配列フレームワーク。\nMLX-LM/MLX-VLM により Metal 最適化推論を実現。 [22]",
      "normalized_text": "エグゼクティブサマリ(詳細 2 ) 実運用ランタイムとハードウェア実用ライン\n結論:ランタイムは llama.cpp/GGUF 系・ MLX が第一選択となり、ハードウェアはメモリ容量で Tier 化されます(長文は KV 支配)。\n実運用ランタイム(第一選択)\nクロスプラットフォーム標準: llama.cpp\nGGUF 形式必須。最小セットアップで CPU/GPU 推論が可能。\n軽量・汎用で多くのフロントエンドの基盤。 [8]\nUI +ローカル API : Ollama / LM Studio\nOpenAI 互換 API サーバとして動作( LAN 公開可)。\nWindows/macOS/Linux 対応で導入が容易。 [23]\nApple Silicon 特化: MLX\nUnified Memory 前提で設計された配列フレームワーク。\nMLX-LM/MLX-VLM により Metal 最適化推論を実現。 [22]",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "8",
        "23",
        "22"
      ],
      "hash": "a2627df2ea692e59",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p006_c00029",
      "block_type": "text",
      "page_no": 6,
      "order": 29,
      "bbox": [
        520.76,
        205.97,
        773.44,
        306.79
      ],
      "text": "ハードウェア実用ライン( 4bit 中心概算)\nApple Silicon (Unified Memory):\n3B 〜 7/8B 級(インタラクティブ) 16GB\n7B 〜 14B 級(業務実用・ RAG 最小ライン) 24-32GB\n14B 〜 32B 級以上(高品質・長文要約可) 64GB+",
      "normalized_text": "ハードウェア実用ライン( 4bit 中心概算)\nApple Silicon (Unified Memory):\n3B 〜 7/8B 級(インタラクティブ) 16GB\n7B 〜 14B 級(業務実用・ RAG 最小ライン) 24-32GB\n14B 〜 32B 級以上(高品質・長文要約可) 64GB+",
      "heading_level": 3,
      "numbers": [
        "4",
        "3",
        "7",
        "8",
        "16",
        "7",
        "14",
        "24",
        "32",
        "14",
        "32",
        "64"
      ],
      "hash": "bdf1ae057c3fd31c",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p006_c00030",
      "block_type": "text",
      "page_no": 6,
      "order": 30,
      "bbox": [
        522.27,
        320.3,
        753.51,
        387.56
      ],
      "text": "Windows (Discrete GPU):\n7B 級(最小ライン) VRAM 8GB\n14B 級〜 27B 級(品質寄り) VRAM 12-16GB\n32-34B 級・重い画像生成が実務域 VRAM 24GB",
      "normalized_text": "Windows (Discrete GPU):\n7B 級(最小ライン) VRAM 8GB\n14B 級〜 27B 級(品質寄り) VRAM 12-16GB\n32-34B 級・重い画像生成が実務域 VRAM 24GB",
      "heading_level": 3,
      "numbers": [
        "7",
        "8",
        "14",
        "27",
        "12",
        "16",
        "32",
        "34",
        "24"
      ],
      "hash": "9c7b74790ac3a628",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p006_c00031",
      "block_type": "text",
      "page_no": 6,
      "order": 31,
      "bbox": [
        66.79,
        401.11,
        914.21,
        508.46
      ],
      "text": "Windows CPU-only:\n7B 級が「使える」境界( AVX 活用)。 32GB RAM+\n注意:長文コンテキストとメモリ\n上記は重み( weight-only )の概算です。長文( Long Context )を扱う場合、 KV キャッシュがメモリを線形に圧迫するため、別途 KV 量子化やコンテキスト長制限( 4k/8k 等)の設計が必要です。",
      "normalized_text": "Windows CPU-only:\n7B 級が「使える」境界( AVX 活用)。 32GB RAM+\n注意:長文コンテキストとメモリ\n上記は重み( weight-only )の概算です。長文( Long Context )を扱う場合、 KV キャッシュがメモリを線形に圧迫するため、別途 KV 量子化やコンテキスト長制限( 4k/8k 等)の設計が必要です。",
      "heading_level": 3,
      "numbers": [
        "7",
        "32",
        "4",
        "8"
      ],
      "hash": "7400335408b9658d",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p007_c00032",
      "block_type": "text",
      "page_no": 7,
      "order": 32,
      "bbox": [
        58.57,
        21.26,
        910.9,
        213.4
      ],
      "text": "エグゼクティブサマリ(詳細 3 ) 推奨スタック 3 パターン( Tier 別構成案)\n結論:用途とリソースに応じて「低コスト/高品質/ハード制約」の 3 構成を選択します。 ※各構成の具体的な失敗モードと回避策は、後述の「ユースケース別推奨スタック」章で詳細に展開します。\n低コスト構成",
      "normalized_text": "エグゼクティブサマリ(詳細 3 ) 推奨スタック 3 パターン( Tier 別構成案)\n結論:用途とリソースに応じて「低コスト/高品質/ハード制約」の 3 構成を選択します。 ※各構成の具体的な失敗モードと回避策は、後述の「ユースケース別推奨スタック」章で詳細に展開します。\n低コスト構成",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "3 ",
        "3 "
      ],
      "hash": "58e05cc07d7f9dfd",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p007_c00033",
      "block_type": "text",
      "page_no": 7,
      "order": 33,
      "bbox": [
        51.79,
        248.63,
        272.35,
        283.4
      ],
      "text": "Target HW\nCPU のみ / エントリー GPU / Apple 16GB",
      "normalized_text": "Target HW\nCPU のみ / エントリー GPU / Apple 16GB",
      "heading_level": 3,
      "numbers": [
        "16"
      ],
      "hash": "a75aa8455df88ea5",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p007_c00034",
      "block_type": "text",
      "page_no": 7,
      "order": 34,
      "bbox": [
        51.79,
        303.08,
        275.28,
        345.02
      ],
      "text": "LLM Runtime\nllama.cpp (GGUF Q4/K) Ollama / LM Studio",
      "normalized_text": "LLM Runtime\nllama.cpp (GGUF Q4/K) Ollama / LM Studio",
      "heading_level": 3,
      "numbers": [
        "4"
      ],
      "hash": "2e33a8215158eca6",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p007_c00035",
      "block_type": "text",
      "page_no": 7,
      "order": 35,
      "bbox": [
        51.79,
        373.32,
        275.8,
        415.98
      ],
      "text": "ASR / TTS\nfaster-whisper (INT8) Piper / Kokoro ( 軽量 )",
      "normalized_text": "ASR / TTS\nfaster-whisper (INT8) Piper / Kokoro ( 軽量 )",
      "heading_level": 3,
      "numbers": [
        "8"
      ],
      "hash": "25643e3583fbb9d6",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p007_c00036",
      "block_type": "text",
      "page_no": 7,
      "order": 36,
      "bbox": [
        51.79,
        198.25,
        533.46,
        546.16
      ],
      "text": "RAG / Embedding\nBGE-M3 ( 多言語・多用途 ) bge-reranker-base\n最低限の業務自動化\n高品質構成",
      "normalized_text": "RAG / Embedding\nBGE-M3 ( 多言語・多用途 ) bge-reranker-base\n最低限の業務自動化\n高品質構成",
      "heading_level": 3,
      "numbers": [
        "3 "
      ],
      "hash": "0f9e243521fc4cfe",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p007_c00037",
      "block_type": "text",
      "page_no": 7,
      "order": 37,
      "bbox": [
        352.9,
        248.63,
        579.24,
        291.3
      ],
      "text": "Target HW\nGPU 搭載機 (VRAM 16-24GB+) Apple 64GB+",
      "normalized_text": "Target HW\nGPU 搭載機 (VRAM 16-24GB+) Apple 64GB+",
      "heading_level": 3,
      "numbers": [
        "16",
        "24",
        "64"
      ],
      "hash": "9c8bf322c2c073ba",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p007_c00038",
      "block_type": "text",
      "page_no": 7,
      "order": 38,
      "bbox": [
        352.9,
        318.87,
        600.25,
        361.53
      ],
      "text": "LLM Runtime\nGPU 優先 (TensorRT-LLM / vLLM) 32B 〜 70B 級モデル",
      "normalized_text": "LLM Runtime\nGPU 優先 (TensorRT-LLM / vLLM) 32B 〜 70B 級モデル",
      "heading_level": 3,
      "numbers": [
        "32",
        "70"
      ],
      "hash": "d7cf4c34845a7792",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p007_c00039",
      "block_type": "text",
      "page_no": 7,
      "order": 39,
      "bbox": [
        352.9,
        389.11,
        586.49,
        431.77
      ],
      "text": "ASR / TTS\nfaster-whisper (GPU) XTTS / StyleTTS2 ( 高品質 )",
      "normalized_text": "ASR / TTS\nfaster-whisper (GPU) XTTS / StyleTTS2 ( 高品質 )",
      "heading_level": 3,
      "numbers": [
        "2 "
      ],
      "hash": "edfdd88ced7e4299",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p007_c00040",
      "block_type": "text",
      "page_no": 7,
      "order": 40,
      "bbox": [
        352.9,
        198.25,
        849.92,
        546.16
      ],
      "text": "RAG / VLM\nbge-reranker-large Qwen2-VL 7B / InternVL2\nリッチな体験・高精度\nハード制約構成",
      "normalized_text": "RAG / VLM\nbge-reranker-large Qwen2-VL 7B / InternVL2\nリッチな体験・高精度\nハード制約構成",
      "heading_level": 3,
      "numbers": [
        "2",
        "7",
        "2\n"
      ],
      "hash": "c3534304188f0a82",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p007_c00041",
      "block_type": "text",
      "page_no": 7,
      "order": 41,
      "bbox": [
        654.08,
        248.63,
        883.88,
        291.3
      ],
      "text": "Target HW\n低スペック PC / 古い Mac メモリ 8GB 以下等",
      "normalized_text": "Target HW\n低スペック PC / 古い Mac メモリ 8GB 以下等",
      "heading_level": 3,
      "numbers": [
        "8"
      ],
      "hash": "38e61d3a4ad77165",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p007_c00042",
      "block_type": "text",
      "page_no": 7,
      "order": 42,
      "bbox": [
        654.08,
        318.87,
        879.56,
        361.53
      ],
      "text": "LLM Runtime\nPhi-3 mini (3.8B) 等 GGUF Q4 ( 極小モデル )",
      "normalized_text": "LLM Runtime\nPhi-3 mini (3.8B) 等 GGUF Q4 ( 極小モデル )",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "3.8",
        "4 "
      ],
      "hash": "b3ad89581de4f87d",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p007_c00043",
      "block_type": "text",
      "page_no": 7,
      "order": 43,
      "bbox": [
        654.08,
        389.11,
        905.61,
        436.96
      ],
      "text": "Strategy\n短いコンテキストで完結させる RAG は Embedding 検\n索を重視",
      "normalized_text": "Strategy\n短いコンテキストで完結させる RAG は Embedding 検\n索を重視",
      "heading_level": 3,
      "numbers": [],
      "hash": "283f108eaf2156ed",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p007_c00044",
      "block_type": "text",
      "page_no": 7,
      "order": 44,
      "bbox": [
        654.08,
        459.35,
        907.79,
        546.16
      ],
      "text": "Compromise\nLLM 自体を軽くし、検索精度で補う生成タスクを限\n定する\n極限環境での動作",
      "normalized_text": "Compromise\nLLM 自体を軽くし、検索精度で補う生成タスクを限\n定する\n極限環境での動作",
      "heading_level": 3,
      "numbers": [],
      "hash": "8b9bb0045f9e621b",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p008_c00045",
      "block_type": "text",
      "page_no": 8,
      "order": 45,
      "bbox": [
        75.05,
        22.05,
        904.36,
        48.43
      ],
      "text": "ローカル AI の定義とスコープ Definition & Scope",
      "normalized_text": "ローカル AI の定義とスコープ Definition & Scope",
      "heading_level": 3,
      "numbers": [],
      "hash": "e2c5600e10723811",
      "meta": {
        "body_font_size": 12.27
      }
    },
    {
      "chunk_id": "document_p008_c00046",
      "block_type": "text",
      "page_no": 8,
      "order": 46,
      "bbox": [
        66.07,
        119.8,
        907.05,
        549.28
      ],
      "text": "DEFINITION\n本資料の「ローカル AI 」とは、推論がユーザー管理下(端末/ローカル PC /ローカル LAN )で完結し、 入力データが外部へ送信されない構成を指します。\nスコープの射程(範囲)\nローカル API サーバを含む : localhost だけでなく、 LAN 公開( network )された推論サーバも対象 。\nLM Studio / Ollama\n実装例 : Windows/macOS/Linux 上で動作する Ollama 、 LM Studio 等の OpenAI 互 換 API サーバ。\n非対象 : 推論リクエストがインターネット経由で外部クラウド API ( OpenAI, Anthropic 等)へ飛ぶ構成。\n設計含意( Design Implication )\nプライバシー最優先 : 機密データ(個人情報・社外秘)を入力しても外部流出しない安全 性を担保。\nコスト固定化 : トークン課金を回避し、ハードウェア初期投資のみでランニングコ ストを抑制。\n耐障害性 : ネットワーク切断時でも推論機能が継続動作する自律 性。\n補足:ネットワーク利用の例外",
      "normalized_text": "DEFINITION\n本資料の「ローカル AI 」とは、推論がユーザー管理下(端末/ローカル PC /ローカル LAN )で完結し、 入力データが外部へ送信されない構成を指します。\nスコープの射程(範囲)\nローカル API サーバを含む : localhost だけでなく、 LAN 公開( network )された推論サーバも対象 。\nLM Studio / Ollama\n実装例 : Windows/macOS/Linux 上で動作する Ollama 、 LM Studio 等の OpenAI 互 換 API サーバ。\n非対象 : 推論リクエストがインターネット経由で外部クラウド API ( OpenAI, Anthropic 等)へ飛ぶ構成。\n設計含意( Design Implication )\nプライバシー最優先 : 機密データ(個人情報・社外秘)を入力しても外部流出しない安全 性を担保。\nコスト固定化 : トークン課金を回避し、ハードウェア初期投資のみでランニングコ ストを抑制。\n耐障害性 : ネットワーク切断時でも推論機能が継続動作する自律 性。\n補足:ネットワーク利用の例外",
      "heading_level": 3,
      "numbers": [],
      "hash": "d22419e5acd65287",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 10
      }
    },
    {
      "chunk_id": "document_p009_c00047",
      "block_type": "text",
      "page_no": 9,
      "order": 47,
      "bbox": [
        62.19,
        25.41,
        912.74,
        160.02
      ],
      "text": "未指定事項の扱い(前提条件) オフライン要件・対象 OS ・ライセンス\n結論:「完全オフライン」は推論実行時のみを必須要件とし、 導入・更新時のネットワーク利用は許容する現実的な設計を前提とします。",
      "normalized_text": "未指定事項の扱い(前提条件) オフライン要件・対象 OS ・ライセンス\n結論:「完全オフライン」は推論実行時のみを必須要件とし、 導入・更新時のネットワーク利用は許容する現実的な設計を前提とします。",
      "heading_level": 0,
      "numbers": [],
      "hash": "5db7ba1efe9ccf65",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p009_c00048",
      "block_type": "text",
      "page_no": 9,
      "order": 48,
      "bbox": [
        36.03,
        233.23,
        923.26,
        536.85
      ],
      "text": "オフライン要件と対象 OS\n“ 完全オフライン ” の定義: 「モデル推論自体はオフラインで成立する」レベルを基本とします。モ デルの初回ダウンロードや RAG 文書の取り込みプロセスにはネットワー ク接続が必要になり得る点を前提とします。\n[19]\n対象 OS のスコープ: macOS Ventura 以降および Windows 10/11 を主要ターゲットとします。 Linux は「 Windows 上で WSL2 を利用」または「別筐体サーバ」の選択肢 としてのみ扱います。\nOffline Inference macOS / Windows WSL2 Option\n実装環境とライセンス管理\nLinux 前提の技術( TensorRT-LLM 等): NVIDIA TensorRT-LLM などは Linux 環境でのドキュメントが中心です。 Windows でこれらを利用する場合、 WSL2 上での Docker 運用が実務的な解 決策となります。\n[20]\nライセンスの分離管理: 「推論コード( OSS ライセンス)」と「モデル重み(商用利用制限など )」は別物です。特に画像生成や音声系モデルでは重みの利用条件が 厳格な場合があるため、個別のリスク確認を必須とします。\nLM Studio の RAG 仕様に関する補足\nLocal AI Technical Survey Report 2026 9 / 80",
      "normalized_text": "オフライン要件と対象 OS\n“ 完全オフライン ” の定義: 「モデル推論自体はオフラインで成立する」レベルを基本とします。モ デルの初回ダウンロードや RAG 文書の取り込みプロセスにはネットワー ク接続が必要になり得る点を前提とします。\n[19]\n対象 OS のスコープ: macOS Ventura 以降および Windows 10/11 を主要ターゲットとします。 Linux は「 Windows 上で WSL2 を利用」または「別筐体サーバ」の選択肢 としてのみ扱います。\nOffline Inference macOS / Windows WSL2 Option\n実装環境とライセンス管理\nLinux 前提の技術( TensorRT-LLM 等): NVIDIA TensorRT-LLM などは Linux 環境でのドキュメントが中心です。 Windows でこれらを利用する場合、 WSL2 上での Docker 運用が実務的な解 決策となります。\n[20]\nライセンスの分離管理: 「推論コード( OSS ライセンス)」と「モデル重み(商用利用制限など )」は別物です。特に画像生成や音声系モデルでは重みの利用条件が 厳格な場合があるため、個別のリスク確認を必須とします。\nLM Studio の RAG 仕様に関する補足\nLocal AI Technical Survey Report 2026 9 / 80",
      "heading_level": 3,
      "numbers": [
        "19",
        "10",
        "11 ",
        "2 ",
        "2 ",
        "2 ",
        "20",
        "2026 ",
        "9 ",
        "80"
      ],
      "hash": "ab6e14b56f3fb98d",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p010_c00049",
      "block_type": "text",
      "page_no": 10,
      "order": 49,
      "bbox": [
        60.01,
        112.77,
        159.84,
        213.28
      ],
      "text": "04",
      "normalized_text": "04",
      "heading_level": 1,
      "numbers": [
        "04"
      ],
      "hash": "b440dae0d6b8dc0c",
      "meta": {
        "body_font_size": 12.98
      }
    },
    {
      "chunk_id": "document_p010_c00050",
      "block_type": "text",
      "page_no": 10,
      "order": 50,
      "bbox": [
        60.01,
        262.39,
        349.67,
        409.99
      ],
      "text": "ローカル AI ランドスケープ\nオンデバイス実行環境の 全体像と技術スタック",
      "normalized_text": "ローカル AI ランドスケープ\nオンデバイス実行環境の 全体像と技術スタック",
      "heading_level": 1,
      "numbers": [],
      "hash": "31c20d0c36b0e765",
      "meta": {
        "body_font_size": 12.98,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p010_c00051",
      "block_type": "text",
      "page_no": 10,
      "order": 51,
      "bbox": [
        492.23,
        86.78,
        883.16,
        449.91
      ],
      "text": "KE Y TAKEAWAYS\n本章の要点\nテキスト LLM の二大潮流:\nGGUF ( llama.cpp 系)と GPU 量子化形式( GPTQ/AWQ/EXL2 等 )に大別されます。\nプラットフォームの最適解:\nmacOS は Metal/Unified Memory 、 Windows は Ollama/LM Studio が実務的選択肢です。\n構成要素の多様性:\nLLM だけでなく、 ASR/TTS/VLM/RAG/Agent まで網羅的にマッ プ化します。\nカテゴリ網羅図 代表モデル ランタイム 主用途",
      "normalized_text": "KE Y TAKEAWAYS\n本章の要点\nテキスト LLM の二大潮流:\nGGUF ( llama.cpp 系)と GPU 量子化形式( GPTQ/AWQ/EXL2 等 )に大別されます。\nプラットフォームの最適解:\nmacOS は Metal/Unified Memory 、 Windows は Ollama/LM Studio が実務的選択肢です。\n構成要素の多様性:\nLLM だけでなく、 ASR/TTS/VLM/RAG/Agent まで網羅的にマッ プ化します。\nカテゴリ網羅図 代表モデル ランタイム 主用途",
      "heading_level": 3,
      "numbers": [
        "2 "
      ],
      "hash": "112cd95b0f0756bd",
      "meta": {
        "body_font_size": 12.98,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p011_c00052",
      "block_type": "text",
      "page_no": 11,
      "order": 52,
      "bbox": [
        36.03,
        19.33,
        925.32,
        531.79
      ],
      "text": "ローカル AI ランドスケープ カテゴリ網羅・代表モデルとランタイムの関係図\nLocal AI Technical Survey Report 2026 11 / 80\n結論:ローカル AI は 9 カテゴリ( LLM/VLM/ASR/TTS/Embedding/OCR/ 画像生成 /Agent/ 音声前処理)で構成され、 モデル系とランタイム系が実務上の結節点となります。\nCore (AI Category) Model Examples Runtime / Stack",
      "normalized_text": "ローカル AI ランドスケープ カテゴリ網羅・代表モデルとランタイムの関係図\nLocal AI Technical Survey Report 2026 11 / 80\n結論:ローカル AI は 9 カテゴリ( LLM/VLM/ASR/TTS/Embedding/OCR/ 画像生成 /Agent/ 音声前処理)で構成され、 モデル系とランタイム系が実務上の結節点となります。\nCore (AI Category) Model Examples Runtime / Stack",
      "heading_level": 3,
      "numbers": [
        "2026 ",
        "11 ",
        "80\n",
        "9 "
      ],
      "hash": "dd45165db37f5b40",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p012_c00053",
      "block_type": "text",
      "page_no": 12,
      "order": 53,
      "bbox": [
        58.57,
        23.08,
        925.06,
        129.02
      ],
      "text": "ローカル AI ランドスケープ(詳細) カテゴリ別代表モデル・ランタイム・主用途\n結論:実務上の収束点は「 LLM は GGUF (llama.cpp) か GPU 量子化系 」「 macOS は MLX/Unified Memory 最適化」「 Windows は Ollama/LM Studio + 必要に 応じ WSL2 」。",
      "normalized_text": "ローカル AI ランドスケープ(詳細) カテゴリ別代表モデル・ランタイム・主用途\n結論:実務上の収束点は「 LLM は GGUF (llama.cpp) か GPU 量子化系 」「 macOS は MLX/Unified Memory 最適化」「 Windows は Ollama/LM Studio + 必要に 応じ WSL2 」。",
      "heading_level": 3,
      "numbers": [
        "2 "
      ],
      "hash": "435c5c13781ec2cc",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p012_c00054",
      "block_type": "text",
      "page_no": 12,
      "order": 54,
      "bbox": [
        47.25,
        183.8,
        285.25,
        271.47
      ],
      "text": "テキスト LLM\n形式 : GGUF (llama.cpp 必須 ), GPTQ/AWQ/EXL2 (GPU 向け ) [21]\n代表 : Llama 3.1, Qwen2.5, Gemma 2, Phi-3/4\n用途 : 汎用対話 , 要約 , RAG, 翻訳",
      "normalized_text": "テキスト LLM\n形式 : GGUF (llama.cpp 必須 ), GPTQ/AWQ/EXL2 (GPU 向け ) [21]\n代表 : Llama 3.1, Qwen2.5, Gemma 2, Phi-3/4\n用途 : 汎用対話 , 要約 , RAG, 翻訳",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "21",
        "3.1",
        "2.5",
        "2, ",
        "3",
        "4\n"
      ],
      "hash": "9430f84c6cfd8c18",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p012_c00055",
      "block_type": "text",
      "page_no": 12,
      "order": 55,
      "bbox": [
        347.44,
        184.22,
        556.19,
        257.92
      ],
      "text": "VLM / LMM\n代表 : Qwen2-VL, Phi-3-Vision, LLaVA\n実行 : MLX-VLM (Mac 最適化 ), Transformers, vLLM [28]\n用途 : 画像理解 , OCR 代替 , UI 解析",
      "normalized_text": "VLM / LMM\n代表 : Qwen2-VL, Phi-3-Vision, LLaVA\n実行 : MLX-VLM (Mac 最適化 ), Transformers, vLLM [28]\n用途 : 画像理解 , OCR 代替 , UI 解析",
      "heading_level": 3,
      "numbers": [
        "2",
        "3",
        "28"
      ],
      "hash": "35c4cf80cd605cdc",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p012_c00056",
      "block_type": "text",
      "page_no": 12,
      "order": 56,
      "bbox": [
        647.6,
        183.8,
        895.78,
        257.92
      ],
      "text": "ASR (音声認識)\n方式 : Whisper 系 (faster-whisper, whisper.cpp) [50] [51]\n特徴 : CTranslate2 で高速・省メモリ , C++ 軽量実装\n用途 : 議事録 , 音声コマンド , 文字起こし",
      "normalized_text": "ASR (音声認識)\n方式 : Whisper 系 (faster-whisper, whisper.cpp) [50] [51]\n特徴 : CTranslate2 で高速・省メモリ , C++ 軽量実装\n用途 : 議事録 , 音声コマンド , 文字起こし",
      "heading_level": 3,
      "numbers": [
        "50",
        "51",
        "2 "
      ],
      "hash": "717290039aee5a39",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p012_c00057",
      "block_type": "text",
      "page_no": 12,
      "order": 57,
      "bbox": [
        47.25,
        314.08,
        282.75,
        388.29
      ],
      "text": "TTS (音声合成)\n軽量 : Piper / Kokoro ( 高速・ CPU 向け ) [52][53]\n高品質 : XTTS / StyleTTS2 ( クローン・ GPU 推奨 ) [54][55]\n用途 : 読み上げ , 通知 , ボイスボット",
      "normalized_text": "TTS (音声合成)\n軽量 : Piper / Kokoro ( 高速・ CPU 向け ) [52][53]\n高品質 : XTTS / StyleTTS2 ( クローン・ GPU 推奨 ) [54][55]\n用途 : 読み上げ , 通知 , ボイスボット",
      "heading_level": 3,
      "numbers": [
        "52",
        "53",
        "2 ",
        "54",
        "55"
      ],
      "hash": "78ad0d9b51a55d2f",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p012_c00058",
      "block_type": "text",
      "page_no": 12,
      "order": 58,
      "bbox": [
        347.44,
        314.54,
        580.9,
        388.29
      ],
      "text": "Embedding/Reranker\n代表 : bge-m3, multilingual-e5, bge-reranker [58-62]\n重要性 : RAG の検索精度( Recall/Precision )を決定\n用途 : ベクトル検索 , 再ランキング",
      "normalized_text": "Embedding/Reranker\n代表 : bge-m3, multilingual-e5, bge-reranker [58-62]\n重要性 : RAG の検索精度( Recall/Precision )を決定\n用途 : ベクトル検索 , 再ランキング",
      "heading_level": 3,
      "numbers": [
        "3, ",
        "5, ",
        "58",
        "62"
      ],
      "hash": "3222bacf653c9b2b",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p012_c00059",
      "block_type": "text",
      "page_no": 12,
      "order": 59,
      "bbox": [
        47.25,
        314.54,
        853.64,
        505.05
      ],
      "text": "OCR / Document AI\n古典 : Tesseract / PaddleOCR ( 構造化 ) [64][66]\nAI 型 : Donut / LayoutLMv3 ( 文書理解 ) [68][69]\n用途 : PDF 解析 , 請求書読取 , スキャン処理\n画像生成\n代表 : SDXL, FLUX.1 ( 要 VRAM 確認 ) [71][75]\n実行 : ComfyUI, SD WebUI [72] [73]\n用途 : 素材生成 , UI モック , コンテンツ制作",
      "normalized_text": "OCR / Document AI\n古典 : Tesseract / PaddleOCR ( 構造化 ) [64][66]\nAI 型 : Donut / LayoutLMv3 ( 文書理解 ) [68][69]\n用途 : PDF 解析 , 請求書読取 , スキャン処理\n画像生成\n代表 : SDXL, FLUX.1 ( 要 VRAM 確認 ) [71][75]\n実行 : ComfyUI, SD WebUI [72] [73]\n用途 : 素材生成 , UI モック , コンテンツ制作",
      "heading_level": 3,
      "numbers": [
        "64",
        "66",
        "3 ",
        "68",
        "69",
        "1 ",
        "71",
        "75",
        "72",
        "73"
      ],
      "hash": "51d86803657c7abc",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p012_c00060",
      "block_type": "text",
      "page_no": 12,
      "order": 60,
      "bbox": [
        36.03,
        430.89,
        921.95,
        534.56
      ],
      "text": "Agent / Tool-use\n方式 : OpenAI 互換 API での Function Calling [24]\n実行 : Ollama / LM Studio が実務的 [23][41]\n用途 : タスク自動化 , 外部ツール連携\n音声前処理\n代表 : Silero VAD, pyannote, RNNoise [82][84]\n役割 : 品質の下限担保(無音除去・話者分離)\n用途 : 会議録音の前処理 , ノイズ除去\nLocal AI Technical Survey Report 2026 12 / 80",
      "normalized_text": "Agent / Tool-use\n方式 : OpenAI 互換 API での Function Calling [24]\n実行 : Ollama / LM Studio が実務的 [23][41]\n用途 : タスク自動化 , 外部ツール連携\n音声前処理\n代表 : Silero VAD, pyannote, RNNoise [82][84]\n役割 : 品質の下限担保(無音除去・話者分離)\n用途 : 会議録音の前処理 , ノイズ除去\nLocal AI Technical Survey Report 2026 12 / 80",
      "heading_level": 3,
      "numbers": [
        "24",
        "23",
        "41",
        "82",
        "84",
        "2026 ",
        "12 ",
        "80"
      ],
      "hash": "9add5c843f324ea3",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 8
      }
    },
    {
      "chunk_id": "document_p013_c00061",
      "block_type": "text",
      "page_no": 13,
      "order": 61,
      "bbox": [
        60.01,
        89.61,
        159.84,
        190.12
      ],
      "text": "05",
      "normalized_text": "05",
      "heading_level": 1,
      "numbers": [
        "05"
      ],
      "hash": "5672cbcf9c9ebb0e",
      "meta": {
        "body_font_size": 12.98
      }
    },
    {
      "chunk_id": "document_p013_c00062",
      "block_type": "text",
      "page_no": 13,
      "order": 62,
      "bbox": [
        60.01,
        242.13,
        349.67,
        433.12
      ],
      "text": "特徴軸(評価軸 ) の定義\n評価の共通物差しと 測定基準",
      "normalized_text": "特徴軸(評価軸 ) の定義\n評価の共通物差しと 測定基準",
      "heading_level": 3,
      "numbers": [],
      "hash": "ef62f82183a3fa3e",
      "meta": {
        "body_font_size": 12.98,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p013_c00063",
      "block_type": "text",
      "page_no": 13,
      "order": 63,
      "bbox": [
        492.23,
        83.75,
        883.33,
        453.97
      ],
      "text": "KE Y TAKEAWAYS\n本章の要点\n13 の評価軸:\n指示追従、 JSON 堅牢性、長文耐性、速度、メモリなど、多 角的な視点でモデルを比較評価します。\nKV キャッシュ最適化:\npaged/quantized KV や reuse 機能が、長文コンテキストや多同 時リクエスト処理の鍵となります。\nJSON 制約の重要性:\nツール利用において JSON 出力の安定性は必須ですが、機能 はランタイムに強く依存します。\n13 の評価軸定義 測定手順 設計含意 KV 最適化",
      "normalized_text": "KE Y TAKEAWAYS\n本章の要点\n13 の評価軸:\n指示追従、 JSON 堅牢性、長文耐性、速度、メモリなど、多 角的な視点でモデルを比較評価します。\nKV キャッシュ最適化:\npaged/quantized KV や reuse 機能が、長文コンテキストや多同 時リクエスト処理の鍵となります。\nJSON 制約の重要性:\nツール利用において JSON 出力の安定性は必須ですが、機能 はランタイムに強く依存します。\n13 の評価軸定義 測定手順 設計含意 KV 最適化",
      "heading_level": 3,
      "numbers": [
        "13 ",
        "13 "
      ],
      "hash": "ff3618f922056650",
      "meta": {
        "body_font_size": 12.98,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p014_c00064",
      "block_type": "text",
      "page_no": 14,
      "order": 64,
      "bbox": [
        37.54,
        22.46,
        173.37,
        32.54
      ],
      "text": "Evaluation Metrics Overview",
      "normalized_text": "Evaluation Metrics Overview",
      "heading_level": 0,
      "numbers": [],
      "hash": "093afca0336e4131",
      "meta": {
        "body_font_size": 12.24,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p014_c00065",
      "block_type": "text",
      "page_no": 14,
      "order": 65,
      "bbox": [
        37.54,
        45.34,
        934.23,
        528.41
      ],
      "text": "特徴軸(評価軸)の定義 Total 13 Metrics\n01 指示追従 (Instruction Following) プロンプトの制約・禁止事項・形式指定(箇条書き禁止等)を遵守する能力。\n02 幻覚耐性 (Hallucination Resistance) 根拠のない固有名詞や数値を捏造せず、不確実性を提示できる性質。\n03 JSON 堅牢性 (Structured Output) スキーマ遵守・構文破壊率の低さ。ツール呼び出しやシステム連携の要。\n04 長文耐性 (Long-context Stability) 長い入力に対する要約崩れや重要情報の欠落、自己矛盾の抑制。\n05 推論計画 (Planning) タスク分解、複数ステップの自己検証、外部ツール呼び出し順序の妥当性。\n06 ツール呼び出し適性 (Tool-use) 関数呼び出しの意思決定、必要引数の抽出、曖昧性の解消能力。\n07 日本語品質 (Japanese Quality) 文法・敬語・語彙の自然さと、業務文書スタイルの再現性。\n08 音声品質 (TTS Quality) 自然さ(抑揚・間)、アクセント、ノイズの少なさ、声質再現性。\n09 画像理解精度 (VLM Accuracy) OCR 的読み取り、図表理解、空間関係、 UI スクリーンショットの解釈。\n10 速度 (Speed / Throughput) LLM の生成速度 (tok/s) および ASR/TTS の実時間比 (RTF) 。\n11 メモリ /VRAM (Memory Footprint) 重み+ KV キャッシュ+ランタイムオーバーヘッドの総消費量。\n12 量子化耐性 (Quantization Robustness) 4bit 等に圧縮した際の品質劣化の小ささ(モデルと手法の相性)。\n13 ライセンス (License & Compliance) コードの OSS ライセンスと、モデル重みの利用条件(商用制限等)の区別。\n各評価軸の測定手法と設計含意は、次スライド以降で詳細に解説します\nPage 14",
      "normalized_text": "特徴軸(評価軸)の定義 Total 13 Metrics\n01 指示追従 (Instruction Following) プロンプトの制約・禁止事項・形式指定(箇条書き禁止等)を遵守する能力。\n02 幻覚耐性 (Hallucination Resistance) 根拠のない固有名詞や数値を捏造せず、不確実性を提示できる性質。\n03 JSON 堅牢性 (Structured Output) スキーマ遵守・構文破壊率の低さ。ツール呼び出しやシステム連携の要。\n04 長文耐性 (Long-context Stability) 長い入力に対する要約崩れや重要情報の欠落、自己矛盾の抑制。\n05 推論計画 (Planning) タスク分解、複数ステップの自己検証、外部ツール呼び出し順序の妥当性。\n06 ツール呼び出し適性 (Tool-use) 関数呼び出しの意思決定、必要引数の抽出、曖昧性の解消能力。\n07 日本語品質 (Japanese Quality) 文法・敬語・語彙の自然さと、業務文書スタイルの再現性。\n08 音声品質 (TTS Quality) 自然さ(抑揚・間)、アクセント、ノイズの少なさ、声質再現性。\n09 画像理解精度 (VLM Accuracy) OCR 的読み取り、図表理解、空間関係、 UI スクリーンショットの解釈。\n10 速度 (Speed / Throughput) LLM の生成速度 (tok/s) および ASR/TTS の実時間比 (RTF) 。\n11 メモリ /VRAM (Memory Footprint) 重み+ KV キャッシュ+ランタイムオーバーヘッドの総消費量。\n12 量子化耐性 (Quantization Robustness) 4bit 等に圧縮した際の品質劣化の小ささ(モデルと手法の相性)。\n13 ライセンス (License & Compliance) コードの OSS ライセンスと、モデル重みの利用条件(商用制限等)の区別。\n各評価軸の測定手法と設計含意は、次スライド以降で詳細に解説します\nPage 14",
      "heading_level": 3,
      "numbers": [
        "13 ",
        "01 ",
        "02 ",
        "03 ",
        "04 ",
        "05 ",
        "06 ",
        "07 ",
        "08 ",
        "09 ",
        "10 ",
        "11 ",
        "12 ",
        "4",
        "13 ",
        "14"
      ],
      "hash": "d02f3b8ca240fb5d",
      "meta": {
        "body_font_size": 12.24,
        "body_line_count": 14
      }
    },
    {
      "chunk_id": "document_p015_c00066",
      "block_type": "text",
      "page_no": 15,
      "order": 66,
      "bbox": [
        30.04,
        19.66,
        919.99,
        40.59
      ],
      "text": "特徴軸(詳細):測定と設計含意 13 の評価軸における計測手順と技術的背景",
      "normalized_text": "特徴軸(詳細):測定と設計含意 13 の評価軸における計測手順と技術的背景",
      "heading_level": 0,
      "numbers": [
        "13 "
      ],
      "hash": "99ba83ca7c38d603",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p015_c00067",
      "block_type": "text",
      "page_no": 15,
      "order": 67,
      "bbox": [
        90.77,
        108.82,
        240.57,
        123.97
      ],
      "text": "機能・性能評価のコア",
      "normalized_text": "機能・性能評価のコア",
      "heading_level": 3,
      "numbers": [],
      "hash": "8cda7d6fa9d1643a",
      "meta": {
        "body_font_size": 10.08
      }
    },
    {
      "chunk_id": "document_p015_c00068",
      "block_type": "text",
      "page_no": 15,
      "order": 68,
      "bbox": [
        68.3,
        159.83,
        453.4,
        361.73
      ],
      "text": "JSON 堅牢性と指示追従\nツール呼び出し等の業務連携ではスキーマ破壊が致命的。 Ollama/LM Studio の OpenAI 互換 API を用いて同一条件で比較検証を行う。 llama-cpp-python 等のランタイム依存機 能( response_format )も活用。\n[24]\n[25]\n長文耐性とメモリ管理\nコンテキスト長に応じて KV キャッシュメモリは線形に増加し、支配的要因となる。単 純なトークン数だけでなく、 KV キャッシュの最適化技術( Paged Attention 等)の有無 が安定性を左右する。\n[5][6]\n速度指標 (tok/s & RTF)\nLLM は生成スループット (tok/s) だけでなく、初動遅延 (TTFT) を分離して測定。 ASR/TTS は 実時間比 (RTF) で評価し、実務要件( RTF<1 など)との適合を見る。",
      "normalized_text": "JSON 堅牢性と指示追従\nツール呼び出し等の業務連携ではスキーマ破壊が致命的。 Ollama/LM Studio の OpenAI 互換 API を用いて同一条件で比較検証を行う。 llama-cpp-python 等のランタイム依存機 能( response_format )も活用。\n[24]\n[25]\n長文耐性とメモリ管理\nコンテキスト長に応じて KV キャッシュメモリは線形に増加し、支配的要因となる。単 純なトークン数だけでなく、 KV キャッシュの最適化技術( Paged Attention 等)の有無 が安定性を左右する。\n[5][6]\n速度指標 (tok/s & RTF)\nLLM は生成スループット (tok/s) だけでなく、初動遅延 (TTFT) を分離して測定。 ASR/TTS は 実時間比 (RTF) で評価し、実務要件( RTF<1 など)との適合を見る。",
      "heading_level": 3,
      "numbers": [
        "24",
        "25",
        "5",
        "6",
        "1 "
      ],
      "hash": "6f669cfdc9e2b9c8",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p015_c00069",
      "block_type": "text",
      "page_no": 15,
      "order": 69,
      "bbox": [
        68.3,
        383.92,
        453.98,
        430.18
      ],
      "text": "画像理解 (VLM)\n視覚トークン数が増加すると速度・メモリ効率が急落するため、解像度や max-pixels 制 御が重要。 [28]",
      "normalized_text": "画像理解 (VLM)\n視覚トークン数が増加すると速度・メモリ効率が急落するため、解像度や max-pixels 制 御が重要。 [28]",
      "heading_level": 3,
      "numbers": [
        "28"
      ],
      "hash": "439eb786b743da05",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p015_c00070",
      "block_type": "text",
      "page_no": 15,
      "order": 70,
      "bbox": [
        552.31,
        108.82,
        702.11,
        123.97
      ],
      "text": "技術的詳細と設計含意",
      "normalized_text": "技術的詳細と設計含意",
      "heading_level": 3,
      "numbers": [],
      "hash": "35634fe75a4371f3",
      "meta": {
        "body_font_size": 10.08
      }
    },
    {
      "chunk_id": "document_p015_c00071",
      "block_type": "text",
      "page_no": 15,
      "order": 71,
      "bbox": [
        403.1,
        159.83,
        915.54,
        531.4
      ],
      "text": "KV キャッシュ最適化技術\nvLLM は Paged KV Cache 設計でメモリ断片化を防ぎ、 KV キャッシュを FP8 等に量子化して フットプリントを削減。 NVIDIA TensorRT-LLM も KV reuse (再利用)を最適化として強調 。長文・多同時接続時の鍵となる。\n[5][6]\n[7]\n量子化耐性と相性\n4bit 化による品質劣化はモデルと量子化方式( AWQ, GPTQ, EXL2 等)の相性に依存。実 機ベンチマークでの確認が必須。\nライセンスとコンプライアンス\n「コード (OSS) 」と「モデル重み」のライセンスは別物。特に画像生成や TTS では重み に非商用制限が含まれる場合が多く、個別精査が必要。\nKey Takeaways: 評価は「モデル × 量子化 × ランタイム × ハード」の組み合わせで決ま るため、標準化された測定手順(条件固定・ウォームアップ・ログ保存)が不可 欠です。\nPage 15 | ローカル AI 技術調査レポート",
      "normalized_text": "KV キャッシュ最適化技術\nvLLM は Paged KV Cache 設計でメモリ断片化を防ぎ、 KV キャッシュを FP8 等に量子化して フットプリントを削減。 NVIDIA TensorRT-LLM も KV reuse (再利用)を最適化として強調 。長文・多同時接続時の鍵となる。\n[5][6]\n[7]\n量子化耐性と相性\n4bit 化による品質劣化はモデルと量子化方式( AWQ, GPTQ, EXL2 等)の相性に依存。実 機ベンチマークでの確認が必須。\nライセンスとコンプライアンス\n「コード (OSS) 」と「モデル重み」のライセンスは別物。特に画像生成や TTS では重み に非商用制限が含まれる場合が多く、個別精査が必要。\nKey Takeaways: 評価は「モデル × 量子化 × ランタイム × ハード」の組み合わせで決ま るため、標準化された測定手順(条件固定・ウォームアップ・ログ保存)が不可 欠です。\nPage 15 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "8 ",
        "5",
        "6",
        "7",
        "4",
        "2 ",
        "15 "
      ],
      "hash": "75bafe221e41f07c",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p016_c00072",
      "block_type": "text",
      "page_no": 16,
      "order": 72,
      "bbox": [
        60.01,
        112.77,
        159.84,
        213.28
      ],
      "text": "06",
      "normalized_text": "06",
      "heading_level": 1,
      "numbers": [
        "06"
      ],
      "hash": "44db67cb19067177",
      "meta": {
        "body_font_size": 12.98
      }
    },
    {
      "chunk_id": "document_p016_c00073",
      "block_type": "text",
      "page_no": 16,
      "order": 73,
      "bbox": [
        60.01,
        262.39,
        349.67,
        409.99
      ],
      "text": "カテゴリ別 モデルカタログ\n主要 9 カテゴリの 詳細データと採用判断",
      "normalized_text": "カテゴリ別 モデルカタログ\n主要 9 カテゴリの 詳細データと採用判断",
      "heading_level": 3,
      "numbers": [
        "9 "
      ],
      "hash": "5de80b5f36e3c1ee",
      "meta": {
        "body_font_size": 12.98,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p016_c00074",
      "block_type": "text",
      "page_no": 16,
      "order": 74,
      "bbox": [
        492.23,
        79.29,
        876.5,
        458.51
      ],
      "text": "SECTION OVERVIEW\n提示方針:三層構造\n概要( Overview ):\nカテゴリごとの主要トレンドと代表モデルの要点を概説。\n完全版表( Full Data ):\n規模・量子化・得意不得意・ランタイム相性・リスク・参 考文献を列落ちなく全掲載。\n採用判断( Decision ):\n「どう選ぶか」の意思決定基準と失敗回避策を提示。\nテキスト LLM VLM/LMM ASR TTS Embedding\nOCR 画像生成 Agent",
      "normalized_text": "SECTION OVERVIEW\n提示方針:三層構造\n概要( Overview ):\nカテゴリごとの主要トレンドと代表モデルの要点を概説。\n完全版表( Full Data ):\n規模・量子化・得意不得意・ランタイム相性・リスク・参 考文献を列落ちなく全掲載。\n採用判断( Decision ):\n「どう選ぶか」の意思決定基準と失敗回避策を提示。\nテキスト LLM VLM/LMM ASR TTS Embedding\nOCR 画像生成 Agent",
      "heading_level": 3,
      "numbers": [],
      "hash": "614a4b88d9301119",
      "meta": {
        "body_font_size": 12.98,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p017_c00075",
      "block_type": "text",
      "page_no": 17,
      "order": 75,
      "bbox": [
        61.45,
        25.41,
        917.51,
        392.99
      ],
      "text": "テキスト LLM (汎用)概要 主要 7 モデルと実運用環境\n結論:ローカル実務のボリュームゾーンは 7B 〜 14B ( 4bit ) 。 形式は GGUF ( llama.cpp 系)か GPU 量子化( GPTQ/AWQ/EXL2 )の二択が現実解です。\n代表モデル群(要点)\nLlama 3.1 Instruct: 8B/70B/405B 系。多言語対話最適化を明記。 [29-31]\nQwen2.5 Instruct: 0.5 〜 72B 。幅広いサイズ展開と高性能。 [32-33]\nGemma 2: 2B/9B/27B 。 9B がローカル向き。責任ある利用を明示。 [35]\nMixtral 8x7B: MoE 構成。 Llama2 70B 超の性能を標榜。 [36-37]\nPhi-3 Mini: 3.8B (128k) 。軽量かつ長文対応( KV 設計必須)。 [38]\nPhi-4: 14B 。合成データ活用で高品質。ローカル上位ライン。 [39-40]\ngpt-oss: 20B/120B 。ツール呼び出しガイド充実。 [41] Weights 4bit GGUF AWQ",
      "normalized_text": "テキスト LLM (汎用)概要 主要 7 モデルと実運用環境\n結論:ローカル実務のボリュームゾーンは 7B 〜 14B ( 4bit ) 。 形式は GGUF ( llama.cpp 系)か GPU 量子化( GPTQ/AWQ/EXL2 )の二択が現実解です。\n代表モデル群(要点)\nLlama 3.1 Instruct: 8B/70B/405B 系。多言語対話最適化を明記。 [29-31]\nQwen2.5 Instruct: 0.5 〜 72B 。幅広いサイズ展開と高性能。 [32-33]\nGemma 2: 2B/9B/27B 。 9B がローカル向き。責任ある利用を明示。 [35]\nMixtral 8x7B: MoE 構成。 Llama2 70B 超の性能を標榜。 [36-37]\nPhi-3 Mini: 3.8B (128k) 。軽量かつ長文対応( KV 設計必須)。 [38]\nPhi-4: 14B 。合成データ活用で高品質。ローカル上位ライン。 [39-40]\ngpt-oss: 20B/120B 。ツール呼び出しガイド充実。 [41] Weights 4bit GGUF AWQ",
      "heading_level": 1,
      "numbers": [
        "7 ",
        "7",
        "14",
        "4",
        "2 ",
        "3.1 ",
        "8",
        "70",
        "405",
        "29",
        "31",
        "2.5 ",
        "0.5 ",
        "72",
        "32",
        "33",
        "2",
        "2",
        "9",
        "27",
        "9",
        "35",
        "8",
        "7",
        "2 ",
        "70",
        "36",
        "37",
        "3 ",
        "3.8",
        "128",
        "38",
        "4",
        "14",
        "39",
        "40",
        "20",
        "120",
        "41",
        "4"
      ],
      "hash": "cb1a46174691d231",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 8
      }
    },
    {
      "chunk_id": "document_p017_c00076",
      "block_type": "text",
      "page_no": 17,
      "order": 76,
      "bbox": [
        36.03,
        216.49,
        922.54,
        531.79
      ],
      "text": "実行環境と形式( Runtime & Format )\nクロスプラットフォーム標準。 CPU/GPU ハイブリッド実行が可能。 GGUF 形式が必須。\n[21][116]\nMLX / MLX-LM による変換・実行、または GGUF (llama.cpp) が最適。 Unified Memory を活用 。 [22][120]\nvLLM / ExLlamaV2 / AutoGPTQ 等。 GPU 量子化形式( AWQ/GPTQ/EXL2 )で高速推論。\n[87][121]\nOllama / LM Studio による OpenAI 互換 API 提供が実務的。 [23][41] ライセンス注意( License Caution )\nモデルファミリー内でもサイズやバージョンによりライセンス条件が異なる場合があります(例: Qwen2.5 の一部例外など)。採用時は必ず各モデルカードおよび公式ブログの最新条件を確認してください。\n[33]\nLocal AI Technical Survey Report 2026 17 / 80",
      "normalized_text": "実行環境と形式( Runtime & Format )\nクロスプラットフォーム標準。 CPU/GPU ハイブリッド実行が可能。 GGUF 形式が必須。\n[21][116]\nMLX / MLX-LM による変換・実行、または GGUF (llama.cpp) が最適。 Unified Memory を活用 。 [22][120]\nvLLM / ExLlamaV2 / AutoGPTQ 等。 GPU 量子化形式( AWQ/GPTQ/EXL2 )で高速推論。\n[87][121]\nOllama / LM Studio による OpenAI 互換 API 提供が実務的。 [23][41] ライセンス注意( License Caution )\nモデルファミリー内でもサイズやバージョンによりライセンス条件が異なる場合があります(例: Qwen2.5 の一部例外など)。採用時は必ず各モデルカードおよび公式ブログの最新条件を確認してください。\n[33]\nLocal AI Technical Survey Report 2026 17 / 80",
      "heading_level": 3,
      "numbers": [
        "21",
        "116",
        "22",
        "120",
        "2 ",
        "2 ",
        "87",
        "121",
        "23",
        "41",
        "2.5 ",
        "33",
        "2026 ",
        "17 ",
        "80"
      ],
      "hash": "72ae1690b8f90afb",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 9
      }
    },
    {
      "chunk_id": "document_p018_c00077",
      "block_type": "text",
      "page_no": 18,
      "order": 77,
      "bbox": [
        48.76,
        34.72,
        913.16,
        340.52
      ],
      "text": "テキスト LLM (完全版表 1 ) 主要モデル詳細比較: Llama 3.1 / Qwen2.5 / Gemma 2 / Mixtral\n代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断\nLlama 3.1 Instruct Meta [29-31]\n8B 70B 405B\n4bit 中心 (GGUF 等 ) ※方式は環境依存\n得意 多言語対話最適化を明記\n得意 大規模は品質が出るがローカルは ハード制約 [30]\nMac: GGUF / MLX 変換 Win: GGUF / 各 GPU ランタイム (形式依存)\nRisk コミュニティライセンス 系で用途制約の精査が必要 [31]\nQwen2.5 Instruct Alibaba [32-33]\n0.5B 〜 72B\n小〜中は 4bit (GGUF/ 他 ) でローカル向き 72B は上位ハード前提\n得意 幅広いサイズ展開を明記\n得意 ライセンスはサイズにより差があ る旨を公式ブログで明記 [33]\nMac: GGUF / MLX Win: GGUF / ExLlama / vLLM 等( 形式依存)\nRisk 「 3B/72B は Apache 2.0 例外」といった条件差があ\n例外\nるためリポジトリ個別確認 必須 [33]",
      "normalized_text": "テキスト LLM (完全版表 1 ) 主要モデル詳細比較: Llama 3.1 / Qwen2.5 / Gemma 2 / Mixtral\n代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断\nLlama 3.1 Instruct Meta [29-31]\n8B 70B 405B\n4bit 中心 (GGUF 等 ) ※方式は環境依存\n得意 多言語対話最適化を明記\n得意 大規模は品質が出るがローカルは ハード制約 [30]\nMac: GGUF / MLX 変換 Win: GGUF / 各 GPU ランタイム (形式依存)\nRisk コミュニティライセンス 系で用途制約の精査が必要 [31]\nQwen2.5 Instruct Alibaba [32-33]\n0.5B 〜 72B\n小〜中は 4bit (GGUF/ 他 ) でローカル向き 72B は上位ハード前提\n得意 幅広いサイズ展開を明記\n得意 ライセンスはサイズにより差があ る旨を公式ブログで明記 [33]\nMac: GGUF / MLX Win: GGUF / ExLlama / vLLM 等( 形式依存)\nRisk 「 3B/72B は Apache 2.0 例外」といった条件差があ\n例外\nるためリポジトリ個別確認 必須 [33]",
      "heading_level": 3,
      "numbers": [
        "1 ",
        "3.1 ",
        "2.5 ",
        "2 ",
        "3.1 ",
        "29",
        "31",
        "8",
        "70",
        "405",
        "4",
        "30",
        "31",
        "2.5 ",
        "32",
        "33",
        "0.5",
        "72",
        "4",
        "72",
        "33",
        "3",
        "72",
        "2.0 ",
        "33"
      ],
      "hash": "144dd258ea53810e",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 15
      }
    },
    {
      "chunk_id": "document_p018_c00078",
      "block_type": "text",
      "page_no": 18,
      "order": 78,
      "bbox": [
        48.76,
        382.75,
        911.64,
        417.04
      ],
      "text": "Gemma 2 Google [35]\n2B 9B 27B\n2B/9B はローカル向き 27B は VRAM/ メモリ要求 増\n得意 モデルカード更新日が明示されて おり、責任ある利用を前提に整理さ れている [35]\nMac: MLX / GGUF 変換 Win: GGUF / 各 GPU\nRisk “ 事実質問用途での誤生成 成 ” は一般に起こり得る(モ モデル一般リスク)",
      "normalized_text": "Gemma 2 Google [35]\n2B 9B 27B\n2B/9B はローカル向き 27B は VRAM/ メモリ要求 増\n得意 モデルカード更新日が明示されて おり、責任ある利用を前提に整理さ れている [35]\nMac: MLX / GGUF 変換 Win: GGUF / 各 GPU\nRisk “ 事実質問用途での誤生成 成 ” は一般に起こり得る(モ モデル一般リスク)",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "35",
        "2",
        "9",
        "27",
        "2",
        "9",
        "27",
        "35"
      ],
      "hash": "373ae018efd274a1",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p018_c00079",
      "block_type": "text",
      "page_no": 18,
      "order": 79,
      "bbox": [
        36.03,
        458.4,
        916.47,
        531.44
      ],
      "text": "Mixtral 8x7B Mistral [36-37]\nMoE (8×7B)\nGPU 向けで真価が出やす い (量子化 / 実装依存)\n得意 「 Llama2 70B を多くのベンチで上 上回る」等をモデル説明で明記 [37]\nWin GPU: 適合ランタイム( vLLM 等)前提 Mac: 難易度高め\nRisk MoE は実装・ VRAM ・スル ループットのブレが大きい\n採用判断の要点:ローカル実務のボリュームゾーンは 7B 〜 14B 級( 4bit )です。 3B 級はハード制約時、 32B 以上は高品質要件時(メモリ増設前提)に選択します。ライセンスはファミリー内でも異なる場合がある ため、必ず最新のモデルカードを確認してください。\nPage 18 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [29-37]",
      "normalized_text": "Mixtral 8x7B Mistral [36-37]\nMoE (8×7B)\nGPU 向けで真価が出やす い (量子化 / 実装依存)\n得意 「 Llama2 70B を多くのベンチで上 上回る」等をモデル説明で明記 [37]\nWin GPU: 適合ランタイム( vLLM 等)前提 Mac: 難易度高め\nRisk MoE は実装・ VRAM ・スル ループットのブレが大きい\n採用判断の要点:ローカル実務のボリュームゾーンは 7B 〜 14B 級( 4bit )です。 3B 級はハード制約時、 32B 以上は高品質要件時(メモリ増設前提)に選択します。ライセンスはファミリー内でも異なる場合がある ため、必ず最新のモデルカードを確認してください。\nPage 18 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [29-37]",
      "heading_level": 3,
      "numbers": [
        "8",
        "7",
        "36",
        "37",
        "8",
        "7",
        "2 ",
        "70",
        "37",
        "7",
        "14",
        "4",
        "3",
        "32",
        "18 ",
        "2026 ",
        "29",
        "37"
      ],
      "hash": "9d1b3b8fd8388e9d",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p019_c00080",
      "block_type": "text",
      "page_no": 19,
      "order": 80,
      "bbox": [
        48.76,
        34.72,
        906.05,
        153.62
      ],
      "text": "テキスト LLM (完全版表 2 ) 主要モデル詳細比較: Phi-3 Mini / Phi-4 / gpt-oss\n代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断",
      "normalized_text": "テキスト LLM (完全版表 2 ) 主要モデル詳細比較: Phi-3 Mini / Phi-4 / gpt-oss\n代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "3 ",
        "4 "
      ],
      "hash": "9b393b61794a4307",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p019_c00081",
      "block_type": "text",
      "page_no": 19,
      "order": 81,
      "bbox": [
        48.76,
        195.97,
        911.52,
        229.16
      ],
      "text": "Phi-3 Mini Microsoft [38]\n3.8B 4bit でローカル適性高い 長文版 (128K) 運用は KV が 支配\n得意 「 128K 文脈」をうたうが長文はメ メモリ設計前提 [38]\nMac/Win: 比較的回しやすい( 形式依存)\nRisk “ 小さい=万能 ” ではなく く、専門領域は RAG 前提で補 補う判断",
      "normalized_text": "Phi-3 Mini Microsoft [38]\n3.8B 4bit でローカル適性高い 長文版 (128K) 運用は KV が 支配\n得意 「 128K 文脈」をうたうが長文はメ メモリ設計前提 [38]\nMac/Win: 比較的回しやすい( 形式依存)\nRisk “ 小さい=万能 ” ではなく く、専門領域は RAG 前提で補 補う判断",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "38",
        "3.8",
        "4",
        "128",
        "128",
        "38"
      ],
      "hash": "3c9edc8d554ef740",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p019_c00082",
      "block_type": "text",
      "page_no": 19,
      "order": 82,
      "bbox": [
        48.76,
        273.38,
        911.91,
        307.64
      ],
      "text": "Phi-4 Microsoft [39-40]\n14B 4bit でローカル上位ライ ン ( 32GB 級以上推奨)\n得意 データ品質重視・合成データ活用 を明示 [40]\nWin GPU: 最適化次第 Mac: MLX / 変換次第\nRisk 14B は “ 重いが現実的 ” の境界。\nの境 KV/ 長文設計が重要",
      "normalized_text": "Phi-4 Microsoft [39-40]\n14B 4bit でローカル上位ライ ン ( 32GB 級以上推奨)\n得意 データ品質重視・合成データ活用 を明示 [40]\nWin GPU: 最適化次第 Mac: MLX / 変換次第\nRisk 14B は “ 重いが現実的 ” の境界。\nの境 KV/ 長文設計が重要",
      "heading_level": 3,
      "numbers": [
        "4 ",
        "39",
        "40",
        "14",
        "4",
        "32",
        "40",
        "14"
      ],
      "hash": "1384babbc5b1c7a2",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p019_c00083",
      "block_type": "text",
      "page_no": 19,
      "order": 83,
      "bbox": [
        36.03,
        350.72,
        917.83,
        531.44
      ],
      "text": "gpt-oss OpenAI [41]\n20B 120B\nMXFP4 量子化で出荷を明 明記 (他量子化なし)\n得意 ツール呼び出し・ローカル API 利 用までガイドあり [27]\nMac/Win: LM Studio / Ollama で 手順が提示される [41]\nRisk 20B は 16GB 以上、 120B は\nは 60GB 以上推奨を明記 [41]\n採用判断の要点: 14B クラス( Phi-4 等)はローカル運用の「上位実用ライン」であり、 32GB 以上のメモリ環境が推奨されます。長文コンテキスト( 128K 等)を活用する場合は、モデルサイズだけでなく KV キャッ シュのメモリ消費が支配的になるため、メモリ設計( KV 量子化やコンテキスト長制限)が不可欠です。\nPage 19 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [38-41]",
      "normalized_text": "gpt-oss OpenAI [41]\n20B 120B\nMXFP4 量子化で出荷を明 明記 (他量子化なし)\n得意 ツール呼び出し・ローカル API 利 用までガイドあり [27]\nMac/Win: LM Studio / Ollama で 手順が提示される [41]\nRisk 20B は 16GB 以上、 120B は\nは 60GB 以上推奨を明記 [41]\n採用判断の要点: 14B クラス( Phi-4 等)はローカル運用の「上位実用ライン」であり、 32GB 以上のメモリ環境が推奨されます。長文コンテキスト( 128K 等)を活用する場合は、モデルサイズだけでなく KV キャッ シュのメモリ消費が支配的になるため、メモリ設計( KV 量子化やコンテキスト長制限)が不可欠です。\nPage 19 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [38-41]",
      "heading_level": 3,
      "numbers": [
        "41",
        "20",
        "120",
        "4 ",
        "27",
        "41",
        "20",
        "16",
        "120",
        "60",
        "41",
        "14",
        "4 ",
        "32",
        "128",
        "19 ",
        "2026 ",
        "38",
        "41"
      ],
      "hash": "d81eeaad7aaed3dc",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 8
      }
    },
    {
      "chunk_id": "document_p020_c00084",
      "block_type": "text",
      "page_no": 20,
      "order": 84,
      "bbox": [
        58.57,
        19.33,
        925.01,
        531.4
      ],
      "text": "採用判断基準(テキスト LLM ) 選定フローと実務上の重要ポイント\n結論:モデル選定は「必要品質 → 許容レイテンシ → 許容メモリ → 形式 → ランタイム」の順に行うのが、実務上最も破綻しにくい フローです。\n実務の現実ライン\n7B 〜 14B がボリュームゾーン\n一般業務や RAG において、品質と速度のバランスが良いスイー トスポット。\n3B 級はハード制約モード\nメモリ不足時のフォールバックや、抽出特化タスクで使い分 ける。\n形式とライセンス\n形式先行は詰まりやすい\nランタイム(例 : LM Studio 使いたい)から入ると形式制約で選 択肢が狭まるため逆順推奨。\nライセンスの個別確認\nQwen2.5 等、同ファミリー内でもサイズにより条件が異なる例 外あり。\n[33]\nメモリ・最適化\n長文は KV キャッシュが支配\nコンテキスト長が増えると KV メモリが急増する。\n設計上の必須要件\nコンテキスト設計に加え、 KV 量子化 (FP8/INT4) や KV 再利用 (Reuse) の併用を前提に見積もる。\nPage 20 | ローカル AI 技術調査レポート",
      "normalized_text": "採用判断基準(テキスト LLM ) 選定フローと実務上の重要ポイント\n結論:モデル選定は「必要品質 → 許容レイテンシ → 許容メモリ → 形式 → ランタイム」の順に行うのが、実務上最も破綻しにくい フローです。\n実務の現実ライン\n7B 〜 14B がボリュームゾーン\n一般業務や RAG において、品質と速度のバランスが良いスイー トスポット。\n3B 級はハード制約モード\nメモリ不足時のフォールバックや、抽出特化タスクで使い分 ける。\n形式とライセンス\n形式先行は詰まりやすい\nランタイム(例 : LM Studio 使いたい)から入ると形式制約で選 択肢が狭まるため逆順推奨。\nライセンスの個別確認\nQwen2.5 等、同ファミリー内でもサイズにより条件が異なる例 外あり。\n[33]\nメモリ・最適化\n長文は KV キャッシュが支配\nコンテキスト長が増えると KV メモリが急増する。\n設計上の必須要件\nコンテキスト設計に加え、 KV 量子化 (FP8/INT4) や KV 再利用 (Reuse) の併用を前提に見積もる。\nPage 20 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "7",
        "14",
        "3",
        "2.5 ",
        "33",
        "8",
        "4",
        "20 "
      ],
      "hash": "0d0198cd7c30fb34",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p020_c00085",
      "block_type": "text",
      "page_no": 20,
      "order": 85,
      "bbox": [
        84.17,
        193.07,
        172.03,
        273.57
      ],
      "text": "1\n必要品質\nタスク難易度でサイズ決\n定 ( 例 : 70B vs 8B)",
      "normalized_text": "1\n必要品質\nタスク難易度でサイズ決\n定 ( 例 : 70B vs 8B)",
      "heading_level": 1,
      "numbers": [
        "1\n",
        "70",
        "8"
      ],
      "hash": "9334e17060742031",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p020_c00086",
      "block_type": "text",
      "page_no": 20,
      "order": 86,
      "bbox": [
        269.3,
        198.48,
        339.19,
        268.66
      ],
      "text": "2\n許容レイテンシ\nリアルタイム性か\nバッチ処理か",
      "normalized_text": "2\n許容レイテンシ\nリアルタイム性か\nバッチ処理か",
      "heading_level": 1,
      "numbers": [
        "2\n"
      ],
      "hash": "17cdf06c2a608c1d",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p020_c00087",
      "block_type": "text",
      "page_no": 20,
      "order": 87,
      "bbox": [
        440.66,
        198.48,
        519.67,
        269.52
      ],
      "text": "3\n許容メモリ\n4bit 量子化前提で VRAM/RAM に収まるか",
      "normalized_text": "3\n許容メモリ\n4bit 量子化前提で VRAM/RAM に収まるか",
      "heading_level": 1,
      "numbers": [
        "3\n",
        "4"
      ],
      "hash": "da8fa1705e825683",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p020_c00088",
      "block_type": "text",
      "page_no": 20,
      "order": 88,
      "bbox": [
        622.48,
        198.48,
        690.37,
        268.66
      ],
      "text": "4\n形式選定\nGGUF or GPTQ/AWQ\n要件先行で決定",
      "normalized_text": "4\n形式選定\nGGUF or GPTQ/AWQ\n要件先行で決定",
      "heading_level": 1,
      "numbers": [
        "4\n"
      ],
      "hash": "8e1b1aec3f1f2fdd",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p020_c00089",
      "block_type": "text",
      "page_no": 20,
      "order": 89,
      "bbox": [
        792.32,
        198.48,
        872.26,
        269.52
      ],
      "text": "5\nランタイム\n形式に合うものを選択\n(llama.cpp / vLLM 等 )",
      "normalized_text": "5\nランタイム\n形式に合うものを選択\n(llama.cpp / vLLM 等 )",
      "heading_level": 1,
      "numbers": [
        "5\n"
      ],
      "hash": "8ab546008663f1e1",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p021_c00090",
      "block_type": "text",
      "page_no": 21,
      "order": 90,
      "bbox": [
        62.19,
        24.28,
        919.05,
        227.98
      ],
      "text": "VLM/LMM 概要(画像理解・マルチモーダル) カテゴリ別モデルカタログ\n結論:ローカル運用の現実ラインは 2B/7B 級 が中心。 画像トークン化の前処理依存が強く、ランタイムの明示サポートが重要です。\n代表モデルと特徴",
      "normalized_text": "VLM/LMM 概要(画像理解・マルチモーダル) カテゴリ別モデルカタログ\n結論:ローカル運用の現実ラインは 2B/7B 級 が中心。 画像トークン化の前処理依存が強く、ランタイムの明示サポートが重要です。\n代表モデルと特徴",
      "heading_level": 3,
      "numbers": [
        "2",
        "7"
      ],
      "hash": "c0d65b02bc05c487",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p021_c00091",
      "block_type": "text",
      "page_no": 21,
      "order": 91,
      "bbox": [
        72.77,
        249.5,
        431.21,
        278.29
      ],
      "text": "Qwen2-VL (2B/7B/72B) [42]\nローカル現実ラインの主力。 2B/7B が実用的。 72B は上位ハード( 96GB+ )前提。",
      "normalized_text": "Qwen2-VL (2B/7B/72B) [42]\nローカル現実ラインの主力。 2B/7B が実用的。 72B は上位ハード( 96GB+ )前提。",
      "heading_level": 3,
      "numbers": [
        "2",
        "2",
        "7",
        "72",
        "42",
        "2",
        "7",
        "72",
        "96"
      ],
      "hash": "f535a2d290ab99f7",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p021_c00092",
      "block_type": "text",
      "page_no": 21,
      "order": 92,
      "bbox": [
        72.77,
        215.0,
        686.27,
        453.67
      ],
      "text": "Phi-3-Vision (128K) [43]\n軽量マルチモーダル+長文対応を標榜。テキスト重視の画像理解に適正。\nInternVL2 (4B 等 ) [44-45]\n画像理解の系列として設計。モデル群が複数サイズ展開。\nLLaVA (7B/13B 系 ) [46-47]\n画像チャット用途の先駆者。 4bit 運用で 12GB VRAM でも動作可能。\nランタイム相性と技術課題",
      "normalized_text": "Phi-3-Vision (128K) [43]\n軽量マルチモーダル+長文対応を標榜。テキスト重視の画像理解に適正。\nInternVL2 (4B 等 ) [44-45]\n画像理解の系列として設計。モデル群が複数サイズ展開。\nLLaVA (7B/13B 系 ) [46-47]\n画像チャット用途の先駆者。 4bit 運用で 12GB VRAM でも動作可能。\nランタイム相性と技術課題",
      "heading_level": 3,
      "numbers": [
        "3",
        "128",
        "43",
        "2 ",
        "4",
        "44",
        "45",
        "7",
        "13",
        "46",
        "47",
        "4",
        "12"
      ],
      "hash": "e33dfc359cb60b58",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p021_c00093",
      "block_type": "text",
      "page_no": 21,
      "order": 93,
      "bbox": [
        526.02,
        249.5,
        834.44,
        290.54
      ],
      "text": "macOS / Apple Silicon\nMLX-VLM: Qwen2-VL 等の利用例を具体コマンドで提示。\nUnified Memory を活用し、システムメモリで大型モデルを実行可能。\n[28]",
      "normalized_text": "macOS / Apple Silicon\nMLX-VLM: Qwen2-VL 等の利用例を具体コマンドで提示。\nUnified Memory を活用し、システムメモリで大型モデルを実行可能。\n[28]",
      "heading_level": 3,
      "numbers": [
        "2",
        "28"
      ],
      "hash": "4d3e784da74d7ef9",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p021_c00094",
      "block_type": "text",
      "page_no": 21,
      "order": 94,
      "bbox": [
        68.3,
        317.83,
        913.1,
        537.31
      ],
      "text": "Windows / GPU\ntransformers / vLLM: GPU 推論が主流。モデル形式( Safetensors/AWQ 等)とランタイム の対応に依存。\n技術的ボトルネック\n画像トークンが増えるとメモリ / 速度が急落。解像度制御や max-pixels 設定が運用の肝と なる。\nVisual Tokens Multi-modal RAG Resolution Control\n設計含意( Design Implication )",
      "normalized_text": "Windows / GPU\ntransformers / vLLM: GPU 推論が主流。モデル形式( Safetensors/AWQ 等)とランタイム の対応に依存。\n技術的ボトルネック\n画像トークンが増えるとメモリ / 速度が急落。解像度制御や max-pixels 設定が運用の肝と なる。\nVisual Tokens Multi-modal RAG Resolution Control\n設計含意( Design Implication )",
      "heading_level": 3,
      "numbers": [],
      "hash": "f63bf260739ee0ee",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p022_c00095",
      "block_type": "text",
      "page_no": 22,
      "order": 95,
      "bbox": [
        48.76,
        34.72,
        893.79,
        155.81
      ],
      "text": "VLM/LMM (完全版表) Qwen2-VL / Phi-3-Vision / InternVL2 / LLaVA 詳細比較\n代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断",
      "normalized_text": "VLM/LMM (完全版表) Qwen2-VL / Phi-3-Vision / InternVL2 / LLaVA 詳細比較\n代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断",
      "heading_level": 3,
      "numbers": [
        "2",
        "3",
        "2 "
      ],
      "hash": "b42ba872912bd553",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p022_c00096",
      "block_type": "text",
      "page_no": 22,
      "order": 96,
      "bbox": [
        48.76,
        200.36,
        907.24,
        234.62
      ],
      "text": "Qwen2-VL Qwen [42]\n2B 7B 72B\n2B/7B がローカル現実ライン 72B は上位ハード前提\n得意 画像+テキストの汎用 画像トークン増でメモリ / 速度急 落 → 解像度制御が肝\nMac: MLX-VLM が利用例を明 明示 [28] Win: transformers / vLLM\nRisk 視覚トークン肥大による リソース枯渇",
      "normalized_text": "Qwen2-VL Qwen [42]\n2B 7B 72B\n2B/7B がローカル現実ライン 72B は上位ハード前提\n得意 画像+テキストの汎用 画像トークン増でメモリ / 速度急 落 → 解像度制御が肝\nMac: MLX-VLM が利用例を明 明示 [28] Win: transformers / vLLM\nRisk 視覚トークン肥大による リソース枯渇",
      "heading_level": 3,
      "numbers": [
        "2",
        "42",
        "2",
        "7",
        "72",
        "2",
        "7",
        "72",
        "28"
      ],
      "hash": "244a87abc46724fe",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p022_c00097",
      "block_type": "text",
      "page_no": 22,
      "order": 97,
      "bbox": [
        48.76,
        282.1,
        908.25,
        316.35
      ],
      "text": "Phi-3-Vision Microsoft [43]\n4.2B (128K)\n形式依存でローカル適性あり 128K 長文対応\n得意 軽量マルチモーダル+長文を 標榜 “ 実務的な OCR 読取 ” に強み\nMac: MLX 変換次第 Win: GPU 推論\nRisk 長文は KV が支配しやすく 、メモリ管理が必須 [43]",
      "normalized_text": "Phi-3-Vision Microsoft [43]\n4.2B (128K)\n形式依存でローカル適性あり 128K 長文対応\n得意 軽量マルチモーダル+長文を 標榜 “ 実務的な OCR 読取 ” に強み\nMac: MLX 変換次第 Win: GPU 推論\nRisk 長文は KV が支配しやすく 、メモリ管理が必須 [43]",
      "heading_level": 3,
      "numbers": [
        "3",
        "43",
        "4.2",
        "128",
        "128",
        "43"
      ],
      "hash": "50d28a76817d8c75",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p022_c00098",
      "block_type": "text",
      "page_no": 22,
      "order": 98,
      "bbox": [
        48.76,
        363.83,
        907.24,
        398.08
      ],
      "text": "InternVL2 OpenGVLab [44-45]\n1B/2B 4B/8B 等\n4B 等がローカル向き モデル群が複数サイズ展開\n得意 画像理解の系列として設計 構成要素をモデルカードで詳細説 明 [45]\nWin: GPU / 形式次第 Mac: 実装依存\nRisk エコシステム差(変換・ プロセッサ依存)が運用の 難所",
      "normalized_text": "InternVL2 OpenGVLab [44-45]\n1B/2B 4B/8B 等\n4B 等がローカル向き モデル群が複数サイズ展開\n得意 画像理解の系列として設計 構成要素をモデルカードで詳細説 明 [45]\nWin: GPU / 形式次第 Mac: 実装依存\nRisk エコシステム差(変換・ プロセッサ依存)が運用の 難所",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "44",
        "45",
        "1",
        "2",
        "4",
        "8",
        "4",
        "45"
      ],
      "hash": "c207ab3587fbd346",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p022_c00099",
      "block_type": "text",
      "page_no": 22,
      "order": 99,
      "bbox": [
        36.03,
        445.56,
        916.93,
        531.44
      ],
      "text": "LLaVA Community [46-47]\n7B 13B\n4bit での運用示唆 13B で 12GB VRAM 動作可 [47]\n得意 画像チャット用途 Win GPU で構築しやすい\nWin: GPU 推論で実績多 Mac: llama.cpp 等\nRisk ベース LLM や実装差が大 きく、モデルカードとの整 合確認が必要 採用判断の要点:画像理解はテキスト LLM より前処理・プロセッサ依存が強いため、「ランタイムがそのモデルを明示サポートしているか」を一次確認してください(例: MLX-VLM が Qwen2-VL のコマンド提示) 。 72B 級はローカルでは上位機( 96GB+ 等)が必要で、現実は 2B/7B 級で設計します。\nPage 22 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [28, 42-47]",
      "normalized_text": "LLaVA Community [46-47]\n7B 13B\n4bit での運用示唆 13B で 12GB VRAM 動作可 [47]\n得意 画像チャット用途 Win GPU で構築しやすい\nWin: GPU 推論で実績多 Mac: llama.cpp 等\nRisk ベース LLM や実装差が大 きく、モデルカードとの整 合確認が必要 採用判断の要点:画像理解はテキスト LLM より前処理・プロセッサ依存が強いため、「ランタイムがそのモデルを明示サポートしているか」を一次確認してください(例: MLX-VLM が Qwen2-VL のコマンド提示) 。 72B 級はローカルでは上位機( 96GB+ 等)が必要で、現実は 2B/7B 級で設計します。\nPage 22 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [28, 42-47]",
      "heading_level": 3,
      "numbers": [
        "46",
        "47",
        "7",
        "13",
        "4",
        "13",
        "12",
        "47",
        "2",
        "72",
        "96",
        "2",
        "7",
        "22 ",
        "2026 ",
        "28, ",
        "42",
        "47"
      ],
      "hash": "e4e15abeda06e288",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p023_c00100",
      "block_type": "text",
      "page_no": 23,
      "order": 100,
      "bbox": [
        58.57,
        19.33,
        924.92,
        491.99
      ],
      "text": "採用判断基準( VLM/LMM ) ランタイム適合性と運用設計\n結論:画像理解は前処理・プロセッサ依存が強いため、「ランタイムの明示サポート確認」が最優先です。 2B/7B 級を基本とし 、不足分を RAG/OCR で補うのが安全策です。\nランタイム適合性\n一次情報の確認\nランタイムがモデルを明示サポートしているか確認が安全( 例 : MLX-VLM の Qwen2-VL 対応コマンド提示)。\nプロセッサ依存性\nテキスト LLM より前処理(画像トークン化)の依存が強く、変 換ミスが起きやすい。",
      "normalized_text": "採用判断基準( VLM/LMM ) ランタイム適合性と運用設計\n結論:画像理解は前処理・プロセッサ依存が強いため、「ランタイムの明示サポート確認」が最優先です。 2B/7B 級を基本とし 、不足分を RAG/OCR で補うのが安全策です。\nランタイム適合性\n一次情報の確認\nランタイムがモデルを明示サポートしているか確認が安全( 例 : MLX-VLM の Qwen2-VL 対応コマンド提示)。\nプロセッサ依存性\nテキスト LLM より前処理(画像トークン化)の依存が強く、変 換ミスが起きやすい。",
      "heading_level": 3,
      "numbers": [
        "2",
        "7",
        "2"
      ],
      "hash": "37878fb42f45b6ac",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p023_c00101",
      "block_type": "text",
      "page_no": 23,
      "order": 101,
      "bbox": [
        360.4,
        345.67,
        613.16,
        491.99
      ],
      "text": "運用設計( 2B/7B 級)\n現実的なライン\n72B 級は 96GB+ メモリや複数 GPU が必要なため、実務では 2B/7B 級での運用設計に寄せる。\n視覚トークン制御\n長文・多画像時は KV 支配とトークン肥大に注意。 max-pixels/ マ ルチ画像数を制御する。",
      "normalized_text": "運用設計( 2B/7B 級)\n現実的なライン\n72B 級は 96GB+ メモリや複数 GPU が必要なため、実務では 2B/7B 級での運用設計に寄せる。\n視覚トークン制御\n長文・多画像時は KV 支配とトークン肥大に注意。 max-pixels/ マ ルチ画像数を制御する。",
      "heading_level": 3,
      "numbers": [
        "2",
        "7",
        "72",
        "96",
        "2",
        "7"
      ],
      "hash": "2f19d7dca38b7b1d",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p023_c00102",
      "block_type": "text",
      "page_no": 23,
      "order": 102,
      "bbox": [
        107.98,
        232.67,
        910.35,
        531.4
      ],
      "text": "補完戦略( OCR/RAG )\n不得意のカバー\n微細な文字認識や構造化は OCR ( PaddleOCR 等)に任せ、 VLM は「意味理解」に集中させる。\nハイブリッド構成\n画像から情報をテキスト抽出し、 Embedding 化して RAG で検索 可能にする設計が堅牢。\nPage 23 | ローカル AI 技術調査レポート\nサポート確認\nREADME 等でランタイム (MLX/vLLM) の対応を確認\n[28]\n規模選定\n2B/7B 級で設計開始 (72B 級は上位 HW 要件 )\nトークン制御\n解像度・ max-pixels 制御\nでメモリ爆発を防ぐ\nハイブリッド\nOCR や RAG を併用して\n精度と速度を補完",
      "normalized_text": "補完戦略( OCR/RAG )\n不得意のカバー\n微細な文字認識や構造化は OCR ( PaddleOCR 等)に任せ、 VLM は「意味理解」に集中させる。\nハイブリッド構成\n画像から情報をテキスト抽出し、 Embedding 化して RAG で検索 可能にする設計が堅牢。\nPage 23 | ローカル AI 技術調査レポート\nサポート確認\nREADME 等でランタイム (MLX/vLLM) の対応を確認\n[28]\n規模選定\n2B/7B 級で設計開始 (72B 級は上位 HW 要件 )\nトークン制御\n解像度・ max-pixels 制御\nでメモリ爆発を防ぐ\nハイブリッド\nOCR や RAG を併用して\n精度と速度を補完",
      "heading_level": 3,
      "numbers": [
        "23 ",
        "28",
        "2",
        "7",
        "72"
      ],
      "hash": "d52c5557687a0bd8",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 12
      }
    },
    {
      "chunk_id": "document_p024_c00103",
      "block_type": "text",
      "page_no": 24,
      "order": 103,
      "bbox": [
        58.57,
        23.56,
        919.05,
        156.49
      ],
      "text": "ASR (音声認識)概要 カテゴリ別モデルカタログ\n結論:ローカル ASR は「 RTF ・メモリ・誤転記」の同時最適化が必須。 faster-whisper を基準実装とし、速度と精度(幻覚抑制)のバランスを図ります。",
      "normalized_text": "ASR (音声認識)概要 カテゴリ別モデルカタログ\n結論:ローカル ASR は「 RTF ・メモリ・誤転記」の同時最適化が必須。 faster-whisper を基準実装とし、速度と精度(幻覚抑制)のバランスを図ります。",
      "heading_level": 1,
      "numbers": [],
      "hash": "3e4f28f5bed88a09",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p024_c00104",
      "block_type": "text",
      "page_no": 24,
      "order": 104,
      "bbox": [
        54.74,
        220.99,
        910.03,
        526.71
      ],
      "text": "Whisper 系実装の比較\n実装名 特徴・最適化 推奨用途\nWhisper (Original) [48] 研究 / 公開ベース、 PyTorch 依存 精度検証・基準\nfaster-whisper [50] CTranslate2 、 8bit 量子化、最大 4 倍速 実務の第一選択\nwhisper.cpp [51] C/C+ + 軽量、 g gml 、 Apple Silicon 最 組込み・省メモ リ\n主な技術要素 :\nCTranslate2 VAD Integration INT8 Quantization Beam Search\n最適化戦略とリスク管理\nfaster-whisper は同等精度でメモリ消費を大幅削減。 RTF < 1 (実時間より速い) を目標にベンチマークを実施します。\n[50]\n高リスク領域では誤転記が実害になる可能性が指摘されています。 VAD (無音 除去)や信頼度スコアによるフィルタリングが不可欠です。\n[49]\n文字起こし精度だけでなく、 VAD ・話者分離( Diarization )の品質が全体の UX を決定します。\n設計のポイント( Key Takeaways )\n会議議事録などの長尺音声では、単に ASR モデルを回すだけでなく、前処理( VAD による無音カット)と後処理( LLM による整文)を組み合わせたパイプライン設計が品質を左右します。ま ずは faster-whisper でベースラインを構築することを推奨します。",
      "normalized_text": "Whisper 系実装の比較\n実装名 特徴・最適化 推奨用途\nWhisper (Original) [48] 研究 / 公開ベース、 PyTorch 依存 精度検証・基準\nfaster-whisper [50] CTranslate2 、 8bit 量子化、最大 4 倍速 実務の第一選択\nwhisper.cpp [51] C/C+ + 軽量、 g gml 、 Apple Silicon 最 組込み・省メモ リ\n主な技術要素 :\nCTranslate2 VAD Integration INT8 Quantization Beam Search\n最適化戦略とリスク管理\nfaster-whisper は同等精度でメモリ消費を大幅削減。 RTF < 1 (実時間より速い) を目標にベンチマークを実施します。\n[50]\n高リスク領域では誤転記が実害になる可能性が指摘されています。 VAD (無音 除去)や信頼度スコアによるフィルタリングが不可欠です。\n[49]\n文字起こし精度だけでなく、 VAD ・話者分離( Diarization )の品質が全体の UX を決定します。\n設計のポイント( Key Takeaways )\n会議議事録などの長尺音声では、単に ASR モデルを回すだけでなく、前処理( VAD による無音カット)と後処理( LLM による整文)を組み合わせたパイプライン設計が品質を左右します。ま ずは faster-whisper でベースラインを構築することを推奨します。",
      "heading_level": 3,
      "numbers": [
        "48",
        "50",
        "2 ",
        "8",
        "4 ",
        "51",
        "2 ",
        "8 ",
        "1 ",
        "50",
        "49"
      ],
      "hash": "869cda1da3b2c05f",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 12
      }
    },
    {
      "chunk_id": "document_p025_c00105",
      "block_type": "text",
      "page_no": 25,
      "order": 105,
      "bbox": [
        48.76,
        34.72,
        895.13,
        155.49
      ],
      "text": "ASR (完全版表) 主要モデル・方式比較: Whisper / faster-whisper / whisper.cpp\n代表モデル / 方式 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断",
      "normalized_text": "ASR (完全版表) 主要モデル・方式比較: Whisper / faster-whisper / whisper.cpp\n代表モデル / 方式 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断",
      "heading_level": 3,
      "numbers": [],
      "hash": "7ed63fd1afd02a7f",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p025_c00106",
      "block_type": "text",
      "page_no": 25,
      "order": 106,
      "bbox": [
        48.76,
        199.72,
        911.67,
        315.13
      ],
      "text": "Whisper OpenAI [48-49]\nTiny 〜 Large-v3\n実装多数 (下記派生に分岐)\n得意 68 万時間規模の多言語データで頑 健性を示し、翻訳も可能 [48]\n汎用 : 直接実装 / 各派生 Risk 高リスク領域では “ 誤転記 記(幻覚) ” が実害になる可 可能性が報道されており、 検証プロセスが必須 [49]\nfaster-whisper CTranslate2 [50]\nWhisper 互換\nCPU/GPU で 8bit 量子化を明記\n得意 同等精度で最大 4 倍速・低メモリ を主張 [50]\nWin/Mac/Linux: Python で実装 しやすい\nRec 実時間( RTF )要件がある る業務はまずこれでベンチ し、足りなければ GPU 化",
      "normalized_text": "Whisper OpenAI [48-49]\nTiny 〜 Large-v3\n実装多数 (下記派生に分岐)\n得意 68 万時間規模の多言語データで頑 健性を示し、翻訳も可能 [48]\n汎用 : 直接実装 / 各派生 Risk 高リスク領域では “ 誤転記 記(幻覚) ” が実害になる可 可能性が報道されており、 検証プロセスが必須 [49]\nfaster-whisper CTranslate2 [50]\nWhisper 互換\nCPU/GPU で 8bit 量子化を明記\n得意 同等精度で最大 4 倍速・低メモリ を主張 [50]\nWin/Mac/Linux: Python で実装 しやすい\nRec 実時間( RTF )要件がある る業務はまずこれでベンチ し、足りなければ GPU 化",
      "heading_level": 3,
      "numbers": [
        "48",
        "49",
        "3\n",
        "68 ",
        "48",
        "49",
        "2 ",
        "50",
        "8",
        "4 ",
        "50"
      ],
      "hash": "ad3d717524017380",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 10
      }
    },
    {
      "chunk_id": "document_p025_c00107",
      "block_type": "text",
      "page_no": 25,
      "order": 107,
      "bbox": [
        36.03,
        362.02,
        914.15,
        531.44
      ],
      "text": "whisper.cpp ggml [51]\nWhisper 互換\nggml 系で ローカル最適化\n得意 C/C++ で軽量運用、各種最適化例 例が豊富\nMac/Win: 共に導入可能 Info バッチ向きに設計し、ス トリーミング要件は別設計 が必要になるケースがある (一般論) [51]\n採用判断の要点:ローカル ASR は「 RTF (実時間比)」「メモリ」「誤転記リスク」の 3 要素を同時に満たす必要があります。 faster-whisper は速度・メモリ面での利点を明示しているため、まずはこれを基準実装 として検証し、要件に応じて GPU 化やモデルサイズの調整を行うのが合理的です。\nPage 25 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [48-51]",
      "normalized_text": "whisper.cpp ggml [51]\nWhisper 互換\nggml 系で ローカル最適化\n得意 C/C++ で軽量運用、各種最適化例 例が豊富\nMac/Win: 共に導入可能 Info バッチ向きに設計し、ス トリーミング要件は別設計 が必要になるケースがある (一般論) [51]\n採用判断の要点:ローカル ASR は「 RTF (実時間比)」「メモリ」「誤転記リスク」の 3 要素を同時に満たす必要があります。 faster-whisper は速度・メモリ面での利点を明示しているため、まずはこれを基準実装 として検証し、要件に応じて GPU 化やモデルサイズの調整を行うのが合理的です。\nPage 25 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [48-51]",
      "heading_level": 3,
      "numbers": [
        "51",
        "51",
        "3 ",
        "25 ",
        "2026 ",
        "48",
        "51"
      ],
      "hash": "09d2d3010c20f51b",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p026_c00108",
      "block_type": "text",
      "page_no": 26,
      "order": 108,
      "bbox": [
        58.57,
        19.33,
        920.37,
        531.4
      ],
      "text": "採用判断基準( ASR ) RTF 要件と誤転記リスクに基づく選定フロー\n結論: faster-whisper を基準実装とし、 RTF 要件・誤転記リスクに応じて GPU 化やモデル拡大を検討する段階的最適化が合理的で す。\n前処理の固定\nVAD 設定の統一\n無音区間の除去は認識精度と速度に直結するため、 Silero VAD 等の設定を固定し再現性を担保する。\nベンチマーク条件\n同一音声・同一前処理での比較が必須(後章のベンチ設計参 照)。\n会議用途の要点\n話者分離が品質の土台\npyannote.audio 等の話者分離( Diarization )精度が議事録の質を 左右する。\n無音除去の設計\n幻覚(無音区間にテキストが入る現象)を防ぐため、 VAD によ る無音区間の破棄を徹底する。\nリスク管理\n誤転記(幻覚)への対処\n医療や重要業務では誤転記が実害となる可能性があるため、 検証プロセスを必須化。\n[49]\nリソース監視\nストリーミング用途では RTF だけでなく、長時間稼働時のメモ リリーク等も監視対象とする。\nPage 26 | ローカル AI 技術調査レポート",
      "normalized_text": "採用判断基準( ASR ) RTF 要件と誤転記リスクに基づく選定フロー\n結論: faster-whisper を基準実装とし、 RTF 要件・誤転記リスクに応じて GPU 化やモデル拡大を検討する段階的最適化が合理的で す。\n前処理の固定\nVAD 設定の統一\n無音区間の除去は認識精度と速度に直結するため、 Silero VAD 等の設定を固定し再現性を担保する。\nベンチマーク条件\n同一音声・同一前処理での比較が必須(後章のベンチ設計参 照)。\n会議用途の要点\n話者分離が品質の土台\npyannote.audio 等の話者分離( Diarization )精度が議事録の質を 左右する。\n無音除去の設計\n幻覚(無音区間にテキストが入る現象)を防ぐため、 VAD によ る無音区間の破棄を徹底する。\nリスク管理\n誤転記(幻覚)への対処\n医療や重要業務では誤転記が実害となる可能性があるため、 検証プロセスを必須化。\n[49]\nリソース監視\nストリーミング用途では RTF だけでなく、長時間稼働時のメモ リリーク等も監視対象とする。\nPage 26 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "49",
        "26 "
      ],
      "hash": "3fb05266d4446630",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p026_c00109",
      "block_type": "text",
      "page_no": 26,
      "order": 109,
      "bbox": [
        125.93,
        193.07,
        186.15,
        264.19
      ],
      "text": "1\n基準実装\nfaster-whisper (CPU/INT8) で検証",
      "normalized_text": "1\n基準実装\nfaster-whisper (CPU/INT8) で検証",
      "heading_level": 1,
      "numbers": [
        "1\n",
        "8"
      ],
      "hash": "70ec3198f47db792",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p026_c00110",
      "block_type": "text",
      "page_no": 26,
      "order": 110,
      "bbox": [
        344.11,
        193.07,
        401.01,
        263.32
      ],
      "text": "2\nRTF 評価\n実時間比 (RTF)<1 を満たすか確認",
      "normalized_text": "2\nRTF 評価\n実時間比 (RTF)<1 を満たすか確認",
      "heading_level": 1,
      "numbers": [
        "2\n",
        "1 "
      ],
      "hash": "9df5f09d4caddd61",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p026_c00111",
      "block_type": "text",
      "page_no": 26,
      "order": 111,
      "bbox": [
        562.22,
        193.07,
        614.92,
        263.32
      ],
      "text": "3\n品質評価\n誤転記 ( 幻覚 ) の\n許容範囲内か",
      "normalized_text": "3\n品質評価\n誤転記 ( 幻覚 ) の\n許容範囲内か",
      "heading_level": 1,
      "numbers": [
        "3\n"
      ],
      "hash": "939353cd235e8df1",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p026_c00112",
      "block_type": "text",
      "page_no": 26,
      "order": 112,
      "bbox": [
        772.71,
        193.07,
        836.81,
        263.32
      ],
      "text": "4\n最適化\n不足なら GPU 化 またはモデル拡大",
      "normalized_text": "4\n最適化\n不足なら GPU 化 またはモデル拡大",
      "heading_level": 1,
      "numbers": [
        "4\n"
      ],
      "hash": "6b7e08190521a642",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p027_c00113",
      "block_type": "text",
      "page_no": 27,
      "order": 113,
      "bbox": [
        62.69,
        24.28,
        917.51,
        395.38
      ],
      "text": "TTS (音声合成)概要 読み上げと音声クローンの分離設計\n結論:「読み上げ」は 軽量モデル( Piper/Kokoro ) で常駐化し、 「音声クローン」は 高品質モデル( XTTS/StyleTTS2 ) で分離設計します。\n代表システムと特徴\nPiper: \"fast, local neural TTS\" を標榜。省リソース・常駐向き。 [52]\nKokoro: 82M パラメータで高品質・高効率。 [53]\nXTTS-v2: 数秒の参照音声で多言語クローンが可能。 [54]\nStyleTTS2: 研究系で人間レベルの自然さを追求。 [55]",
      "normalized_text": "TTS (音声合成)概要 読み上げと音声クローンの分離設計\n結論:「読み上げ」は 軽量モデル( Piper/Kokoro ) で常駐化し、 「音声クローン」は 高品質モデル( XTTS/StyleTTS2 ) で分離設計します。\n代表システムと特徴\nPiper: \"fast, local neural TTS\" を標榜。省リソース・常駐向き。 [52]\nKokoro: 82M パラメータで高品質・高効率。 [53]\nXTTS-v2: 数秒の参照音声で多言語クローンが可能。 [54]\nStyleTTS2: 研究系で人間レベルの自然さを追求。 [55]",
      "heading_level": 1,
      "numbers": [
        "2 ",
        "52",
        "82",
        "53",
        "2",
        "54",
        "2",
        "55"
      ],
      "hash": "4edd81b5f3d9da28",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p027_c00114",
      "block_type": "text",
      "page_no": 27,
      "order": 114,
      "bbox": [
        70.36,
        241.22,
        909.16,
        526.7
      ],
      "text": "Text-to-Speech Voice Cloning\n運用の分離とリスク管理\n軽量 TTS を採用し、システム負荷を最小化。バックグラウンドでの安定 動作を優先。\nGPU リソースを割り当て品質を最大化。ただし、法務・権利リスク管理 (許諾・ログ監査)を必須要件とします。 [56]\nRisk Management Resource Optimization\n日本語品質への注意( Quality Assurance )\n日本語の品質(アクセント、抑揚)は、モデルだけでなく「辞書」や「テキスト前処理」に強く依存します。業務投入前には、特定のドメイン用語を用いた事前検証が不可欠です。",
      "normalized_text": "Text-to-Speech Voice Cloning\n運用の分離とリスク管理\n軽量 TTS を採用し、システム負荷を最小化。バックグラウンドでの安定 動作を優先。\nGPU リソースを割り当て品質を最大化。ただし、法務・権利リスク管理 (許諾・ログ監査)を必須要件とします。 [56]\nRisk Management Resource Optimization\n日本語品質への注意( Quality Assurance )\n日本語の品質(アクセント、抑揚)は、モデルだけでなく「辞書」や「テキスト前処理」に強く依存します。業務投入前には、特定のドメイン用語を用いた事前検証が不可欠です。",
      "heading_level": 3,
      "numbers": [
        "56"
      ],
      "hash": "c7695856533f7048",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p028_c00115",
      "block_type": "text",
      "page_no": 28,
      "order": 115,
      "bbox": [
        48.76,
        34.72,
        895.94,
        153.25
      ],
      "text": "TTS (完全版表) 主要モデル詳細比較: Piper / Kokoro / XTTS-v2 / StyleTTS2\n代表モデル / 方式 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断",
      "normalized_text": "TTS (完全版表) 主要モデル詳細比較: Piper / Kokoro / XTTS-v2 / StyleTTS2\n代表モデル / 方式 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "2\n"
      ],
      "hash": "4b648e95bdc0543c",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p028_c00116",
      "block_type": "text",
      "page_no": 28,
      "order": 116,
      "bbox": [
        48.76,
        195.23,
        907.91,
        229.5
      ],
      "text": "Piper Fast/Local [52]\n軽量 ローカル常駐 TTS 向き\n得意 \"fast, local neural TTS\" を明示 得意 省リソース環境で有利\nWin/Mac: 導入容易 Risk リポジトリ移転あり(運 用はフォーク / 移転先を確認 ) [52]",
      "normalized_text": "Piper Fast/Local [52]\n軽量 ローカル常駐 TTS 向き\n得意 \"fast, local neural TTS\" を明示 得意 省リソース環境で有利\nWin/Mac: 導入容易 Risk リポジトリ移転あり(運 用はフォーク / 移転先を確認 ) [52]",
      "heading_level": 3,
      "numbers": [
        "52",
        "52"
      ],
      "hash": "54597eda6d2149b8",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p028_c00117",
      "block_type": "text",
      "page_no": 28,
      "order": 117,
      "bbox": [
        48.76,
        271.85,
        910.51,
        305.04
      ],
      "text": "Kokoro Open Weight [53]\n82M 軽量 TTS として 使い分け\n得意 82M で高速・コスト効率を主張 得意 Apache ライセンス重みをうたう [53]\nCPU: 成立しやすい Note 日本語品質は声・辞書・ 前処理に依存(事前確認必 須)",
      "normalized_text": "Kokoro Open Weight [53]\n82M 軽量 TTS として 使い分け\n得意 82M で高速・コスト効率を主張 得意 Apache ライセンス重みをうたう [53]\nCPU: 成立しやすい Note 日本語品質は声・辞書・ 前処理に依存(事前確認必 須)",
      "heading_level": 3,
      "numbers": [
        "53",
        "82",
        "82",
        "53"
      ],
      "hash": "c5997e787394fdd5",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p028_c00118",
      "block_type": "text",
      "page_no": 28,
      "order": 118,
      "bbox": [
        48.76,
        348.51,
        907.24,
        379.88
      ],
      "text": "Coqui XTTS-v2 Clone [54]\n大きめ GPU 推奨 音声クローン寄り\n得意 数秒の参照音声で多言語クローン をうたう [54]\nGPU 環境 : 実務的 Risk 法務・倫理・権利リスク 大(同意・監査が必須) [54,56]",
      "normalized_text": "Coqui XTTS-v2 Clone [54]\n大きめ GPU 推奨 音声クローン寄り\n得意 数秒の参照音声で多言語クローン をうたう [54]\nGPU 環境 : 実務的 Risk 法務・倫理・権利リスク 大(同意・監査が必須) [54,56]",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "54",
        "54",
        "54,56"
      ],
      "hash": "99e3bf66bba18240",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p028_c00119",
      "block_type": "text",
      "page_no": 28,
      "order": 119,
      "bbox": [
        36.03,
        425.11,
        907.25,
        531.44
      ],
      "text": "StyleTTS2 Research [55]\n研究系 高品質 TTS 志向 得意 \"human-level\" を目標とする研究系 [55]\nGPU: 望ましい Risk 実運用は依存関係・再現 性の確認が必須\n採用判断の要点:「読み上げ(通知・要約)」と「クローン(本人声)」は別物として分離設計します。前者は Piper/Kokoro のような軽量系、後者は XTTS 等で、法務・倫理リスク管理を別レイヤに置きます。\n[56]\nPage 28 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [52-56]",
      "normalized_text": "StyleTTS2 Research [55]\n研究系 高品質 TTS 志向 得意 \"human-level\" を目標とする研究系 [55]\nGPU: 望ましい Risk 実運用は依存関係・再現 性の確認が必須\n採用判断の要点:「読み上げ(通知・要約)」と「クローン(本人声)」は別物として分離設計します。前者は Piper/Kokoro のような軽量系、後者は XTTS 等で、法務・倫理リスク管理を別レイヤに置きます。\n[56]\nPage 28 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [52-56]",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "55",
        "55",
        "56",
        "28 ",
        "2026 ",
        "52",
        "56"
      ],
      "hash": "a94a7f3cbad8ea47",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p029_c00120",
      "block_type": "text",
      "page_no": 29,
      "order": 120,
      "bbox": [
        58.57,
        19.33,
        924.37,
        124.74
      ],
      "text": "採用判断基準( TTS ) 品質・コスト・リスクの分離管理\n結論:「読み上げ(通知・要約)」と「クローン(本人声)」は別レイヤで運用すべきです。前者は軽量化し、後者は法務・ 倫理リスクを管理します。",
      "normalized_text": "採用判断基準( TTS ) 品質・コスト・リスクの分離管理\n結論:「読み上げ(通知・要約)」と「クローン(本人声)」は別レイヤで運用すべきです。前者は軽量化し、後者は法務・ 倫理リスクを管理します。",
      "heading_level": 1,
      "numbers": [],
      "hash": "ab588fc8c48ea3db",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p029_c00121",
      "block_type": "text",
      "page_no": 29,
      "order": 121,
      "bbox": [
        59.29,
        327.74,
        605.82,
        484.09
      ],
      "text": "分離運用( Layered )\n軽量 TTS の常駐\n通知・要約読み上げは Piper/Kokoro 等で省リソース化し、シス テムに常駐させる。\nクローンは別系統\nXTTS 等は GPU リソースを消費するため、必要な時のみ呼び出す 別サービスとして切り出す。\nリスク・コンプライアンス\n権利同意の必須化\n音声クローンは法務・倫理リスクが高い。業務利用時は対象 者の書面同意をプロセス化する。\nログ監査\n「誰が・いつ・何を」生成したかのログを保存し、不正利用 を追跡可能にする。\n[56]",
      "normalized_text": "分離運用( Layered )\n軽量 TTS の常駐\n通知・要約読み上げは Piper/Kokoro 等で省リソース化し、シス テムに常駐させる。\nクローンは別系統\nXTTS 等は GPU リソースを消費するため、必要な時のみ呼び出す 別サービスとして切り出す。\nリスク・コンプライアンス\n権利同意の必須化\n音声クローンは法務・倫理リスクが高い。業務利用時は対象 者の書面同意をプロセス化する。\nログ監査\n「誰が・いつ・何を」生成したかのログを保存し、不正利用 を追跡可能にする。\n[56]",
      "heading_level": 3,
      "numbers": [
        "56"
      ],
      "hash": "2623b856c706f284",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p029_c00122",
      "block_type": "text",
      "page_no": 29,
      "order": 122,
      "bbox": [
        403.1,
        327.74,
        907.0,
        531.4
      ],
      "text": "品質保証( QA )\n日本語品質の事前試験\nモデルによりアクセントや読み間違い(数値・固有名詞)の 差が大きい。\n辞書・前処理の標準化\n業務特有の用語はユーザー辞書や前処理ルール(正規化)で カバーする義務を課す。\nPage 29 | ローカル AI 技術調査レポート",
      "normalized_text": "品質保証( QA )\n日本語品質の事前試験\nモデルによりアクセントや読み間違い(数値・固有名詞)の 差が大きい。\n辞書・前処理の標準化\n業務特有の用語はユーザー辞書や前処理ルール(正規化)で カバーする義務を課す。\nPage 29 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "29 "
      ],
      "hash": "d30c0390c1392add",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p029_c00123",
      "block_type": "text",
      "page_no": 29,
      "order": 123,
      "bbox": [
        118.18,
        193.07,
        182.28,
        263.32
      ],
      "text": "1\n用途定義\n単なる読み上げか\n本人性の再現か",
      "normalized_text": "1\n用途定義\n単なる読み上げか\n本人性の再現か",
      "heading_level": 1,
      "numbers": [
        "1\n"
      ],
      "hash": "943a97b8b7af2488",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p029_c00124",
      "block_type": "text",
      "page_no": 29,
      "order": 124,
      "bbox": [
        345.23,
        193.07,
        395.66,
        264.19
      ],
      "text": "2\nモデル選定\n軽量 (Piper) vs\n高品質 (XTTS)",
      "normalized_text": "2\nモデル選定\n軽量 (Piper) vs\n高品質 (XTTS)",
      "heading_level": 1,
      "numbers": [
        "2\n"
      ],
      "hash": "b0ccc7cce0cb6aca",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p029_c00125",
      "block_type": "text",
      "page_no": 29,
      "order": 125,
      "bbox": [
        562.3,
        193.07,
        618.48,
        263.32
      ],
      "text": "3\nリスク判定\n声の権利許諾と\nログ監査設計",
      "normalized_text": "3\nリスク判定\n声の権利許諾と\nログ監査設計",
      "heading_level": 1,
      "numbers": [
        "3\n"
      ],
      "hash": "18e0fe5dc836314c",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p029_c00126",
      "block_type": "text",
      "page_no": 29,
      "order": 126,
      "bbox": [
        776.86,
        193.07,
        843.9,
        264.19
      ],
      "text": "4\n実装・運用\n常駐 (CPU) or オンデマンド (GPU)",
      "normalized_text": "4\n実装・運用\n常駐 (CPU) or オンデマンド (GPU)",
      "heading_level": 1,
      "numbers": [
        "4\n"
      ],
      "hash": "c00f2485cffe64c5",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p030_c00127",
      "block_type": "text",
      "page_no": 30,
      "order": 127,
      "bbox": [
        62.19,
        24.28,
        916.54,
        521.58
      ],
      "text": "Embedding / Reranker 概要 検索・ RAG 基盤モデル\n結論: RAG システムの品質上限は Embedding の検索精度 で決まります。 まず BGE-M3 等で Recall を確保し、 Reranker で Precision を向上させる二段構えが定石です。\nEmbedding ( ベクトル検索 )\nRetrieval Vector DB\nMulti-Functionality / Multi-Linguality / Multi-Granularity を特徴とし、多言語 ・多用途で強力なベースラインとなります。 [58]\n多言語検索の定番モデル。日本語を含む多言語環境での実績が豊富です 。 [59][60]\nReranker ( 再順位付け )\nPrecision Cross-Encoder\nクエリと文書をペアで入力し、関連度スコアを直接出力します。計算コ ストは高いですが、検索精度を大幅に引き上げます。 [61][62]\nEmbedding 検索で広めに取得した候補( Top-K )を絞り込み、 LLM へのコ ンテキスト汚染を防ぎます。\n運用上の設計含意( Operational Design )\nローカル運用では、リソース競合を避けるため「 Embedding 化(前計算)」を夜間バッチ等で済ませておくことが推奨されます。これにより、日中の推論時には実時間負荷を LLM の応答生成 に集中させることが可能になります。",
      "normalized_text": "Embedding / Reranker 概要 検索・ RAG 基盤モデル\n結論: RAG システムの品質上限は Embedding の検索精度 で決まります。 まず BGE-M3 等で Recall を確保し、 Reranker で Precision を向上させる二段構えが定石です。\nEmbedding ( ベクトル検索 )\nRetrieval Vector DB\nMulti-Functionality / Multi-Linguality / Multi-Granularity を特徴とし、多言語 ・多用途で強力なベースラインとなります。 [58]\n多言語検索の定番モデル。日本語を含む多言語環境での実績が豊富です 。 [59][60]\nReranker ( 再順位付け )\nPrecision Cross-Encoder\nクエリと文書をペアで入力し、関連度スコアを直接出力します。計算コ ストは高いですが、検索精度を大幅に引き上げます。 [61][62]\nEmbedding 検索で広めに取得した候補( Top-K )を絞り込み、 LLM へのコ ンテキスト汚染を防ぎます。\n運用上の設計含意( Operational Design )\nローカル運用では、リソース競合を避けるため「 Embedding 化(前計算)」を夜間バッチ等で済ませておくことが推奨されます。これにより、日中の推論時には実時間負荷を LLM の応答生成 に集中させることが可能になります。",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "58",
        "59",
        "60",
        "61",
        "62"
      ],
      "hash": "1bce1d93c4330712",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p031_c00128",
      "block_type": "text",
      "page_no": 31,
      "order": 128,
      "bbox": [
        48.76,
        34.72,
        891.13,
        151.74
      ],
      "text": "Embedding/Reranker (完全版表) 検索・ RAG 基盤: BGE-M3 / multilingual-e5 / bge-reranker\n代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断",
      "normalized_text": "Embedding/Reranker (完全版表) 検索・ RAG 基盤: BGE-M3 / multilingual-e5 / bge-reranker\n代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "5 "
      ],
      "hash": "dbde3f1fe6572a1e",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p031_c00129",
      "block_type": "text",
      "page_no": 31,
      "order": 129,
      "bbox": [
        48.76,
        192.22,
        910.37,
        288.59
      ],
      "text": "BGE-M3 BAAI [58]\nModel Card 参照\nFP16/INT8 で安定運用 必要なら量子化\n特徴 Multi-Functionality Multi-Linguality Multi-Granularity を明示 [58]\nCPU/GPU/ONNX 等で運用可能 (環境依存)\n推奨 RAG は Embedding 品質が が上限を決めるため、まず ここを堅くする\nmultilingual-e5 Intfloat [59-60]\nlarge large-instruct\nlarge 多言語検索用途で定番\n実績 技術報告が示され、使用例が 明確 [60]\ntransformers / ONNX 候補 日本語含む多言語を要す るなら有力候補",
      "normalized_text": "BGE-M3 BAAI [58]\nModel Card 参照\nFP16/INT8 で安定運用 必要なら量子化\n特徴 Multi-Functionality Multi-Linguality Multi-Granularity を明示 [58]\nCPU/GPU/ONNX 等で運用可能 (環境依存)\n推奨 RAG は Embedding 品質が が上限を決めるため、まず ここを堅くする\nmultilingual-e5 Intfloat [59-60]\nlarge large-instruct\nlarge 多言語検索用途で定番\n実績 技術報告が示され、使用例が 明確 [60]\ntransformers / ONNX 候補 日本語含む多言語を要す るなら有力候補",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "58",
        "16",
        "8 ",
        "58",
        "5 ",
        "59",
        "60",
        "60"
      ],
      "hash": "f81a5ad262425a5c",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 10
      }
    },
    {
      "chunk_id": "document_p031_c00130",
      "block_type": "text",
      "page_no": 31,
      "order": 130,
      "bbox": [
        36.03,
        339.48,
        914.95,
        531.44
      ],
      "text": "bge-reranker BAAI [61-62]\n278M 〜 560M 等\nlarge/v2 等 RAG の “ 再ランキング ” で 精度向上\n機能 「クエリ+文書を入力しスコ アを直接出す」とモデルカードで 説明 [62]\nCPU/GPU (遅延要件次第)\nRisk reranker はレイテンシを増 増やすため、 p95 要件で採否 を判断\n採用判断の要点: RAG の失敗の多くは「検索外れ」「上位が弱い」「文脈過多」です。 Embedding で Recall を確保し、 reranker で Precision を上げる二段構えが実務的推奨です。ローカル運用では、 Embedding の前計 算(夜間バッチ)により実時間の負荷を LLM 応答に集中させる設計が有効です。\nPage 31 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [58-62]",
      "normalized_text": "bge-reranker BAAI [61-62]\n278M 〜 560M 等\nlarge/v2 等 RAG の “ 再ランキング ” で 精度向上\n機能 「クエリ+文書を入力しスコ アを直接出す」とモデルカードで 説明 [62]\nCPU/GPU (遅延要件次第)\nRisk reranker はレイテンシを増 増やすため、 p95 要件で採否 を判断\n採用判断の要点: RAG の失敗の多くは「検索外れ」「上位が弱い」「文脈過多」です。 Embedding で Recall を確保し、 reranker で Precision を上げる二段構えが実務的推奨です。ローカル運用では、 Embedding の前計 算(夜間バッチ)により実時間の負荷を LLM 応答に集中させる設計が有効です。\nPage 31 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [58-62]",
      "heading_level": 3,
      "numbers": [
        "61",
        "62",
        "278",
        "560",
        "2 ",
        "62",
        "95 ",
        "31 ",
        "2026 ",
        "58",
        "62"
      ],
      "hash": "796307dd14dbd168",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p032_c00131",
      "block_type": "text",
      "page_no": 32,
      "order": 131,
      "bbox": [
        58.57,
        19.33,
        924.84,
        531.4
      ],
      "text": "採用判断基準( Embedding / Reranker ) 検索精度と運用安定化のための二段構え設計\n結論: Embedding で網羅性 (Recall) を確保し、 Reranker で精度 (Precision) を上げる「二段構え」が実務的な最適解です。\n運用の安定化\n前計算(夜間バッチ)\n検索対象文書の Embedding 化や OCR 処理は夜間にバッチ実行し 、日中の計算資源を LLM の応答生成に集中させる。\nインデックス更新\n頻繁な更新が必要ない場合は、静的インデックスとして管理 し、運用負荷を下げる。\n失敗パターンと回避策\n検索が外れる( Recall 不足)\n回避策:文書の前処理(チャンク分割)を見直す、または Hybrid 検索(キーワード検索併用)を導入する。\n上位文書の関連度が弱い\n回避策: Reranker を導入して上位の並び順を補正し、 LLM に渡 すノイズを減らす。\n文脈長対策\n文脈が長すぎる失敗\n検索結果を詰め込みすぎると LLM の「 Lost in the Middle 」現象 やメモリ枯渇を招く。\nReranker での絞り込み\nReranker でスコアの低い文書を大胆にカットし、 LLM には「確 度の高い少数精鋭」のコンテキストを渡す。\nPage 32 | ローカル AI 技術調査レポート",
      "normalized_text": "採用判断基準( Embedding / Reranker ) 検索精度と運用安定化のための二段構え設計\n結論: Embedding で網羅性 (Recall) を確保し、 Reranker で精度 (Precision) を上げる「二段構え」が実務的な最適解です。\n運用の安定化\n前計算(夜間バッチ)\n検索対象文書の Embedding 化や OCR 処理は夜間にバッチ実行し 、日中の計算資源を LLM の応答生成に集中させる。\nインデックス更新\n頻繁な更新が必要ない場合は、静的インデックスとして管理 し、運用負荷を下げる。\n失敗パターンと回避策\n検索が外れる( Recall 不足)\n回避策:文書の前処理(チャンク分割)を見直す、または Hybrid 検索(キーワード検索併用)を導入する。\n上位文書の関連度が弱い\n回避策: Reranker を導入して上位の並び順を補正し、 LLM に渡 すノイズを減らす。\n文脈長対策\n文脈が長すぎる失敗\n検索結果を詰め込みすぎると LLM の「 Lost in the Middle 」現象 やメモリ枯渇を招く。\nReranker での絞り込み\nReranker でスコアの低い文書を大胆にカットし、 LLM には「確 度の高い少数精鋭」のコンテキストを渡す。\nPage 32 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "32 "
      ],
      "hash": "7572b0b2b87fa602",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p032_c00132",
      "block_type": "text",
      "page_no": 32,
      "order": 132,
      "bbox": [
        186.83,
        173.33,
        195.97,
        191.36
      ],
      "text": "1",
      "normalized_text": "1",
      "heading_level": 1,
      "numbers": [
        "1"
      ],
      "hash": "1e4016f0cb247834",
      "meta": {
        "body_font_size": 9.38
      }
    },
    {
      "chunk_id": "document_p032_c00133",
      "block_type": "text",
      "page_no": 32,
      "order": 133,
      "bbox": [
        137.64,
        212.19,
        245.09,
        264.77
      ],
      "text": "Embedding\n広く候補を取得 (Recall 重視\n) BGE-M3 / E5",
      "normalized_text": "Embedding\n広く候補を取得 (Recall 重視\n) BGE-M3 / E5",
      "heading_level": 2,
      "numbers": [
        "3 ",
        "5"
      ],
      "hash": "27cca6560e440c23",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p032_c00134",
      "block_type": "text",
      "page_no": 32,
      "order": 134,
      "bbox": [
        475.7,
        173.33,
        484.84,
        191.36
      ],
      "text": "2",
      "normalized_text": "2",
      "heading_level": 1,
      "numbers": [
        "2"
      ],
      "hash": "a4eccc06d5dec598",
      "meta": {
        "body_font_size": 9.38
      }
    },
    {
      "chunk_id": "document_p032_c00135",
      "block_type": "text",
      "page_no": 32,
      "order": 135,
      "bbox": [
        428.33,
        212.19,
        532.21,
        253.95
      ],
      "text": "Reranker\n上位を厳選 (Precision 重視 )\nbge-reranker",
      "normalized_text": "Reranker\n上位を厳選 (Precision 重視 )\nbge-reranker",
      "heading_level": 2,
      "numbers": [],
      "hash": "41efc689f6423395",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p032_c00136",
      "block_type": "text",
      "page_no": 32,
      "order": 136,
      "bbox": [
        764.57,
        179.17,
        773.71,
        197.2
      ],
      "text": "3",
      "normalized_text": "3",
      "heading_level": 1,
      "numbers": [
        "3"
      ],
      "hash": "9ca806cb8d0e3698",
      "meta": {
        "body_font_size": 9.38
      }
    },
    {
      "chunk_id": "document_p032_c00137",
      "block_type": "text",
      "page_no": 32,
      "order": 137,
      "bbox": [
        724.52,
        216.76,
        814.52,
        259.79
      ],
      "text": "LLM 生成\n精度の高い文脈で回答\n( 幻覚抑制 )",
      "normalized_text": "LLM 生成\n精度の高い文脈で回答\n( 幻覚抑制 )",
      "heading_level": 3,
      "numbers": [],
      "hash": "5332a8850ddf4b38",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p033_c00138",
      "block_type": "text",
      "page_no": 33,
      "order": 138,
      "bbox": [
        58.57,
        24.28,
        914.24,
        256.37
      ],
      "text": "OCR / Document AI 概要 スキャン品質と構造化要件に基づく手法選択\n結論:スキャン品質やレイアウトの複雑度に応じて、 古典的 OCR と OCR-free ( Doc 理解) を使い分けるハイブリッド戦略が推奨されます。\n代表的なシステム・モデル",
      "normalized_text": "OCR / Document AI 概要 スキャン品質と構造化要件に基づく手法選択\n結論:スキャン品質やレイアウトの複雑度に応じて、 古典的 OCR と OCR-free ( Doc 理解) を使い分けるハイブリッド戦略が推奨されます。\n代表的なシステム・モデル",
      "heading_level": 3,
      "numbers": [],
      "hash": "adb2620bb627d2d9",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p033_c00139",
      "block_type": "text",
      "page_no": 33,
      "order": 139,
      "bbox": [
        72.77,
        282.25,
        373.15,
        312.99
      ],
      "text": "Tesseract OCR [64]\n古典的 OCR + LSTM 。単純なテキスト化のデファクトスタンダード。",
      "normalized_text": "Tesseract OCR [64]\n古典的 OCR + LSTM 。単純なテキスト化のデファクトスタンダード。",
      "heading_level": 3,
      "numbers": [
        "64"
      ],
      "hash": "9adb8eec5094c8ac",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p033_c00140",
      "block_type": "text",
      "page_no": 33,
      "order": 140,
      "bbox": [
        72.77,
        331.53,
        373.05,
        361.12
      ],
      "text": "PaddleOCR [66]\n高精度かつ多機能。表構造や縦書きなど複雑なレイアウトに強い。",
      "normalized_text": "PaddleOCR [66]\n高精度かつ多機能。表構造や縦書きなど複雑なレイアウトに強い。",
      "heading_level": 3,
      "numbers": [
        "66"
      ],
      "hash": "b27c0326f51f4130",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p033_c00141",
      "block_type": "text",
      "page_no": 33,
      "order": 141,
      "bbox": [
        72.77,
        380.74,
        323.44,
        411.56
      ],
      "text": "Donut (OCR-free) [68]\n画像を直接 Transformer で処理し、構造化データを抽出。",
      "normalized_text": "Donut (OCR-free) [68]\n画像を直接 Transformer で処理し、構造化データを抽出。",
      "heading_level": 3,
      "numbers": [
        "68"
      ],
      "hash": "bd41e2ba7b486391",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p033_c00142",
      "block_type": "text",
      "page_no": 33,
      "order": 142,
      "bbox": [
        68.3,
        241.22,
        908.9,
        548.63
      ],
      "text": "LayoutLMv3 [69]\nテキスト、画像、レイアウト情報を統合して文書理解を行う。\n運用指針と使い分け\n高品質スキャン・単純文書:\nTesseract を採用。高速かつ軽量で、単純なテキスト化に最適。\n低品質・複雑レイアウト・構造化:\nPaddleOCR 、 Donut 、 LayoutLMv3 を採用。傾きやノイズに強く、フォームや表の理解が 可能。\nHybrid Strategy Layout Aware Structure Extraction\nRAG 精度への影響( Key Takeaway )\nレイアウト情報の保持は RAG の回答精度に直結します。単なるテキスト抽出では表や図表の意味関係が失われるため、 LayoutLMv3 や Donut による構造的理解が、特に業務文書 RAG において重",
      "normalized_text": "LayoutLMv3 [69]\nテキスト、画像、レイアウト情報を統合して文書理解を行う。\n運用指針と使い分け\n高品質スキャン・単純文書:\nTesseract を採用。高速かつ軽量で、単純なテキスト化に最適。\n低品質・複雑レイアウト・構造化:\nPaddleOCR 、 Donut 、 LayoutLMv3 を採用。傾きやノイズに強く、フォームや表の理解が 可能。\nHybrid Strategy Layout Aware Structure Extraction\nRAG 精度への影響( Key Takeaway )\nレイアウト情報の保持は RAG の回答精度に直結します。単なるテキスト抽出では表や図表の意味関係が失われるため、 LayoutLMv3 や Donut による構造的理解が、特に業務文書 RAG において重",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "69",
        "3 ",
        "3 "
      ],
      "hash": "f65eeb48d7a2ec4d",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 9
      }
    },
    {
      "chunk_id": "document_p034_c00143",
      "block_type": "text",
      "page_no": 34,
      "order": 143,
      "bbox": [
        48.76,
        34.72,
        889.54,
        153.84
      ],
      "text": "OCR / Document AI (完全版表) 主要モデル詳細比較: Tesseract / PaddleOCR / Donut / LayoutLMv3\n代表モデル / 方式 方式 推奨実行形態 得意 / 不得意(要点) ランタイム相性",
      "normalized_text": "OCR / Document AI (完全版表) 主要モデル詳細比較: Tesseract / PaddleOCR / Donut / LayoutLMv3\n代表モデル / 方式 方式 推奨実行形態 得意 / 不得意(要点) ランタイム相性",
      "heading_level": 3,
      "numbers": [
        "3\n"
      ],
      "hash": "318be123831bddeb",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p034_c00144",
      "block_type": "text",
      "page_no": 34,
      "order": 144,
      "bbox": [
        48.76,
        196.42,
        873.81,
        230.71
      ],
      "text": "Tesseract OCR Google [64]\n古典 OCR + LSTM\nテキスト化基盤 スキャン品質高なら CPU で十分\n得意 長年の実績と安定性 不得意 複雑レイアウトや手書き文字は苦手\nCross-Platform: Win/Mac/Linux 問わず導入容易",
      "normalized_text": "Tesseract OCR Google [64]\n古典 OCR + LSTM\nテキスト化基盤 スキャン品質高なら CPU で十分\n得意 長年の実績と安定性 不得意 複雑レイアウトや手書き文字は苦手\nCross-Platform: Win/Mac/Linux 問わず導入容易",
      "heading_level": 3,
      "numbers": [
        "64"
      ],
      "hash": "ca5f96922f41bee4",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p034_c00145",
      "block_type": "text",
      "page_no": 34,
      "order": 145,
      "bbox": [
        48.76,
        274.28,
        869.8,
        297.0
      ],
      "text": "PaddleOCR Baidu [66]\nDeep Learning OCR\n高精度・多機能 構造化出力向け\n得意 軽量かつ高精度な認識 特徴 表認識やレイアウト解析機能も充実\nPython / ONNX: Python 環境で容易に実装可能",
      "normalized_text": "PaddleOCR Baidu [66]\nDeep Learning OCR\n高精度・多機能 構造化出力向け\n得意 軽量かつ高精度な認識 特徴 表認識やレイアウト解析機能も充実\nPython / ONNX: Python 環境で容易に実装可能",
      "heading_level": 3,
      "numbers": [
        "66"
      ],
      "hash": "09da9392391e78c8",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p034_c00146",
      "block_type": "text",
      "page_no": 34,
      "order": 146,
      "bbox": [
        48.76,
        352.06,
        872.35,
        386.35
      ],
      "text": "Donut Clova AI [68]\nOCR-free Transformer\nDoc 理解・抽出 画像から直接 JSON 等へ\n得意 OCR 誤りを経由せず直接情報抽出 特徴 フォームや請求書等の定型文書に強み\nTransformers: Hugging Face Transformers 対応",
      "normalized_text": "Donut Clova AI [68]\nOCR-free Transformer\nDoc 理解・抽出 画像から直接 JSON 等へ\n得意 OCR 誤りを経由せず直接情報抽出 特徴 フォームや請求書等の定型文書に強み\nTransformers: Hugging Face Transformers 対応",
      "heading_level": 3,
      "numbers": [
        "68"
      ],
      "hash": "3567059f50bf5ff6",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p034_c00147",
      "block_type": "text",
      "page_no": 34,
      "order": 147,
      "bbox": [
        36.03,
        429.9,
        913.9,
        531.44
      ],
      "text": "LayoutLMv3 Microsoft [69]\nMultimodal Transformer\nレイアウト保持 文脈理解統合\n得意 視覚情報とテキスト情報を統合して理解 特徴 表 / 図 / レイアウト由来の欠落を補完 [69]\nTransformers / ONNX: 推論エンジンでの最適化が可能\nレイアウト保持の重要性: RAG において、単なるテキスト化では図表やレイアウト構造に含まれる情報が欠落しがちです。 PaddleOCR の構造化出力や、 Donut/LayoutLMv3 のような Document AI モデルを活用し、レ イアウト情報を保持することが検索精度の向上に直結します。\nPage 34 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [64, 66, 68, 69]",
      "normalized_text": "LayoutLMv3 Microsoft [69]\nMultimodal Transformer\nレイアウト保持 文脈理解統合\n得意 視覚情報とテキスト情報を統合して理解 特徴 表 / 図 / レイアウト由来の欠落を補完 [69]\nTransformers / ONNX: 推論エンジンでの最適化が可能\nレイアウト保持の重要性: RAG において、単なるテキスト化では図表やレイアウト構造に含まれる情報が欠落しがちです。 PaddleOCR の構造化出力や、 Donut/LayoutLMv3 のような Document AI モデルを活用し、レ イアウト情報を保持することが検索精度の向上に直結します。\nPage 34 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [64, 66, 68, 69]",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "69",
        "69",
        "3 ",
        "34 ",
        "2026 ",
        "64, ",
        "66, ",
        "68, ",
        "69"
      ],
      "hash": "320957c6fbc0ebed",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p035_c00148",
      "block_type": "text",
      "page_no": 35,
      "order": 148,
      "bbox": [
        58.57,
        19.33,
        923.36,
        467.76
      ],
      "text": "OCR/Document AI 採用判断基準 リスク緩和のための UX 要件とハイブリッド構成\n結論:誤読リスクをゼロにはできない前提で、前処理の標準化と「原文引用」による人間系確認の UX を要件化します。\n前処理の標準化\n品質の下限を担保\n傾き補正、ノイズ除去、二値化を OCR 前段に固定的に組み込む ことで、認識精度のベースラインを確保。\n画像の正規化\n解像度やコントラストのバラつきを抑え、モデルの得意な入 力形式に合わせる。\n高品質方針:ハイブリッド\n構造化とレイアウト保持\nPaddleOCR でテキスト構造化を行い、 Donut/LayoutLMv3 でレイ アウト情報を保持して RAG 連携精度を高める。\n図表・ UI への対応\n複雑な図表や UI スクショは VLM と OCR を併用し、視覚情報と文 字情報を相互補完させて堅牢化する。",
      "normalized_text": "OCR/Document AI 採用判断基準 リスク緩和のための UX 要件とハイブリッド構成\n結論:誤読リスクをゼロにはできない前提で、前処理の標準化と「原文引用」による人間系確認の UX を要件化します。\n前処理の標準化\n品質の下限を担保\n傾き補正、ノイズ除去、二値化を OCR 前段に固定的に組み込む ことで、認識精度のベースラインを確保。\n画像の正規化\n解像度やコントラストのバラつきを抑え、モデルの得意な入 力形式に合わせる。\n高品質方針:ハイブリッド\n構造化とレイアウト保持\nPaddleOCR でテキスト構造化を行い、 Donut/LayoutLMv3 でレイ アウト情報を保持して RAG 連携精度を高める。\n図表・ UI への対応\n複雑な図表や UI スクショは VLM と OCR を併用し、視覚情報と文 字情報を相互補完させて堅牢化する。",
      "heading_level": 3,
      "numbers": [
        "3 "
      ],
      "hash": "26e6c7a35dc9aeee",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p035_c00149",
      "block_type": "text",
      "page_no": 35,
      "order": 149,
      "bbox": [
        403.1,
        306.76,
        914.5,
        531.4
      ],
      "text": "UX 要件と運用\n原文スニペット引用\nAI 回答の根拠となった原文箇所(画像切り出し等)を提示し、 ユーザーが誤読を検証できる UX を必須化。\n夜間バッチへのオフロード\n重い OCR/Embedding 処理は夜間に回し、日中のリソースは検索 と短い回答生成に集中させる。\nPage 35 | ローカル AI 技術調査レポート",
      "normalized_text": "UX 要件と運用\n原文スニペット引用\nAI 回答の根拠となった原文箇所(画像切り出し等)を提示し、 ユーザーが誤読を検証できる UX を必須化。\n夜間バッチへのオフロード\n重い OCR/Embedding 処理は夜間に回し、日中のリソースは検索 と短い回答生成に集中させる。\nPage 35 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "35 "
      ],
      "hash": "892ed84c78304ff0",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p035_c00150",
      "block_type": "text",
      "page_no": 35,
      "order": 150,
      "bbox": [
        84.17,
        172.09,
        172.0,
        242.27
      ],
      "text": "1\n品質評価\nスキャン品質と レイアウト複雑度を確認",
      "normalized_text": "1\n品質評価\nスキャン品質と レイアウト複雑度を確認",
      "heading_level": 1,
      "numbers": [
        "1\n"
      ],
      "hash": "1bb46a9903983205",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p035_c00151",
      "block_type": "text",
      "page_no": 35,
      "order": 151,
      "bbox": [
        268.19,
        172.09,
        340.21,
        242.27
      ],
      "text": "2\n前処理\n傾き補正・二値化の 標準パイプライン化",
      "normalized_text": "2\n前処理\n傾き補正・二値化の 標準パイプライン化",
      "heading_level": 1,
      "numbers": [
        "2\n"
      ],
      "hash": "1044531f13519cf2",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p035_c00152",
      "block_type": "text",
      "page_no": 35,
      "order": 152,
      "bbox": [
        446.0,
        172.09,
        515.06,
        243.13
      ],
      "text": "3\nモデル選択\nPaddleOCR( 構造化 ) +Donut( レイアウト )",
      "normalized_text": "3\nモデル選択\nPaddleOCR( 構造化 ) +Donut( レイアウト )",
      "heading_level": 1,
      "numbers": [
        "3\n"
      ],
      "hash": "9d8b4f506128791a",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p035_c00153",
      "block_type": "text",
      "page_no": 35,
      "order": 153,
      "bbox": [
        616.27,
        172.09,
        696.21,
        242.27
      ],
      "text": "4\nUX 要件化\n原文スニペット表示で\n誤読リスクを緩和",
      "normalized_text": "4\nUX 要件化\n原文スニペット表示で\n誤読リスクを緩和",
      "heading_level": 1,
      "numbers": [
        "4\n"
      ],
      "hash": "8d71b1f136aedf89",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p035_c00154",
      "block_type": "text",
      "page_no": 35,
      "order": 154,
      "bbox": [
        800.47,
        172.09,
        864.66,
        242.27
      ],
      "text": "5\n運用設計\nOCR/Embedding は 夜間バッチ処理へ",
      "normalized_text": "5\n運用設計\nOCR/Embedding は 夜間バッチ処理へ",
      "heading_level": 1,
      "numbers": [
        "5\n"
      ],
      "hash": "92e9d71f063e3506",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p036_c00155",
      "block_type": "text",
      "page_no": 36,
      "order": 155,
      "bbox": [
        62.69,
        24.28,
        916.72,
        544.69
      ],
      "text": "画像生成( Diffusion )概要 モデル・ランタイム・実務ポイント\n結論:画像生成は SDXL を軸に、 ComfyUI/SD WebUI で運用。 FLUX 等はライセンス精査が必須であり、 VRAM 要件はワークフローに依存します。\n主要モデルと UI ( Runtime )\n高品質生成の標準モデル。ライセンス確認の上、低コスト構成の軸に。\n[71]\nノードベース UI 。 Windows/Linux/macOS 対応を明示し、ワークフローの 再利用性が高い。 [72]\n豊富なプラグインエコシステムを持つ定番 UI 。 [73]\n最新の上位モデル。高い品質ポテンシャルを持つが、商用利用等のライ センス条件( dev 版等)に注意。 [75][76]\nDiffusion ComfyUI VRAM Optimization\n実務運用ポイントとリスク\n解像度・ステップ数・バッチサイズにより VRAM 消費が激増します。 attention slicing 等の最適化適用が必須。\n特に FLUX.1-dev 等はライセンス条件が異なる場合があるため、採用前に 必ず確認してください。 [76]\n業務利用では「 seed 固定」と「ワークフロー固定( ComfyUI json )」で 出力を安定させます。",
      "normalized_text": "画像生成( Diffusion )概要 モデル・ランタイム・実務ポイント\n結論:画像生成は SDXL を軸に、 ComfyUI/SD WebUI で運用。 FLUX 等はライセンス精査が必須であり、 VRAM 要件はワークフローに依存します。\n主要モデルと UI ( Runtime )\n高品質生成の標準モデル。ライセンス確認の上、低コスト構成の軸に。\n[71]\nノードベース UI 。 Windows/Linux/macOS 対応を明示し、ワークフローの 再利用性が高い。 [72]\n豊富なプラグインエコシステムを持つ定番 UI 。 [73]\n最新の上位モデル。高い品質ポテンシャルを持つが、商用利用等のライ センス条件( dev 版等)に注意。 [75][76]\nDiffusion ComfyUI VRAM Optimization\n実務運用ポイントとリスク\n解像度・ステップ数・バッチサイズにより VRAM 消費が激増します。 attention slicing 等の最適化適用が必須。\n特に FLUX.1-dev 等はライセンス条件が異なる場合があるため、採用前に 必ず確認してください。 [76]\n業務利用では「 seed 固定」と「ワークフロー固定( ComfyUI json )」で 出力を安定させます。",
      "heading_level": 3,
      "numbers": [
        "71",
        "72",
        "73",
        "75",
        "76",
        "1",
        "76"
      ],
      "hash": "e17b8ba66cdaa279",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 12
      }
    },
    {
      "chunk_id": "document_p037_c00156",
      "block_type": "text",
      "page_no": 37,
      "order": 156,
      "bbox": [
        48.76,
        34.72,
        906.37,
        252.85
      ],
      "text": "画像生成(完全版表) 主要モデル・ UI 詳細比較: SDXL / ComfyUI / SD WebUI / FLUX\nモデル / UI 方式 / 特徴 VRAM 要件 得意 / 不得意(要点) リスク / 採用判断\nSDXL base 1.0 Stability AI [71]\nDiffusion ワークフロー依存 (解像度・バッチ・ステップ数 に大きく左右される)\n得意 高品質生成(ベースモデルとして 標準的)\n注意 解像度・バッチ設定で負荷が大幅 変動\n判断 attention slicing 等の最適化を前提にプ プロファイルを固定化",
      "normalized_text": "画像生成(完全版表) 主要モデル・ UI 詳細比較: SDXL / ComfyUI / SD WebUI / FLUX\nモデル / UI 方式 / 特徴 VRAM 要件 得意 / 不得意(要点) リスク / 採用判断\nSDXL base 1.0 Stability AI [71]\nDiffusion ワークフロー依存 (解像度・バッチ・ステップ数 に大きく左右される)\n得意 高品質生成(ベースモデルとして 標準的)\n注意 解像度・バッチ設定で負荷が大幅 変動\n判断 attention slicing 等の最適化を前提にプ プロファイルを固定化",
      "heading_level": 0,
      "numbers": [
        "1.0 ",
        "71"
      ],
      "hash": "1008987ff7c554c0",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p037_c00157",
      "block_type": "text",
      "page_no": 37,
      "order": 157,
      "bbox": [
        48.76,
        296.2,
        911.11,
        330.48
      ],
      "text": "ComfyUI UI [72]\nノード型 UI Win/Linux/macOS 対応\n構成次第 (効率的なメモリ管理が可能)\n得意 再利用性・資産化(ワークフロー 保存) 得意 seed 固定による再現性担保\n判断 ワークフローのブラックボックス化を 防ぐため管理が必要",
      "normalized_text": "ComfyUI UI [72]\nノード型 UI Win/Linux/macOS 対応\n構成次第 (効率的なメモリ管理が可能)\n得意 再利用性・資産化(ワークフロー 保存) 得意 seed 固定による再現性担保\n判断 ワークフローのブラックボックス化を 防ぐため管理が必要",
      "heading_level": 3,
      "numbers": [
        "72"
      ],
      "hash": "4ef70432faa760f5",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p037_c00158",
      "block_type": "text",
      "page_no": 37,
      "order": 158,
      "bbox": [
        48.76,
        385.03,
        911.08,
        418.18
      ],
      "text": "Stable Diffusion WebUI UI [73]\nプラグイン豊富 設定・拡張依存 得意 導入が比較的容易\n注意 設定差による品質ブレが発生しや すい\n判断 簡易利用には適するが、厳密な再現性 には注意",
      "normalized_text": "Stable Diffusion WebUI UI [73]\nプラグイン豊富 設定・拡張依存 得意 導入が比較的容易\n注意 設定差による品質ブレが発生しや すい\n判断 簡易利用には適するが、厳密な再現性 には注意",
      "heading_level": 3,
      "numbers": [
        "73"
      ],
      "hash": "19b5e6713d41991c",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p037_c00159",
      "block_type": "text",
      "page_no": 37,
      "order": 159,
      "bbox": [
        36.03,
        458.4,
        912.7,
        531.44
      ],
      "text": "FLUX.1 Black Forest Labs [75]\n上位モデル系 高め (高品質ゆえのリソース要求)\n得意 高い品質ポテンシャル Risk ライセンス(特に dev 版の条件)の精査 が必須 [76] 商用利用可否を必ず確認\n採用判断の要点: VRAM 容量が最大の制約となります。解像度、バッチサイズ、ステップ数を固定したプロファイルを作成し、 VRAM 不足を防ぐ運用設計が重要です。特に FLUX 等の最新・上位モデルを採用する 場合は、ライセンス条件(商用利用制限など)を事前に入念に確認してください。\nPage 37 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [71-76]",
      "normalized_text": "FLUX.1 Black Forest Labs [75]\n上位モデル系 高め (高品質ゆえのリソース要求)\n得意 高い品質ポテンシャル Risk ライセンス(特に dev 版の条件)の精査 が必須 [76] 商用利用可否を必ず確認\n採用判断の要点: VRAM 容量が最大の制約となります。解像度、バッチサイズ、ステップ数を固定したプロファイルを作成し、 VRAM 不足を防ぐ運用設計が重要です。特に FLUX 等の最新・上位モデルを採用する 場合は、ライセンス条件(商用利用制限など)を事前に入念に確認してください。\nPage 37 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [71-76]",
      "heading_level": 3,
      "numbers": [
        "1 ",
        "75",
        "76",
        "37 ",
        "2026 ",
        "71",
        "76"
      ],
      "hash": "cfed010258f642d5",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p038_c00160",
      "block_type": "text",
      "page_no": 38,
      "order": 160,
      "bbox": [
        5.98,
        5.8,
        641.53,
        550.49
      ],
      "text": "\\\\n\\\\n 採用判断基準(画像生成) \\\\n\nVRAM 制約と再現性の担保\n\\\\n 結論: VRAM 制約に合わせて解像度 / バッチ / ステップを固定し、ワークフローを資産化して再現性を担保します。 \\\\n\n1\nVRAM 制約\nハードウェア上限を まず確認\n2\n解像度固定\nVRAM に収まる 最大サイズを決定\n3\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n \\\\n \\\\n\n採用判断基準(画像生成) \\\\n\n\\\\n \\\\n \\\\n\\\\n \\\\n\n\\\\n \\\\n\\\\n \\\\n \\\\n\n\\\\n\n\\\\n\n\\\\n \\\\n \\\\n \\\\n\n\\\\n\n\\\\n\n\\\\n \\\\n \\\\n \\\\n\n\\\\n",
      "normalized_text": "\\\\n\\\\n 採用判断基準(画像生成) \\\\n\nVRAM 制約と再現性の担保\n\\\\n 結論: VRAM 制約に合わせて解像度 / バッチ / ステップを固定し、ワークフローを資産化して再現性を担保します。 \\\\n\n1\nVRAM 制約\nハードウェア上限を まず確認\n2\n解像度固定\nVRAM に収まる 最大サイズを決定\n3\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n \\\\n \\\\n\n採用判断基準(画像生成) \\\\n\n\\\\n \\\\n \\\\n\\\\n \\\\n\n\\\\n \\\\n\\\\n \\\\n \\\\n\n\\\\n\n\\\\n\n\\\\n \\\\n \\\\n \\\\n\n\\\\n\n\\\\n\n\\\\n \\\\n \\\\n \\\\n\n\\\\n",
      "heading_level": 0,
      "numbers": [
        "1\n",
        "2\n",
        "3\n"
      ],
      "hash": "30b92fc24422eebd",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 20
      }
    },
    {
      "chunk_id": "document_p039_c00161",
      "block_type": "text",
      "page_no": 39,
      "order": 161,
      "bbox": [
        58.57,
        25.41,
        915.56,
        151.01
      ],
      "text": "Agent / Tool-use 概要 ローカル環境でのエージェント構築と実行\n結論: Agent は「 LLM +ツール呼び出し(関数)+実行環境 」で構成され、 Ollama/LM Studio の OpenAI 互換 API を用いる いることで、ローカルでの実務的な構築が現実化しています。",
      "normalized_text": "Agent / Tool-use 概要 ローカル環境でのエージェント構築と実行\n結論: Agent は「 LLM +ツール呼び出し(関数)+実行環境 」で構成され、 Ollama/LM Studio の OpenAI 互換 API を用いる いることで、ローカルでの実務的な構築が現実化しています。",
      "heading_level": 3,
      "numbers": [],
      "hash": "4c91688a93328df8",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p039_c00162",
      "block_type": "text",
      "page_no": 39,
      "order": 162,
      "bbox": [
        68.77,
        221.28,
        455.01,
        515.33
      ],
      "text": "構成要素と API 基盤\nエージェントの基本構成。 LLM が判断し、定義されたツール(関数)を 呼び出し、ローカル環境で実行して結果を返すループ構造。\nOllama や LM Studio は、ローカルで動作しながら OpenAI 互換のエンドポイ ポイントを提供。これにより、既存の Agent フレームワークやクライア ントツールをそのまま利用可能。\n[23][24][41]\nOpenHands 等の自律エージェントツールでも、ローカル LLM を利用する るガイドが整備されています。 [2]\nOpenAI API Compatible Local Execution",
      "normalized_text": "構成要素と API 基盤\nエージェントの基本構成。 LLM が判断し、定義されたツール(関数)を 呼び出し、ローカル環境で実行して結果を返すループ構造。\nOllama や LM Studio は、ローカルで動作しながら OpenAI 互換のエンドポイ ポイントを提供。これにより、既存の Agent フレームワークやクライア ントツールをそのまま利用可能。\n[23][24][41]\nOpenHands 等の自律エージェントツールでも、ローカル LLM を利用する るガイドが整備されています。 [2]\nOpenAI API Compatible Local Execution",
      "heading_level": 3,
      "numbers": [
        "23",
        "24",
        "41",
        "2"
      ],
      "hash": "37617eb5447f7074",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p039_c00163",
      "block_type": "text",
      "page_no": 39,
      "order": 163,
      "bbox": [
        36.03,
        222.93,
        922.54,
        531.79
      ],
      "text": "Function Calling (Tool Use)\nローカル API でも tool_choice や functions パラメータを利用可能。モデ ルが JSON 形式で引数を生成し、システム側で実行します。\n[24][27]\nローカルモデル(特に小型)では JSON スキーマの破壊が発生しやすいた め、型定義の厳格化と、パース失敗時の再試行ロジックが不可欠です。\n生成されたコードやコマンドを盲目的に実行せず、サンドボックス内で の検証や、構文チェックを挟む設計が推奨されます。\nJSON Schema Verification Loop\n設計含意( Design Implication )\nJSON スキーマ遵守と検証ループ設計(失敗時のリトライ / フォールバック)がローカル Agent の成否を分けます。特に「幻覚 API (存在しない関数呼び出し)」への対策として、実行可能な関 数リストのホワイトリスト化が必要です。\nLocal AI Technical Survey Report 2026 39 / 80",
      "normalized_text": "Function Calling (Tool Use)\nローカル API でも tool_choice や functions パラメータを利用可能。モデ ルが JSON 形式で引数を生成し、システム側で実行します。\n[24][27]\nローカルモデル(特に小型)では JSON スキーマの破壊が発生しやすいた め、型定義の厳格化と、パース失敗時の再試行ロジックが不可欠です。\n生成されたコードやコマンドを盲目的に実行せず、サンドボックス内で の検証や、構文チェックを挟む設計が推奨されます。\nJSON Schema Verification Loop\n設計含意( Design Implication )\nJSON スキーマ遵守と検証ループ設計(失敗時のリトライ / フォールバック)がローカル Agent の成否を分けます。特に「幻覚 API (存在しない関数呼び出し)」への対策として、実行可能な関 数リストのホワイトリスト化が必要です。\nLocal AI Technical Survey Report 2026 39 / 80",
      "heading_level": 2,
      "numbers": [
        "24",
        "27",
        "2026 ",
        "39 ",
        "80"
      ],
      "hash": "ec01e128f917a51c",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 8
      }
    },
    {
      "chunk_id": "document_p040_c00164",
      "block_type": "text",
      "page_no": 40,
      "order": 164,
      "bbox": [
        36.03,
        34.72,
        918.06,
        550.54
      ],
      "text": "Agent/Tool-use (完全版表) OpenAI 互換 API / Function Calling / 実装基盤の詳細比較\n項目・方式 詳細仕様・提供形態 利点・特徴 注意点・リスク\nOpenAI 互換 API インターフェース [24,41]\n提供: Ollama / LM Studio ローカル LLM を標準的な REST API として公開 Chat Completions API 互換\n互換性 既存のエージェントフレー ムワークやクライアントツールを そのまま接続可能\n仕様差 一部機能( Stateful 等)が完 全互換ではない場合があるため検 証が必要\nFunction Calling Tool Use [24,27]\n出力形式:構造化 JSON LLM がツール実行を判断し、引数を JSON で生成\n構造化 自然言語ではなく実行可能 なデータ構造で出力\n実務的 業務システム連携の核\nJSON 破壊 スキーマ遵守率(破壊率 )はモデル性能と量子化に依存\n実装基盤 : Ollama / LM Studio Runtime [3,19,23]\nOllama : CLI/API 中心、 Modelfile 管理 LM Studio : GUI 中心、サーバー機能内蔵\n導入容易 複雑な環境構築なしで API サーバー化\nローカル 完全オフライン実行が可 能\n設定 コンテキスト長や GPU オフロ ード設定の最適化が必要\n参考実装 : OpenHands Framework [2]\n構成:ローカル LLM 利用ガイドあり 自律型エージェント開発のフレームワーク\n実践例 ローカル LLM でのエージェ ント構築のベストプラクティスを 参照可能\nリソース エージェント実行は推論 回数が多くなりがちで負荷が高い\n実装のポイント:安定運用の鍵は「 JSON スキーマの固定」と「パラメータ(温度 /top_p )の固定」です。幻覚( Hallucination )による不正な関数呼び出しを防ぐため、実行前に許可された関数リストと照合するホ ワイトリスト方式を推奨します。また、トラブルシューティング用に API の要求・応答ログを必ず保存してください。\nPage 40 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [2,3,19,23,24,27,41]",
      "normalized_text": "Agent/Tool-use (完全版表) OpenAI 互換 API / Function Calling / 実装基盤の詳細比較\n項目・方式 詳細仕様・提供形態 利点・特徴 注意点・リスク\nOpenAI 互換 API インターフェース [24,41]\n提供: Ollama / LM Studio ローカル LLM を標準的な REST API として公開 Chat Completions API 互換\n互換性 既存のエージェントフレー ムワークやクライアントツールを そのまま接続可能\n仕様差 一部機能( Stateful 等)が完 全互換ではない場合があるため検 証が必要\nFunction Calling Tool Use [24,27]\n出力形式:構造化 JSON LLM がツール実行を判断し、引数を JSON で生成\n構造化 自然言語ではなく実行可能 なデータ構造で出力\n実務的 業務システム連携の核\nJSON 破壊 スキーマ遵守率(破壊率 )はモデル性能と量子化に依存\n実装基盤 : Ollama / LM Studio Runtime [3,19,23]\nOllama : CLI/API 中心、 Modelfile 管理 LM Studio : GUI 中心、サーバー機能内蔵\n導入容易 複雑な環境構築なしで API サーバー化\nローカル 完全オフライン実行が可 能\n設定 コンテキスト長や GPU オフロ ード設定の最適化が必要\n参考実装 : OpenHands Framework [2]\n構成:ローカル LLM 利用ガイドあり 自律型エージェント開発のフレームワーク\n実践例 ローカル LLM でのエージェ ント構築のベストプラクティスを 参照可能\nリソース エージェント実行は推論 回数が多くなりがちで負荷が高い\n実装のポイント:安定運用の鍵は「 JSON スキーマの固定」と「パラメータ(温度 /top_p )の固定」です。幻覚( Hallucination )による不正な関数呼び出しを防ぐため、実行前に許可された関数リストと照合するホ ワイトリスト方式を推奨します。また、トラブルシューティング用に API の要求・応答ログを必ず保存してください。\nPage 40 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [2,3,19,23,24,27,41]",
      "heading_level": 3,
      "numbers": [
        "24,41",
        "24,27",
        "3,19,23",
        "2",
        "40 ",
        "2026 ",
        "2,3,19,23,24,27,41"
      ],
      "hash": "261a4877b02fd5e4",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 19
      }
    },
    {
      "chunk_id": "document_p041_c00165",
      "block_type": "text",
      "page_no": 41,
      "order": 165,
      "bbox": [
        58.57,
        19.33,
        919.11,
        492.19
      ],
      "text": "採用判断基準( Agent / Tool-use ) JSON スキーマ固定と検証ループによる安全運用\n結論: JSON スキーマ固定+検証ループ(自動チェック)による安全運用を基本とし、 Ollama/LM Studio の OpenAI 互換 API で実装 を統一します。\nスキーマと検証\nJSON スキーマの厳格化 必須引数、型定義、 Enum 制約を厳密に 記述し、モデルの構造化出力能力を最大限に活用。破壊時は即 時再生成へ。\n検証ループの実装 「コード生成 → コンパイル / テスト実行」自 体をツールとして組み込み、 LLM 自身に結果をフィードバック するループを構築。\n実装基盤の統一\nOpenAI 互換 API で統一 Ollama/LM Studio 等の互換 API を採用し、 クライアントコードを標準化。移行性・保守性を確保。",
      "normalized_text": "採用判断基準( Agent / Tool-use ) JSON スキーマ固定と検証ループによる安全運用\n結論: JSON スキーマ固定+検証ループ(自動チェック)による安全運用を基本とし、 Ollama/LM Studio の OpenAI 互換 API で実装 を統一します。\nスキーマと検証\nJSON スキーマの厳格化 必須引数、型定義、 Enum 制約を厳密に 記述し、モデルの構造化出力能力を最大限に活用。破壊時は即 時再生成へ。\n検証ループの実装 「コード生成 → コンパイル / テスト実行」自 体をツールとして組み込み、 LLM 自身に結果をフィードバック するループを構築。\n実装基盤の統一\nOpenAI 互換 API で統一 Ollama/LM Studio 等の互換 API を採用し、 クライアントコードを標準化。移行性・保守性を確保。",
      "heading_level": 3,
      "numbers": [],
      "hash": "efda48a6ea782cb5",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p041_c00166",
      "block_type": "text",
      "page_no": 41,
      "order": 166,
      "bbox": [
        360.4,
        396.59,
        607.8,
        504.42
      ],
      "text": "[24,41]\nランタイム機能の活用 必要に応じて、 llama-cpp-python 等のラ ンタイム固有機能(文法制約機能など)を補助的に活用。\n[25]",
      "normalized_text": "[24,41]\nランタイム機能の活用 必要に応じて、 llama-cpp-python 等のラ ンタイム固有機能(文法制約機能など)を補助的に活用。\n[25]",
      "heading_level": 3,
      "numbers": [
        "24,41",
        "25"
      ],
      "hash": "4b7550f695b9e7cd",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p041_c00167",
      "block_type": "text",
      "page_no": 41,
      "order": 167,
      "bbox": [
        113.41,
        235.7,
        913.57,
        531.4
      ],
      "text": "幻覚 API 対策\nホワイトリスト化 実行可能な関数セットを厳密にホワイトリ スト化し、存在しない API の呼び出し(幻覚)をシステム側で 遮断。\nタイムアウトとリトライ 無限ループや応答遅延を防ぐため、 標準でタイムアウト設定とリトライ上限を設ける設計を義務化 。\nPage 41 | ローカル AI 技術調査レポート\nスキーマ定義\n必須引数・型・制約を\n厳格化して固定\nツール実行\nOllama/LM Studio 経由\nOpenAI 互換 API\n検証ループ\nコンパイル / テスト実行\n失敗時は再生成",
      "normalized_text": "幻覚 API 対策\nホワイトリスト化 実行可能な関数セットを厳密にホワイトリ スト化し、存在しない API の呼び出し(幻覚)をシステム側で 遮断。\nタイムアウトとリトライ 無限ループや応答遅延を防ぐため、 標準でタイムアウト設定とリトライ上限を設ける設計を義務化 。\nPage 41 | ローカル AI 技術調査レポート\nスキーマ定義\n必須引数・型・制約を\n厳格化して固定\nツール実行\nOllama/LM Studio 経由\nOpenAI 互換 API\n検証ループ\nコンパイル / テスト実行\n失敗時は再生成",
      "heading_level": 3,
      "numbers": [
        "41 "
      ],
      "hash": "1753b310c4efa2ea",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 9
      }
    },
    {
      "chunk_id": "document_p041_c00168",
      "block_type": "text",
      "page_no": 41,
      "order": 168,
      "bbox": [
        767.12,
        235.7,
        837.9,
        275.79
      ],
      "text": "完了 / リトライ\n結果確認または フォールバック",
      "normalized_text": "完了 / リトライ\n結果確認または フォールバック",
      "heading_level": 3,
      "numbers": [],
      "hash": "4f31b26aed25f156",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p042_c00169",
      "block_type": "text",
      "page_no": 42,
      "order": 169,
      "bbox": [
        58.57,
        25.41,
        919.05,
        347.99
      ],
      "text": "音声周辺( VAD/ 話者分離 / ウェイクワード / ノイズ除去)概要 カテゴリ別モデルカタログ\n結論:前処理( VAD/ 話者分離 / ウェイクワード / ノイズ除去)が音声 UX の品質下限を決定します。 Silero VAD による無音除去と pyannote.audio による話者分離が実務上の標準構成です。\n主要コンポーネント( Primary )\nVAD ( Voice Activity Detection ): Silero VAD [82]\n無音・雑音区間を事前除去し、 ASR の幻覚( Hallucination )を抑制。軽量・高精度でデフ ァクトスタンダード。\n話者分離( Diarization ): pyannote.audio [84]\n「誰が話したか」を特定。会議議事録の品質に直結するが、依存関係が重く計算コスト も高い。",
      "normalized_text": "音声周辺( VAD/ 話者分離 / ウェイクワード / ノイズ除去)概要 カテゴリ別モデルカタログ\n結論:前処理( VAD/ 話者分離 / ウェイクワード / ノイズ除去)が音声 UX の品質下限を決定します。 Silero VAD による無音除去と pyannote.audio による話者分離が実務上の標準構成です。\n主要コンポーネント( Primary )\nVAD ( Voice Activity Detection ): Silero VAD [82]\n無音・雑音区間を事前除去し、 ASR の幻覚( Hallucination )を抑制。軽量・高精度でデフ ァクトスタンダード。\n話者分離( Diarization ): pyannote.audio [84]\n「誰が話したか」を特定。会議議事録の品質に直結するが、依存関係が重く計算コスト も高い。",
      "heading_level": 3,
      "numbers": [
        "82",
        "84"
      ],
      "hash": "2cbb37310942aaba",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p042_c00170",
      "block_type": "text",
      "page_no": 42,
      "order": 170,
      "bbox": [
        61.9,
        216.49,
        912.74,
        386.13
      ],
      "text": "Silero VAD pyannote.audio\n補助コンポーネント( Secondary )\nウェイクワード( Wake Word ): openWakeWord [85]\n「 Hey Siri 」等の起動語検出。オンデバイスで動作し、誤検知( False Positive )とのトレー ドオフ設計が鍵。\nノイズ除去( Noise Suppression ): RNNoise [14]\nRNN ベースの軽量ノイズ抑制。実用十分な性能だが、過度な適用は音質劣化を招くため チューニングが必要。",
      "normalized_text": "Silero VAD pyannote.audio\n補助コンポーネント( Secondary )\nウェイクワード( Wake Word ): openWakeWord [85]\n「 Hey Siri 」等の起動語検出。オンデバイスで動作し、誤検知( False Positive )とのトレー ドオフ設計が鍵。\nノイズ除去( Noise Suppression ): RNNoise [14]\nRNN ベースの軽量ノイズ抑制。実用十分な性能だが、過度な適用は音質劣化を招くため チューニングが必要。",
      "heading_level": 3,
      "numbers": [
        "85",
        "14"
      ],
      "hash": "8e31112b18389568",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p042_c00171",
      "block_type": "text",
      "page_no": 42,
      "order": 171,
      "bbox": [
        36.03,
        376.74,
        922.54,
        531.79
      ],
      "text": "openWakeWord RNNoise\n設計含意( Design Implication )\nこれら前処理の設定差(閾値、モデルバージョン)で最終的な認識精度が大きく変動します。ベンチマーク時は「同一前処理条件」を固定することが再現性の担保に不可欠です。\nLocal AI Technical Survey Report 2026 42 / 80",
      "normalized_text": "openWakeWord RNNoise\n設計含意( Design Implication )\nこれら前処理の設定差(閾値、モデルバージョン)で最終的な認識精度が大きく変動します。ベンチマーク時は「同一前処理条件」を固定することが再現性の担保に不可欠です。\nLocal AI Technical Survey Report 2026 42 / 80",
      "heading_level": 3,
      "numbers": [
        "2026 ",
        "42 ",
        "80"
      ],
      "hash": "ba339ce3c11e9217",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p043_c00172",
      "block_type": "text",
      "page_no": 43,
      "order": 172,
      "bbox": [
        48.76,
        34.72,
        890.66,
        152.19
      ],
      "text": "音声周辺(完全版表) Silero VAD / pyannote / openWakeWord / RNNoise 詳細比較\n代表モデル / ツール 役割 実行要件 / 形式 利点 / 特徴 リスク / 注意点",
      "normalized_text": "音声周辺(完全版表) Silero VAD / pyannote / openWakeWord / RNNoise 詳細比較\n代表モデル / ツール 役割 実行要件 / 形式 利点 / 特徴 リスク / 注意点",
      "heading_level": 0,
      "numbers": [],
      "hash": "e570d866ae50e80e",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p043_c00173",
      "block_type": "text",
      "page_no": 43,
      "order": 173,
      "bbox": [
        48.76,
        193.12,
        909.05,
        290.38
      ],
      "text": "Silero VAD Snakers4 [82]\n無音 / 雑音除去 (Voice Activity Detection)\n軽量・ CPU 動作可 ONNX / PyTorch\n標準 前処理のデファクト ASR 負荷を大幅に削減\nAttention 閾値設定に依存(誤って語 語頭を切るリスクあり)\npyannote.audio HuggingFace [84]\n話者分離 (Speaker Diarization)\nGPU 推奨 依存関係が重め\n高精度 会議録で「誰が話したか」を特 定するのに必須\nRisk 処理負荷が高い 環境構築の難易度がやや高い",
      "normalized_text": "Silero VAD Snakers4 [82]\n無音 / 雑音除去 (Voice Activity Detection)\n軽量・ CPU 動作可 ONNX / PyTorch\n標準 前処理のデファクト ASR 負荷を大幅に削減\nAttention 閾値設定に依存(誤って語 語頭を切るリスクあり)\npyannote.audio HuggingFace [84]\n話者分離 (Speaker Diarization)\nGPU 推奨 依存関係が重め\n高精度 会議録で「誰が話したか」を特 定するのに必須\nRisk 処理負荷が高い 環境構築の難易度がやや高い",
      "heading_level": 3,
      "numbers": [
        "4 ",
        "82",
        "84"
      ],
      "hash": "e807c4ff4b89e4fb",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 9
      }
    },
    {
      "chunk_id": "document_p043_c00174",
      "block_type": "text",
      "page_no": 43,
      "order": 174,
      "bbox": [
        48.76,
        342.16,
        904.25,
        375.35
      ],
      "text": "openWakeWord dscripka [85]\nウェイクワード検出 (Wake Word Detection)\nローカル動作 CPU / tflite 等\n実用 カスタムウェイクワード作成が可 能 非常に軽量\nAttention 誤検知( False Positive )の の調整が必要",
      "normalized_text": "openWakeWord dscripka [85]\nウェイクワード検出 (Wake Word Detection)\nローカル動作 CPU / tflite 等\n実用 カスタムウェイクワード作成が可 能 非常に軽量\nAttention 誤検知( False Positive )の の調整が必要",
      "heading_level": 3,
      "numbers": [
        "85"
      ],
      "hash": "65989523e635e199",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p043_c00175",
      "block_type": "text",
      "page_no": 43,
      "order": 175,
      "bbox": [
        36.03,
        416.69,
        910.39,
        531.44
      ],
      "text": "RNNoise Xiph.Org [14]\nノイズ除去 (Noise Suppression)\n極めて軽量 C/C++ / WebAssembly\n高速 リアルタイム処理向き RNN ベースで背景雑音を抑制\nRisk 音質変化(声がロボットっぽく なる等)のリスク\n設計の推奨:会議系ワークフローでは「 VAD → ASR → 要約」の直列処理を基本とし、負荷の高い話者分離( pyannote 等)は必要時のみ追加する設計が実用的です。すべての音声に一律で話者分離を適用すると 、処理時間( RTF )が大幅に悪化する可能性があります。\nPage 43 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [14, 82, 84, 85]",
      "normalized_text": "RNNoise Xiph.Org [14]\nノイズ除去 (Noise Suppression)\n極めて軽量 C/C++ / WebAssembly\n高速 リアルタイム処理向き RNN ベースで背景雑音を抑制\nRisk 音質変化(声がロボットっぽく なる等)のリスク\n設計の推奨:会議系ワークフローでは「 VAD → ASR → 要約」の直列処理を基本とし、負荷の高い話者分離( pyannote 等)は必要時のみ追加する設計が実用的です。すべての音声に一律で話者分離を適用すると 、処理時間( RTF )が大幅に悪化する可能性があります。\nPage 43 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [14, 82, 84, 85]",
      "heading_level": 3,
      "numbers": [
        "14",
        "43 ",
        "2026 ",
        "14, ",
        "82, ",
        "84, ",
        "85"
      ],
      "hash": "3ca62132d0a44cea",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p044_c00176",
      "block_type": "text",
      "page_no": 44,
      "order": 176,
      "bbox": [
        58.57,
        19.33,
        924.86,
        531.4
      ],
      "text": "採用判断基準(音声周辺) 前処理パイプライン標準化と品質管理\n結論:前処理パイプラインを標準化し、閾値・モデルバージョン・ RTF 目標を固定することで、 UX の品質下限を担保します。\nパイプラインの標準化\n基準設定のプリセット化\nVAD 閾値、話者分離の有無、ノイズ除去の有無をユースケース ごとに固定セットとして定義。\n再現性の担保\nモデルバージョンとパラメータをコードで固定し、環境によ る挙動差を排除。\nログ管理・監査\nパフォーマンス監視\nRTF (実時間係数)とエラー率をログに保存し、処理遅延や異 常を検知。\n長時間処理のオフロード\n長時間の音声処理は夜間バッチ化し、日中の計算資源を ASR や 要約などの対話的タスクに集中。\n誤検知対策\nUI による再確認設計\n誤検知(ウェイクワード)や誤割当(話者分離)が発生した 場合、ユーザーが手動修正できる UI を用意。\n安全側の設計\n重要な判定では、自動処理の結果を「提案」として提示し、 最終決定をユーザーに委ねるフェイルセーフ設計。\nPage 44 | ローカル AI 技術調査レポート",
      "normalized_text": "採用判断基準(音声周辺) 前処理パイプライン標準化と品質管理\n結論:前処理パイプラインを標準化し、閾値・モデルバージョン・ RTF 目標を固定することで、 UX の品質下限を担保します。\nパイプラインの標準化\n基準設定のプリセット化\nVAD 閾値、話者分離の有無、ノイズ除去の有無をユースケース ごとに固定セットとして定義。\n再現性の担保\nモデルバージョンとパラメータをコードで固定し、環境によ る挙動差を排除。\nログ管理・監査\nパフォーマンス監視\nRTF (実時間係数)とエラー率をログに保存し、処理遅延や異 常を検知。\n長時間処理のオフロード\n長時間の音声処理は夜間バッチ化し、日中の計算資源を ASR や 要約などの対話的タスクに集中。\n誤検知対策\nUI による再確認設計\n誤検知(ウェイクワード)や誤割当(話者分離)が発生した 場合、ユーザーが手動修正できる UI を用意。\n安全側の設計\n重要な判定では、自動処理の結果を「提案」として提示し、 最終決定をユーザーに委ねるフェイルセーフ設計。\nPage 44 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "44 "
      ],
      "hash": "d4a8985074f8c368",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p044_c00177",
      "block_type": "text",
      "page_no": 44,
      "order": 177,
      "bbox": [
        88.29,
        172.09,
        168.23,
        242.27
      ],
      "text": "1\nユースケース定義\n会議録音か ウェイクワードか",
      "normalized_text": "1\nユースケース定義\n会議録音か ウェイクワードか",
      "heading_level": 1,
      "numbers": [
        "1\n"
      ],
      "hash": "fca38960c235f30e",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p044_c00178",
      "block_type": "text",
      "page_no": 44,
      "order": 178,
      "bbox": [
        264.19,
        172.09,
        344.13,
        242.27
      ],
      "text": "2\nパイプライン構成\nVAD+ASR+ 分離など\nプリセット化",
      "normalized_text": "2\nパイプライン構成\nVAD+ASR+ 分離など\nプリセット化",
      "heading_level": 1,
      "numbers": [
        "2\n"
      ],
      "hash": "fd0ea009c126afce",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p044_c00179",
      "block_type": "text",
      "page_no": 44,
      "order": 179,
      "bbox": [
        445.46,
        172.09,
        515.66,
        242.27
      ],
      "text": "3\n閾値固定\nVAD 感度・分離閾値\nを数値で管理",
      "normalized_text": "3\n閾値固定\nVAD 感度・分離閾値\nを数値で管理",
      "heading_level": 1,
      "numbers": [
        "3\n"
      ],
      "hash": "26e2bc2f2a5a0818",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p044_c00180",
      "block_type": "text",
      "page_no": 44,
      "order": 180,
      "bbox": [
        621.41,
        172.09,
        691.43,
        242.27
      ],
      "text": "4\nバージョン固定\nSilero/pyannote の\nバージョン統一",
      "normalized_text": "4\nバージョン固定\nSilero/pyannote の\nバージョン統一",
      "heading_level": 1,
      "numbers": [
        "4\n"
      ],
      "hash": "f0ba61a8f95d4ad2",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p044_c00181",
      "block_type": "text",
      "page_no": 44,
      "order": 181,
      "bbox": [
        800.37,
        172.09,
        864.44,
        242.27
      ],
      "text": "5\nログ監視\nRTF ・エラー率の 継続モニタリング",
      "normalized_text": "5\nログ監視\nRTF ・エラー率の 継続モニタリング",
      "heading_level": 1,
      "numbers": [
        "5\n"
      ],
      "hash": "acc3bbefa44c490d",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p045_c00182",
      "block_type": "text",
      "page_no": 45,
      "order": 182,
      "bbox": [
        60.01,
        112.77,
        159.84,
        213.28
      ],
      "text": "07",
      "normalized_text": "07",
      "heading_level": 1,
      "numbers": [
        "07"
      ],
      "hash": "ac49b410a7f6f153",
      "meta": {
        "body_font_size": 12.98
      }
    },
    {
      "chunk_id": "document_p045_c00183",
      "block_type": "text",
      "page_no": 45,
      "order": 183,
      "bbox": [
        60.01,
        262.39,
        331.93,
        409.99
      ],
      "text": "メモリ設計の コア\n重み量子化・ KV キャッシュ管理と 最適化技術",
      "normalized_text": "メモリ設計の コア\n重み量子化・ KV キャッシュ管理と 最適化技術",
      "heading_level": 3,
      "numbers": [],
      "hash": "aab004639d71f61f",
      "meta": {
        "body_font_size": 12.98,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p045_c00184",
      "block_type": "text",
      "page_no": 45,
      "order": 184,
      "bbox": [
        492.23,
        76.6,
        883.34,
        461.11
      ],
      "text": "KE Y TAKEAWAYS\n本章の要点\n重みメモリの現実解:\n4bit 量子化( weight-only )が基本。理論値+ 10% オーバーヘ ッドで設計します。\nKV キャッシュの支配性:\n長文コンテキストでは KV がメモリを圧迫。 vLLM の paged/quantized KV [5,6] や TensorRT-LLM の KV reuse [7] が必須で す。\n最適化技術の活用:\nllama.cpp の prompt cache [88,90] 等で TTFT を短縮し、実用性を 高めます。\n重み理論値計算 KV キャッシュ詳細 数式・数値例 最適化技術",
      "normalized_text": "KE Y TAKEAWAYS\n本章の要点\n重みメモリの現実解:\n4bit 量子化( weight-only )が基本。理論値+ 10% オーバーヘ ッドで設計します。\nKV キャッシュの支配性:\n長文コンテキストでは KV がメモリを圧迫。 vLLM の paged/quantized KV [5,6] や TensorRT-LLM の KV reuse [7] が必須で す。\n最適化技術の活用:\nllama.cpp の prompt cache [88,90] 等で TTFT を短縮し、実用性を 高めます。\n重み理論値計算 KV キャッシュ詳細 数式・数値例 最適化技術",
      "heading_level": 3,
      "numbers": [
        "4",
        "10%",
        "5,6",
        "7",
        "88,90"
      ],
      "hash": "a4bb7307ce7f39ec",
      "meta": {
        "body_font_size": 12.98,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p046_c00185",
      "block_type": "text",
      "page_no": 46,
      "order": 185,
      "bbox": [
        58.57,
        24.28,
        914.01,
        162.41
      ],
      "text": "メモリ設計のコア(概要) 重み量子化と KV キャッシュの支配性\n結論:ローカル AI のメモリ制約は、 「重み 4bit 量子化」 と 「 KV キャッシュ最適化」 の 2 点によって決定さ れる支配的な要因です。",
      "normalized_text": "メモリ設計のコア(概要) 重み量子化と KV キャッシュの支配性\n結論:ローカル AI のメモリ制約は、 「重み 4bit 量子化」 と 「 KV キャッシュ最適化」 の 2 点によって決定さ れる支配的な要因です。",
      "heading_level": 0,
      "numbers": [
        "4",
        "2 "
      ],
      "hash": "945cc14dba23ecd8",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p046_c00186",
      "block_type": "text",
      "page_no": 46,
      "order": 186,
      "bbox": [
        69.02,
        241.22,
        449.54,
        431.64
      ],
      "text": "重みメモリ( Weights )\nパラメータ数と bit 幅で物理的な下限が決まります。\nメモリ ≈ パラメータ数 × bit 幅 ÷ 8 ( + 約 10% ラ\nンタイム /CUDA オーバーヘッド)\nFP16 ( 16bit )と比較して約 1/4 のサイズで、品質劣化を最小限に抑えつ つコンシューマ GPU に載せるための必須技術です。",
      "normalized_text": "重みメモリ( Weights )\nパラメータ数と bit 幅で物理的な下限が決まります。\nメモリ ≈ パラメータ数 × bit 幅 ÷ 8 ( + 約 10% ラ\nンタイム /CUDA オーバーヘッド)\nFP16 ( 16bit )と比較して約 1/4 のサイズで、品質劣化を最小限に抑えつ つコンシューマ GPU に載せるための必須技術です。",
      "heading_level": 3,
      "numbers": [
        "8 ",
        "10%",
        "16 ",
        "16",
        "1",
        "4 "
      ],
      "hash": "a1a71639285b062c",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p046_c00187",
      "block_type": "text",
      "page_no": 46,
      "order": 187,
      "bbox": [
        69.32,
        459.8,
        257.37,
        469.91
      ],
      "text": "GGUF Q4_K_M AWQ 4bit GPTQ",
      "normalized_text": "GGUF Q4_K_M AWQ 4bit GPTQ",
      "heading_level": 3,
      "numbers": [
        "4",
        "4"
      ],
      "hash": "8c685a88d2864e52",
      "meta": {
        "body_font_size": 10.82
      }
    },
    {
      "chunk_id": "document_p046_c00188",
      "block_type": "text",
      "page_no": 46,
      "order": 188,
      "bbox": [
        69.82,
        241.22,
        901.23,
        549.14
      ],
      "text": "KV キャッシュと最適化\nKV キャッシュは「コンテキスト長に比例」して増加します。長文・多 同時接続では重み以上にメモリを圧迫します。\n例: Llama3 8B (n_layer=32, n_head_kv=8, head_dim=128)\nPaged KV: メモリ断片化を防ぐ( vLLM 等) [5]\nQuantized KV: FP8/INT4 化で容量削減 [6]\nKV Reuse: 計算再利用( TensorRT-LLM ) [7]\nPrompt Cache: TTFT 短縮( llama.cpp ) [88][90]\n根拠ソース()",
      "normalized_text": "KV キャッシュと最適化\nKV キャッシュは「コンテキスト長に比例」して増加します。長文・多 同時接続では重み以上にメモリを圧迫します。\n例: Llama3 8B (n_layer=32, n_head_kv=8, head_dim=128)\nPaged KV: メモリ断片化を防ぐ( vLLM 等) [5]\nQuantized KV: FP8/INT4 化で容量削減 [6]\nKV Reuse: 計算再利用( TensorRT-LLM ) [7]\nPrompt Cache: TTFT 短縮( llama.cpp ) [88][90]\n根拠ソース()",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "8",
        "32, ",
        "8, ",
        "128",
        "5",
        "8",
        "4 ",
        "6",
        "7",
        "88",
        "90"
      ],
      "hash": "d2609a4ea6f37359",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p047_c00189",
      "block_type": "text",
      "page_no": 47,
      "order": 189,
      "bbox": [
        48.76,
        34.72,
        919.88,
        141.6
      ],
      "text": "重みメモリ理論値(完全版表 1 : 0.5B 〜 14B ) モデル規模別推奨メモリ容量( +10% オーバーヘッド込み概算)\nモデル規模 (Parameters) 4bit (Q4_K_M 等 )\nローカル推奨\nINT8 (8bit) 標準的量子化\nFP16 (16bit) 元精度 / 学習時",
      "normalized_text": "重みメモリ理論値(完全版表 1 : 0.5B 〜 14B ) モデル規模別推奨メモリ容量( +10% オーバーヘッド込み概算)\nモデル規模 (Parameters) 4bit (Q4_K_M 等 )\nローカル推奨\nINT8 (8bit) 標準的量子化\nFP16 (16bit) 元精度 / 学習時",
      "heading_level": 3,
      "numbers": [
        "1 ",
        "0.5",
        "14",
        "10%",
        "4",
        "4",
        "8 ",
        "8",
        "16 ",
        "16"
      ],
      "hash": "f822003f09928bae",
      "meta": {
        "body_font_size": 9.36,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p047_c00190",
      "block_type": "text",
      "page_no": 47,
      "order": 190,
      "bbox": [
        48.76,
        168.41,
        828.36,
        180.65
      ],
      "text": "0.5B (Qwen2.5-0.5B 等 ) 0.3 GiB 0.5 GiB 1.0 GiB",
      "normalized_text": "0.5B (Qwen2.5-0.5B 等 ) 0.3 GiB 0.5 GiB 1.0 GiB",
      "heading_level": 3,
      "numbers": [
        "0.5",
        "2.5",
        "0.5",
        "0.3 ",
        "0.5 ",
        "1.0 "
      ],
      "hash": "20819bb6314d6993",
      "meta": {
        "body_font_size": 9.36
      }
    },
    {
      "chunk_id": "document_p047_c00191",
      "block_type": "text",
      "page_no": 47,
      "order": 191,
      "bbox": [
        48.76,
        209.95,
        828.36,
        222.19
      ],
      "text": "1.5B (Qwen2.5-1.5B 等 ) 0.8 GiB 1.5 GiB 3.1 GiB",
      "normalized_text": "1.5B (Qwen2.5-1.5B 等 ) 0.8 GiB 1.5 GiB 3.1 GiB",
      "heading_level": 3,
      "numbers": [
        "1.5",
        "2.5",
        "1.5",
        "0.8 ",
        "1.5 ",
        "3.1 "
      ],
      "hash": "7acd3fbea48134d0",
      "meta": {
        "body_font_size": 9.36
      }
    },
    {
      "chunk_id": "document_p047_c00192",
      "block_type": "text",
      "page_no": 47,
      "order": 192,
      "bbox": [
        48.76,
        251.48,
        828.36,
        263.72
      ],
      "text": "3B (Phi-3 Mini 等 ) 1.5 GiB 3.1 GiB 6.1 GiB",
      "normalized_text": "3B (Phi-3 Mini 等 ) 1.5 GiB 3.1 GiB 6.1 GiB",
      "heading_level": 3,
      "numbers": [
        "3",
        "3 ",
        "1.5 ",
        "3.1 ",
        "6.1 "
      ],
      "hash": "4dfdd917b8ce2e17",
      "meta": {
        "body_font_size": 9.36
      }
    },
    {
      "chunk_id": "document_p047_c00193",
      "block_type": "text",
      "page_no": 47,
      "order": 193,
      "bbox": [
        48.76,
        293.02,
        831.45,
        305.26
      ],
      "text": "7B (Qwen2.5-7B 等 ) 3.6 GiB 7.2 GiB 14.3 GiB",
      "normalized_text": "7B (Qwen2.5-7B 等 ) 3.6 GiB 7.2 GiB 14.3 GiB",
      "heading_level": 3,
      "numbers": [
        "7",
        "2.5",
        "7",
        "3.6 ",
        "7.2 ",
        "14.3 "
      ],
      "hash": "5710369310bce9ee",
      "meta": {
        "body_font_size": 9.36
      }
    },
    {
      "chunk_id": "document_p047_c00194",
      "block_type": "text",
      "page_no": 47,
      "order": 194,
      "bbox": [
        36.03,
        334.56,
        917.9,
        531.44
      ],
      "text": "8B (Llama 3.1 8B 等 ) 4.1 GiB 8.2 GiB 16.4 GiB\n14B (Qwen2.5-14B/Phi-4 等 ) 7.2 GiB 14.3 GiB 28.7 GiB\n計算根拠:上記数値は「パラメータ数 × bit 幅 ÷ 8 」に、ランタイムオーバヘッドとして約 10% を加算した概算理論値です。これらは重みのみのメモリ消費であり、実運用ではこれに加えて KV キャッシュ(コンテキ スト長に比例)が必要となります。特に 7B 〜 14B モデルは、 8GB/16GB メモリ環境での動作可否の境界線となるため、 4bit 量子化の活用が実務上必須となります。\nPage 47 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report (Calculated Estimates)",
      "normalized_text": "8B (Llama 3.1 8B 等 ) 4.1 GiB 8.2 GiB 16.4 GiB\n14B (Qwen2.5-14B/Phi-4 等 ) 7.2 GiB 14.3 GiB 28.7 GiB\n計算根拠:上記数値は「パラメータ数 × bit 幅 ÷ 8 」に、ランタイムオーバヘッドとして約 10% を加算した概算理論値です。これらは重みのみのメモリ消費であり、実運用ではこれに加えて KV キャッシュ(コンテキ スト長に比例)が必要となります。特に 7B 〜 14B モデルは、 8GB/16GB メモリ環境での動作可否の境界線となるため、 4bit 量子化の活用が実務上必須となります。\nPage 47 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report (Calculated Estimates)",
      "heading_level": 3,
      "numbers": [
        "8",
        "3.1 ",
        "8",
        "4.1 ",
        "8.2 ",
        "16.4 ",
        "14",
        "2.5",
        "14",
        "4 ",
        "7.2 ",
        "14.3 ",
        "28.7 ",
        "8 ",
        "10%",
        "7",
        "14",
        "8",
        "16",
        "4",
        "47 ",
        "2026 "
      ],
      "hash": "b382c7b07d9efff6",
      "meta": {
        "body_font_size": 9.36,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p048_c00195",
      "block_type": "text",
      "page_no": 48,
      "order": 195,
      "bbox": [
        54.74,
        34.72,
        912.89,
        144.61
      ],
      "text": "重みメモリ理論値(完全版表 2 : 27B 〜 70B ) 大規模モデル( 4bit/INT8/FP16 )のメモリ要件\nモデル規模 4bit (Q4) 推奨 IN T8 (Q8) FP16 (Half)",
      "normalized_text": "重みメモリ理論値(完全版表 2 : 27B 〜 70B ) 大規模モデル( 4bit/INT8/FP16 )のメモリ要件\nモデル規模 4bit (Q4) 推奨 IN T8 (Q8) FP16 (Half)",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "27",
        "70",
        "4",
        "8",
        "16 ",
        "4",
        "4",
        "8 ",
        "8",
        "16 "
      ],
      "hash": "9e57e4b1dd3af7b1",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p048_c00196",
      "block_type": "text",
      "page_no": 48,
      "order": 196,
      "bbox": [
        54.74,
        185.19,
        683.38,
        207.75
      ],
      "text": "27B Gemma 2 27B 等 13.8 GiB 27.7 GiB 55.3 GiB",
      "normalized_text": "27B Gemma 2 27B 等 13.8 GiB 27.7 GiB 55.3 GiB",
      "heading_level": 3,
      "numbers": [
        "27",
        "2 ",
        "27",
        "13.8 ",
        "27.7 ",
        "55.3 "
      ],
      "hash": "5b540e977e4a1fc0",
      "meta": {
        "body_font_size": 10.8
      }
    },
    {
      "chunk_id": "document_p048_c00197",
      "block_type": "text",
      "page_no": 48,
      "order": 197,
      "bbox": [
        54.74,
        242.74,
        683.38,
        265.29
      ],
      "text": "32B Qwen2.5 32B 等 16.4 GiB 32.8 GiB 65.6 GiB",
      "normalized_text": "32B Qwen2.5 32B 等 16.4 GiB 32.8 GiB 65.6 GiB",
      "heading_level": 3,
      "numbers": [
        "32",
        "2.5 ",
        "32",
        "16.4 ",
        "32.8 ",
        "65.6 "
      ],
      "hash": "b07874642d9d9b13",
      "meta": {
        "body_font_size": 10.8
      }
    },
    {
      "chunk_id": "document_p048_c00198",
      "block_type": "text",
      "page_no": 48,
      "order": 198,
      "bbox": [
        54.74,
        300.29,
        683.4,
        322.84
      ],
      "text": "34B Yi-34B 等 17.4 GiB 34.8 GiB 69.7 GiB",
      "normalized_text": "34B Yi-34B 等 17.4 GiB 34.8 GiB 69.7 GiB",
      "heading_level": 3,
      "numbers": [
        "34",
        "34",
        "17.4 ",
        "34.8 ",
        "69.7 "
      ],
      "hash": "a2d8d82af619b07d",
      "meta": {
        "body_font_size": 10.8
      }
    },
    {
      "chunk_id": "document_p048_c00199",
      "block_type": "text",
      "page_no": 48,
      "order": 199,
      "bbox": [
        36.03,
        357.84,
        912.51,
        531.44
      ],
      "text": "70B Llama 3 70B 等 35.9 GiB 71.7 GiB 143.4 GiB\n計算前提:パラメータ数 × bit 幅 ÷ 8 で基本容量を算出後、実運用におけるランタイムオーバヘッド等を考慮して約 +10% を加算した概算値です。\n設計含意: 30B 級モデルの実運用には、 4bit 量子化でも約 16-18GiB の VRAM/ 統合メモリが必要です。 70B 級では 4bit でも約 36GiB を消費するため、 Apple Silicon 64GB/96GB や、 VRAM 24GB×2 枚構成などの上位ハードウ ェア構成が現実的なラインとなります。\nPage 48 | ローカル AI 技術調査レポート 2026 Source: Technical Survey Report (Calculated Values based on Weight Quantization)",
      "normalized_text": "70B Llama 3 70B 等 35.9 GiB 71.7 GiB 143.4 GiB\n計算前提:パラメータ数 × bit 幅 ÷ 8 で基本容量を算出後、実運用におけるランタイムオーバヘッド等を考慮して約 +10% を加算した概算値です。\n設計含意: 30B 級モデルの実運用には、 4bit 量子化でも約 16-18GiB の VRAM/ 統合メモリが必要です。 70B 級では 4bit でも約 36GiB を消費するため、 Apple Silicon 64GB/96GB や、 VRAM 24GB×2 枚構成などの上位ハードウ ェア構成が現実的なラインとなります。\nPage 48 | ローカル AI 技術調査レポート 2026 Source: Technical Survey Report (Calculated Values based on Weight Quantization)",
      "heading_level": 3,
      "numbers": [
        "70",
        "3 ",
        "70",
        "35.9 ",
        "71.7 ",
        "143.4 ",
        "8 ",
        "10%",
        "30",
        "4",
        "16",
        "18",
        "70",
        "4",
        "36",
        "64",
        "96",
        "24",
        "2 ",
        "48 ",
        "2026 "
      ],
      "hash": "3960d4976896a698",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p049_c00200",
      "block_type": "text",
      "page_no": 49,
      "order": 200,
      "bbox": [
        36.03,
        34.72,
        909.48,
        531.44
      ],
      "text": "KV キャッシュの支配性(完全版表 1 ) 2k 〜 32k tokens におけるメモリ消費量比較( Llama3 8B 相当)\nコンテキスト長 (Tokens)\nFP16 ( 標準 ) (16-bit)\nFP8 ( 量子化 ) (8-bit)\nINT4 ( 量子化 ) (4-bit)\n2,048 tokens 標準的な短文対話 0.25 GiB 0.12 GiB 0.06 GiB\n8,192 tokens 長文記事・文書要約 1.00 GiB 0.50 GiB 0.25 GiB\n32,768 tokens RAG / 全体像把握 4.00 GiB 2.00 GiB 1.00 GiB\n設計含意: KV キャッシュはコンテキスト長に比例して線形増加します。 32k トークンなどの長文コンテキストでは、 FP16 のままでは 4GB もの VRAM を消費し、モデル重み( 8B 4bit で約 4.5GB )と合わせると 8GB VRAM の限界を超えます。 FP8/INT4 量子化や Paged KV Cache の導入が、長文運用成立の鍵となります。 [5-7]\n※計算前提 : Llama3 8B (n_layer=32, n_head_kv=8, head_dim=128), GQA 有効\nPage 49 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [5-7]",
      "normalized_text": "KV キャッシュの支配性(完全版表 1 ) 2k 〜 32k tokens におけるメモリ消費量比較( Llama3 8B 相当)\nコンテキスト長 (Tokens)\nFP16 ( 標準 ) (16-bit)\nFP8 ( 量子化 ) (8-bit)\nINT4 ( 量子化 ) (4-bit)\n2,048 tokens 標準的な短文対話 0.25 GiB 0.12 GiB 0.06 GiB\n8,192 tokens 長文記事・文書要約 1.00 GiB 0.50 GiB 0.25 GiB\n32,768 tokens RAG / 全体像把握 4.00 GiB 2.00 GiB 1.00 GiB\n設計含意: KV キャッシュはコンテキスト長に比例して線形増加します。 32k トークンなどの長文コンテキストでは、 FP16 のままでは 4GB もの VRAM を消費し、モデル重み( 8B 4bit で約 4.5GB )と合わせると 8GB VRAM の限界を超えます。 FP8/INT4 量子化や Paged KV Cache の導入が、長文運用成立の鍵となります。 [5-7]\n※計算前提 : Llama3 8B (n_layer=32, n_head_kv=8, head_dim=128), GQA 有効\nPage 49 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [5-7]",
      "heading_level": 3,
      "numbers": [
        "1 ",
        "2",
        "32",
        "3 ",
        "8",
        "16 ",
        "16",
        "8 ",
        "8",
        "4 ",
        "4",
        "2,048 ",
        "0.25 ",
        "0.12 ",
        "0.06 ",
        "8,192 ",
        "1.00 ",
        "0.50 ",
        "0.25 ",
        "32,768 ",
        "4.00 ",
        "2.00 ",
        "1.00 ",
        "32",
        "16 ",
        "4",
        "8",
        "4",
        "4.5",
        "8",
        "8",
        "4 ",
        "5",
        "7",
        "3 ",
        "8",
        "32, ",
        "8, ",
        "128",
        "49 ",
        "2026 ",
        "5",
        "7"
      ],
      "hash": "44d041d9da890364",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 10
      }
    },
    {
      "chunk_id": "document_p050_c00201",
      "block_type": "text",
      "page_no": 50,
      "order": 201,
      "bbox": [
        51.72,
        34.72,
        909.68,
        190.13
      ],
      "text": "KV キャッシュの支配性(完全版表 2 :超長文域) コンテキスト長 131k tokens におけるメモリ消費量比較( Llama3 8B 相当)\nコンテキスト長 FP16 (Base) FP8 (Optimized) INT4 (Highly Optimized)\n131,072 tokens 128k context 16.00 GiB Critical モデル本体 (8B) と合わせると 32GB 超",
      "normalized_text": "KV キャッシュの支配性(完全版表 2 :超長文域) コンテキスト長 131k tokens におけるメモリ消費量比較( Llama3 8B 相当)\nコンテキスト長 FP16 (Base) FP8 (Optimized) INT4 (Highly Optimized)\n131,072 tokens 128k context 16.00 GiB Critical モデル本体 (8B) と合わせると 32GB 超",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "131",
        "3 ",
        "8",
        "16 ",
        "8 ",
        "4 ",
        "131,072 ",
        "128",
        "16.00 ",
        "8",
        "32"
      ],
      "hash": "5ce6ad8c4ec815bf",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p050_c00202",
      "block_type": "text",
      "page_no": 50,
      "order": 202,
      "bbox": [
        495.91,
        165.22,
        575.75,
        190.13
      ],
      "text": "8.00 GiB Heavy 50% 削減(実用域へ)",
      "normalized_text": "8.00 GiB Heavy 50% 削減(実用域へ)",
      "heading_level": 3,
      "numbers": [
        "8.00 ",
        "50%"
      ],
      "hash": "e20ced16abc43700",
      "meta": {
        "body_font_size": 10.08
      }
    },
    {
      "chunk_id": "document_p050_c00203",
      "block_type": "text",
      "page_no": 50,
      "order": 203,
      "bbox": [
        36.03,
        165.22,
        908.72,
        531.44
      ],
      "text": "4.00 GiB Manageable 75% 削減(余裕あり)\nメモリ内訳の逆転現象\n超長文( 128k 等)では、 KV キャッシュのメモリ消費量がモデル本体(重み)のメモリ消費 量を上回る現象が発生します。\n例: Llama3 8B ( 4bit 重み ≒ 4.1GiB )に対し、 131k tokens の KV ( FP16 )は 16.00GiB に達し、総 メモリの約 80% を KV が占有します。\n設計による回避策( Design Implications )\n要約・圧縮: Rolling Summary 等でコンテキストを常に一定長以下に保つ。\n分割処理:長文を一括入力せず、チャンク分割して処理( Map-Reduce 等)。\nRAG 活用:全文をコンテキストに入れず、 Embedding 検索で必要箇所のみ抽出。\nKV 量子化: vLLM 等の paged/quantized KV 機能を積極的に利用 [5,6] 。\n設計含意:長文対応の現実解\n「 128k 対応」を謳うモデルでも、ローカル環境(特に GPU VRAM 制約下)では KV キャッシュがボトルネックとなり物理的にロードできない場合があります。 KV 量子化( FP8/INT4 )は品質劣化を抑えつつメモリ を劇的に削減できるため、長文タスクにおける必須の最適化技術となります。\nPage 50 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [Calculated based on Llama3 architecture]",
      "normalized_text": "4.00 GiB Manageable 75% 削減(余裕あり)\nメモリ内訳の逆転現象\n超長文( 128k 等)では、 KV キャッシュのメモリ消費量がモデル本体(重み)のメモリ消費 量を上回る現象が発生します。\n例: Llama3 8B ( 4bit 重み ≒ 4.1GiB )に対し、 131k tokens の KV ( FP16 )は 16.00GiB に達し、総 メモリの約 80% を KV が占有します。\n設計による回避策( Design Implications )\n要約・圧縮: Rolling Summary 等でコンテキストを常に一定長以下に保つ。\n分割処理:長文を一括入力せず、チャンク分割して処理( Map-Reduce 等)。\nRAG 活用:全文をコンテキストに入れず、 Embedding 検索で必要箇所のみ抽出。\nKV 量子化: vLLM 等の paged/quantized KV 機能を積極的に利用 [5,6] 。\n設計含意:長文対応の現実解\n「 128k 対応」を謳うモデルでも、ローカル環境(特に GPU VRAM 制約下)では KV キャッシュがボトルネックとなり物理的にロードできない場合があります。 KV 量子化( FP8/INT4 )は品質劣化を抑えつつメモリ を劇的に削減できるため、長文タスクにおける必須の最適化技術となります。\nPage 50 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [Calculated based on Llama3 architecture]",
      "heading_level": 3,
      "numbers": [
        "4.00 ",
        "75%",
        "128",
        "3 ",
        "8",
        "4",
        "4.1",
        "131",
        "16 ",
        "16.00",
        "80%",
        "5,6",
        "128",
        "8",
        "4 ",
        "50 ",
        "2026 ",
        "3 "
      ],
      "hash": "96fc84ce429985b0",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 10
      }
    },
    {
      "chunk_id": "document_p051_c00204",
      "block_type": "text",
      "page_no": 51,
      "order": 204,
      "bbox": [
        30.04,
        19.66,
        886.39,
        40.59
      ],
      "text": "メモリ最適化技術 paged KV / quantized KV / KV reuse / prompt cache",
      "normalized_text": "メモリ最適化技術 paged KV / quantized KV / KV reuse / prompt cache",
      "heading_level": 0,
      "numbers": [],
      "hash": "dff05663fe6c00ac",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p051_c00205",
      "block_type": "text",
      "page_no": 51,
      "order": 205,
      "bbox": [
        101.27,
        123.37,
        264.26,
        140.97
      ],
      "text": "KV 構造と管理の最適化",
      "normalized_text": "KV 構造と管理の最適化",
      "heading_level": 3,
      "numbers": [],
      "hash": "197aacec4d737f97",
      "meta": {
        "body_font_size": 12.27
      }
    },
    {
      "chunk_id": "document_p051_c00206",
      "block_type": "text",
      "page_no": 51,
      "order": 206,
      "bbox": [
        74.26,
        185.88,
        447.24,
        247.92
      ],
      "text": "Paged KV Cache (vLLM) [5]\nメモリを固定サイズのページ単位で管理し、断片化を抑制する技術。 OS の仮想メモリと同様の仕組みで、 GPU メモリの利用効率を劇的に向 上させ、スループットを高めます。",
      "normalized_text": "Paged KV Cache (vLLM) [5]\nメモリを固定サイズのページ単位で管理し、断片化を抑制する技術。 OS の仮想メモリと同様の仕組みで、 GPU メモリの利用効率を劇的に向 上させ、スループットを高めます。",
      "heading_level": 3,
      "numbers": [
        "5"
      ],
      "hash": "2010128600a2f7da",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p051_c00207",
      "block_type": "text",
      "page_no": 51,
      "order": 207,
      "bbox": [
        74.26,
        123.37,
        913.16,
        531.4
      ],
      "text": "Quantized KV Cache [6]\nKV キャッシュを標準の FP16 から FP8 や INT4 へ量子化。精度劣化を最小 限に抑えつつメモリフットプリントを 50 〜 75% 削減し、より長いコン テキストや大きなバッチサイズでの推論を可能にします。\n再利用とレイテンシ短縮\nKV Cache Reuse (TensorRT-LLM) [7]\n共通のプレフィックス(システムプロンプトやマルチターン対話の履 歴)に対する KV キャッシュを計算・保存。次回推論時に再利用するこ とで、計算コストとメモリ転送時間を節約します。\nPrompt Cache (llama.cpp) [88,90]\n固定の長いシステムプロンプトやドキュメントをキャッシュ( -- cache-prompt )。アプリ再起動後やセッション間での初回ロード時 TTFT ( Time To First Token )を大幅に短縮します。\nKey Takeaways: KV キャッシュ最適化とプロンプト再利用を組み合わせることで 、メモリ制約下での「長文コンテキスト」と「高速な応答性」を両立します。\nPage 51 | ローカル AI 技術調査レポート",
      "normalized_text": "Quantized KV Cache [6]\nKV キャッシュを標準の FP16 から FP8 や INT4 へ量子化。精度劣化を最小 限に抑えつつメモリフットプリントを 50 〜 75% 削減し、より長いコン テキストや大きなバッチサイズでの推論を可能にします。\n再利用とレイテンシ短縮\nKV Cache Reuse (TensorRT-LLM) [7]\n共通のプレフィックス(システムプロンプトやマルチターン対話の履 歴)に対する KV キャッシュを計算・保存。次回推論時に再利用するこ とで、計算コストとメモリ転送時間を節約します。\nPrompt Cache (llama.cpp) [88,90]\n固定の長いシステムプロンプトやドキュメントをキャッシュ( -- cache-prompt )。アプリ再起動後やセッション間での初回ロード時 TTFT ( Time To First Token )を大幅に短縮します。\nKey Takeaways: KV キャッシュ最適化とプロンプト再利用を組み合わせることで 、メモリ制約下での「長文コンテキスト」と「高速な応答性」を両立します。\nPage 51 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "6",
        "16 ",
        "8 ",
        "4 ",
        "50 ",
        "75%",
        "7",
        "88,90",
        "51 "
      ],
      "hash": "1e429973e9cee91a",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 8
      }
    },
    {
      "chunk_id": "document_p052_c00208",
      "block_type": "text",
      "page_no": 52,
      "order": 208,
      "bbox": [
        58.57,
        25.41,
        917.02,
        151.01
      ],
      "text": "Tier 定義と現実ライン(概要) ハードウェア別の実用ライン定義\n結論: Tier は「重み( 4bit )+ KV ( 4k 〜 8k )+オーバーヘッド」を前提に定義。 理論計算値 をベースとしつつ、最終判断は 実機再現ベンチ で行う設計です。",
      "normalized_text": "Tier 定義と現実ライン(概要) ハードウェア別の実用ライン定義\n結論: Tier は「重み( 4bit )+ KV ( 4k 〜 8k )+オーバーヘッド」を前提に定義。 理論計算値 をベースとしつつ、最終判断は 実機再現ベンチ で行う設計です。",
      "heading_level": 3,
      "numbers": [
        "4",
        "4",
        "8"
      ],
      "hash": "7db60e1b760df289",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p052_c00209",
      "block_type": "text",
      "page_no": 52,
      "order": 209,
      "bbox": [
        83.27,
        218.65,
        265.06,
        466.42
      ],
      "text": "Apple Silicon (UMA)\n16GB (A-16) 3B 〜 7/8B (4bit)\n24-32GB (A-24/32) 7B 〜 14B (4bit)\n64GB (A-64) 14B 〜 32B (4bit)\n96-128GB (A-96+) 32B 〜 70B (4bit)",
      "normalized_text": "Apple Silicon (UMA)\n16GB (A-16) 3B 〜 7/8B (4bit)\n24-32GB (A-24/32) 7B 〜 14B (4bit)\n64GB (A-64) 14B 〜 32B (4bit)\n96-128GB (A-96+) 32B 〜 70B (4bit)",
      "heading_level": 3,
      "numbers": [
        "16",
        "16",
        "3",
        "7",
        "8",
        "4",
        "24",
        "32",
        "24",
        "32",
        "7",
        "14",
        "4",
        "64",
        "64",
        "14",
        "32",
        "4",
        "96",
        "128",
        "96",
        "32",
        "70",
        "4"
      ],
      "hash": "e923cacf8ae1f873",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p052_c00210",
      "block_type": "text",
      "page_no": 52,
      "order": 210,
      "bbox": [
        385.47,
        218.65,
        574.76,
        466.42
      ],
      "text": "Windows GPU (VRAM)\nVRAM 8GB (G-8) 7B (4bit) ※短文中心\nVRAM 12GB (G-12) 7B 〜 14B (4bit)\nVRAM 16GB (G-16) 14B 〜 27B (4bit)\nVRAM 24GB (G-24) 27B 〜 34B (4bit)",
      "normalized_text": "Windows GPU (VRAM)\nVRAM 8GB (G-8) 7B (4bit) ※短文中心\nVRAM 12GB (G-12) 7B 〜 14B (4bit)\nVRAM 16GB (G-16) 14B 〜 27B (4bit)\nVRAM 24GB (G-24) 27B 〜 34B (4bit)",
      "heading_level": 3,
      "numbers": [
        "8",
        "8",
        "7",
        "4",
        "12",
        "12",
        "7",
        "14",
        "4",
        "16",
        "16",
        "14",
        "27",
        "4",
        "24",
        "24",
        "27",
        "34",
        "4"
      ],
      "hash": "06ff5ea11da1a7e3",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p052_c00211",
      "block_type": "text",
      "page_no": 52,
      "order": 211,
      "bbox": [
        36.03,
        218.65,
        922.54,
        542.36
      ],
      "text": "Windows CPU-only\nRAM 32GB (W-CPU1) 3B 〜 7B (4bit)\n7B 級が「実用的に動く」境界線。 AVX 命令等の最適化が必須。 速度は GPU 比で大幅劣後。\n実務上の注意点\nLocal AI Technical Survey Report 2026 52 / 80",
      "normalized_text": "Windows CPU-only\nRAM 32GB (W-CPU1) 3B 〜 7B (4bit)\n7B 級が「実用的に動く」境界線。 AVX 命令等の最適化が必須。 速度は GPU 比で大幅劣後。\n実務上の注意点\nLocal AI Technical Survey Report 2026 52 / 80",
      "heading_level": 3,
      "numbers": [
        "32",
        "1",
        "3",
        "7",
        "4",
        "7",
        "2026 ",
        "52 ",
        "80"
      ],
      "hash": "35fd44a553b07624",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p053_c00212",
      "block_type": "text",
      "page_no": 53,
      "order": 212,
      "bbox": [
        48.76,
        34.72,
        895.0,
        179.5
      ],
      "text": "Apple Silicon Tier (完全版表 1 ) 実用ライン定義: A-16 / A-24/32 (Unified Memory Architecture)\nTier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針",
      "normalized_text": "Apple Silicon Tier (完全版表 1 ) 実用ライン定義: A-16 / A-24/32 (Unified Memory Architecture)\nTier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針",
      "heading_level": 3,
      "numbers": [
        "1 ",
        "16 ",
        "24",
        "32 "
      ],
      "hash": "0b75c85b54eaf37e",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p053_c00213",
      "block_type": "text",
      "page_no": 53,
      "order": 213,
      "bbox": [
        48.76,
        252.7,
        910.04,
        315.39
      ],
      "text": "A-16 Entry\nApple Silicon 16GB (M1/M2/M3/M4 等 ) [1,11,92]\n3B 〜 7/8B (4bit 量子化 )\n条件付 “ 短文中心 ” なら可\n軽量構成 ASR/TTS は小さめなら可\n文脈長を抑え、 RAG は Embedding 先計算 算。 Apple 公式の Unified Memory Architecture (UMA) 特性を理解し、システムメモリ との競合を避ける設計が必要。 [10,132]",
      "normalized_text": "A-16 Entry\nApple Silicon 16GB (M1/M2/M3/M4 等 ) [1,11,92]\n3B 〜 7/8B (4bit 量子化 )\n条件付 “ 短文中心 ” なら可\n軽量構成 ASR/TTS は小さめなら可\n文脈長を抑え、 RAG は Embedding 先計算 算。 Apple 公式の Unified Memory Architecture (UMA) 特性を理解し、システムメモリ との競合を避ける設計が必要。 [10,132]",
      "heading_level": 2,
      "numbers": [
        "16 ",
        "16",
        "1",
        "2",
        "3",
        "4 ",
        "1,11,92",
        "3",
        "7",
        "8",
        "4",
        "10,132"
      ],
      "hash": "c99ec197bcaf7f8f",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p053_c00214",
      "block_type": "text",
      "page_no": 53,
      "order": 214,
      "bbox": [
        36.03,
        380.91,
        912.51,
        531.44
      ],
      "text": "A-24/32 Standard\nApple Silicon 24 〜 32GB (M3/M4 Pro/Max 等 ) [93]\n7B 〜 14B (4bit 量子化 )\n可 (業務最小ライン)\n成立 多くのユースケースで 実用可\n14B 級を軸に、 VLM は 2B 〜小型を併用。 。 「 RAG 含め業務用途の最小実用ライン」 になりやすい。 M4 世代でも統合メモリ 前提で構成されるため、 GPU メモリとし てフル活用可能。 [11] UMA (Unified Memory Architecture) の特性: CPU と GPU が同一のメモリプールを共有するため、データ転送のオーバーヘッドが極小化されます。 Metal Performance Shaders (MPS) バックエンドを使用する MLX や PyTorch の実装では、この統合メモリを効率的に利用できますが、画面表示や OS 自身のメモリ消費(数 GB )を差し引いた残量が実効 VRAM となる点に注意が必要です。\n[1,132]\nPage 53 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [1,10,11,92,93,132]",
      "normalized_text": "A-24/32 Standard\nApple Silicon 24 〜 32GB (M3/M4 Pro/Max 等 ) [93]\n7B 〜 14B (4bit 量子化 )\n可 (業務最小ライン)\n成立 多くのユースケースで 実用可\n14B 級を軸に、 VLM は 2B 〜小型を併用。 。 「 RAG 含め業務用途の最小実用ライン」 になりやすい。 M4 世代でも統合メモリ 前提で構成されるため、 GPU メモリとし てフル活用可能。 [11] UMA (Unified Memory Architecture) の特性: CPU と GPU が同一のメモリプールを共有するため、データ転送のオーバーヘッドが極小化されます。 Metal Performance Shaders (MPS) バックエンドを使用する MLX や PyTorch の実装では、この統合メモリを効率的に利用できますが、画面表示や OS 自身のメモリ消費(数 GB )を差し引いた残量が実効 VRAM となる点に注意が必要です。\n[1,132]\nPage 53 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [1,10,11,92,93,132]",
      "heading_level": 2,
      "numbers": [
        "24",
        "32 ",
        "24 ",
        "32",
        "3",
        "4 ",
        "93",
        "7",
        "14",
        "4",
        "14",
        "2",
        "4 ",
        "11",
        "1,132",
        "53 ",
        "2026 ",
        "1,10,11,92,93,132"
      ],
      "hash": "41b892348b8539c8",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p054_c00215",
      "block_type": "text",
      "page_no": 54,
      "order": 215,
      "bbox": [
        48.76,
        34.72,
        917.59,
        184.68
      ],
      "text": "Apple Silicon Tier (完全版表 2 ) A-64 / A-96/128 :高品質ローカルの到達点と運用指針\nTier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針",
      "normalized_text": "Apple Silicon Tier (完全版表 2 ) A-64 / A-96/128 :高品質ローカルの到達点と運用指針\nTier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "64 ",
        "96",
        "128 "
      ],
      "hash": "21abf0f7e90f4a62",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p054_c00216",
      "block_type": "text",
      "page_no": 54,
      "order": 216,
      "bbox": [
        48.76,
        259.61,
        913.0,
        351.91
      ],
      "text": "A-64 High-End\nApple 64GB (M1 Max 等 ) [1, 92]\n14B 〜 32B (4bit)\n可 余裕あり LLM+VLM 等\n長文会議は要約で圧縮し、 KV 最適化を活用。 M1 Max 等で 64GB 構成が公式に提示されており、 32B 級モデルの実用的なスイートスポット。 32B モデル( 4bit )で約 16-18GB 消費、残りで VLM/ASR/OS を十分に賄える。 長文コンテキスト( RAG/ 会議録)では KV キャッシ ュが数 GB 単位で消費されるため、 paged KV 等の活用 が効果的。",
      "normalized_text": "A-64 High-End\nApple 64GB (M1 Max 等 ) [1, 92]\n14B 〜 32B (4bit)\n可 余裕あり LLM+VLM 等\n長文会議は要約で圧縮し、 KV 最適化を活用。 M1 Max 等で 64GB 構成が公式に提示されており、 32B 級モデルの実用的なスイートスポット。 32B モデル( 4bit )で約 16-18GB 消費、残りで VLM/ASR/OS を十分に賄える。 長文コンテキスト( RAG/ 会議録)では KV キャッシ ュが数 GB 単位で消費されるため、 paged KV 等の活用 が効果的。",
      "heading_level": 3,
      "numbers": [
        "64 ",
        "64",
        "1 ",
        "1, ",
        "92",
        "14",
        "32",
        "4",
        "1 ",
        "64",
        "32",
        "32",
        "4",
        "16",
        "18"
      ],
      "hash": "0afe6a8ef454e339",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p054_c00217",
      "block_type": "text",
      "page_no": 54,
      "order": 217,
      "bbox": [
        36.03,
        399.09,
        913.35,
        531.44
      ],
      "text": "A-96/128 Ultra\nApple 96 〜 128GB (M2/M3/M4 Max/Ultra) [93, 94]\n32B 〜 70B (4bit)\n可 ( 重い )\n構成次第 高品質 RAG 等\n“ 高品質ローカル ” の現実ライン。 70B 級( 4bit で約 40GB )をロードしても、まだ 50GB 以上の余裕がある圧倒的なメモリ空間。 M3/M4 で最大 128GB 構成が公式に提示され、ローカ ルで最高品質の推論が可能。 大規模モデルの常駐運用が可能だが、推論速度( tok/s )はメモリ帯域幅に依存するため、実用速度が 出るかの確認が必要。 マルチモーダル( VLM 72B 等)や複雑な Agent ワーク フローも視野に入る。\n到達点の意味: A-64 以上は「妥協のないローカル AI 」を実現する領域です。特に Unified Memory Architecture (UMA) の恩恵により、同等 VRAM を持つディスクリート GPU 構成よりも低コストかつ省電力に大規模モ デルを扱えます。ただし、推論速度(スループット)は専用 GPU に劣る場合があるため、レイテンシ要件が厳しい場合は量子化レベルの調整やプロンプトキャッシュの活用が重要になります。\nPage 54 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [1, 92, 93, 94]",
      "normalized_text": "A-96/128 Ultra\nApple 96 〜 128GB (M2/M3/M4 Max/Ultra) [93, 94]\n32B 〜 70B (4bit)\n可 ( 重い )\n構成次第 高品質 RAG 等\n“ 高品質ローカル ” の現実ライン。 70B 級( 4bit で約 40GB )をロードしても、まだ 50GB 以上の余裕がある圧倒的なメモリ空間。 M3/M4 で最大 128GB 構成が公式に提示され、ローカ ルで最高品質の推論が可能。 大規模モデルの常駐運用が可能だが、推論速度( tok/s )はメモリ帯域幅に依存するため、実用速度が 出るかの確認が必要。 マルチモーダル( VLM 72B 等)や複雑な Agent ワーク フローも視野に入る。\n到達点の意味: A-64 以上は「妥協のないローカル AI 」を実現する領域です。特に Unified Memory Architecture (UMA) の恩恵により、同等 VRAM を持つディスクリート GPU 構成よりも低コストかつ省電力に大規模モ デルを扱えます。ただし、推論速度(スループット)は専用 GPU に劣る場合があるため、レイテンシ要件が厳しい場合は量子化レベルの調整やプロンプトキャッシュの活用が重要になります。\nPage 54 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [1, 92, 93, 94]",
      "heading_level": 3,
      "numbers": [
        "96",
        "128 ",
        "96 ",
        "128",
        "2",
        "3",
        "4 ",
        "93, ",
        "94",
        "32",
        "70",
        "4",
        "70",
        "4",
        "40",
        "50",
        "3",
        "4 ",
        "128",
        "72",
        "64 ",
        "54 ",
        "2026 ",
        "1, ",
        "92, ",
        "93, ",
        "94"
      ],
      "hash": "0990130492832baf",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p055_c00218",
      "block_type": "text",
      "page_no": 55,
      "order": 218,
      "bbox": [
        48.76,
        34.72,
        912.14,
        284.99
      ],
      "text": "Windows Tier (完全版表 1 ) W-CPU1 / W-CPU2 / G-8 / G-12 の現実ライン\nTier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針\nW-CPU1 CPU-only 8C/16T 級 + 32GB RAM\n3B 〜 7B (4bit) 条件付き 負荷高\n厳しい オンデマンド推奨\nCPU 最適化命令( AVX-512/VNNI/AMX 等)を )を活用 [13] 。常駐は “ 小型+短文 ” に限定し 定し、重い処理は避ける。\nW-CPU2 CPU-only 16C/32T 級 + 64GB RAM\n7B 〜 14B (4bit) 可 ただし速度要検証\n設計次第 夜間バッチ( Embedding/OCR )で負荷平 化。メモリ余裕はあるが計算速度が律速。",
      "normalized_text": "Windows Tier (完全版表 1 ) W-CPU1 / W-CPU2 / G-8 / G-12 の現実ライン\nTier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針\nW-CPU1 CPU-only 8C/16T 級 + 32GB RAM\n3B 〜 7B (4bit) 条件付き 負荷高\n厳しい オンデマンド推奨\nCPU 最適化命令( AVX-512/VNNI/AMX 等)を )を活用 [13] 。常駐は “ 小型+短文 ” に限定し 定し、重い処理は避ける。\nW-CPU2 CPU-only 16C/32T 級 + 64GB RAM\n7B 〜 14B (4bit) 可 ただし速度要検証\n設計次第 夜間バッチ( Embedding/OCR )で負荷平 化。メモリ余裕はあるが計算速度が律速。",
      "heading_level": 3,
      "numbers": [
        "1 ",
        "1 ",
        "2 ",
        "8 ",
        "12 ",
        "1 ",
        "8",
        "16",
        "32",
        "3",
        "7",
        "4",
        "512",
        "13",
        "2 ",
        "16",
        "32",
        "64",
        "7",
        "14",
        "4"
      ],
      "hash": "9365093935de0d01",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p055_c00219",
      "block_type": "text",
      "page_no": 55,
      "order": 219,
      "bbox": [
        48.76,
        334.06,
        905.87,
        356.82
      ],
      "text": "G-8 NVIDIA GPU VRAM 8GB\n7B 中心 (4bit) 可 短文に限る\n軽量なら可 画像生成 SDXL 等は設定依存で厳しいため ベンチ。 7B 級 LLM 単体なら快適に動作。",
      "normalized_text": "G-8 NVIDIA GPU VRAM 8GB\n7B 中心 (4bit) 可 短文に限る\n軽量なら可 画像生成 SDXL 等は設定依存で厳しいため ベンチ。 7B 級 LLM 単体なら快適に動作。",
      "heading_level": 3,
      "numbers": [
        "8 ",
        "8",
        "7",
        "4",
        "7"
      ],
      "hash": "db9cf388ec28b676",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p055_c00220",
      "block_type": "text",
      "page_no": 55,
      "order": 220,
      "bbox": [
        36.03,
        405.89,
        914.9,
        531.44
      ],
      "text": "G-12 NVIDIA GPU VRAM 12GB\n7B 〜 14B (4bit) 可 現実化 14B 級が視野に入る。 VLM 2B 〜 7B 級や GPU 版を同時に載せやすく、実用性が高い 。\n最適化のポイント: Windows の CPU 推論は Intel 拡張( IPEX )や OpenVINO 等の最適化が効く場合があります。 GPU ( G-8/12 )では、 VRAM 不足時にシステム RAM へ溢れると劇的に遅くなるため、タスクマネージャー 等で VRAM 使用率を厳密に監視してください。\n[13,135,136]\nPage 55 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [13, 135, 136]",
      "normalized_text": "G-12 NVIDIA GPU VRAM 12GB\n7B 〜 14B (4bit) 可 現実化 14B 級が視野に入る。 VLM 2B 〜 7B 級や GPU 版を同時に載せやすく、実用性が高い 。\n最適化のポイント: Windows の CPU 推論は Intel 拡張( IPEX )や OpenVINO 等の最適化が効く場合があります。 GPU ( G-8/12 )では、 VRAM 不足時にシステム RAM へ溢れると劇的に遅くなるため、タスクマネージャー 等で VRAM 使用率を厳密に監視してください。\n[13,135,136]\nPage 55 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report [13, 135, 136]",
      "heading_level": 3,
      "numbers": [
        "12 ",
        "12",
        "7",
        "14",
        "4",
        "14",
        "2",
        "7",
        "8",
        "12 ",
        "13,135,136",
        "55 ",
        "2026 ",
        "13, ",
        "135, ",
        "136"
      ],
      "hash": "3f1e6f5b8ee7b3e3",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p056_c00221",
      "block_type": "text",
      "page_no": 56,
      "order": 221,
      "bbox": [
        36.03,
        34.72,
        909.34,
        531.44
      ],
      "text": "Windows Tier (完全版表 2 ) 高品質・実務用: G-16 (16GB) / G-24 (24GB) 定義\nTier 想定ハード 現実的な LLM 規模 常駐可否 / 同時実行 設計指針\nG-16 VRAM 16GB GeForce 4080 Laptop RTX 4070 Ti Super 等\n14B 〜 27B (4bit 量子化 )\n常駐 : 可 同時 : 高品質寄り成立 ASR/TTS/VLM との マルチモデル運用が可能\n高品質なマルチモデル運用ライン 長文コンテキストは KV メモリを圧迫するため、 FP8 KV キャッシュ等の採用余地があります。 20B 級モデルの運用も視野に入ります。\nG-24 VRAM 24GB GeForce RTX 3090/4090 RTX 6000 Ada 等\n27B 〜 34B (4bit 量子化 )\n常駐 : 可 同時 : 余裕あり 重い画像生成や大型 VLM も 実務的に動作可能\n実務における高品質ローカルの基準点 30B 級 LLM に加え、 SDXL/FLUX などの重い画像生 成や大型 VLM を同時に回せるポテンシャルがあ ります。 70B 級は単体ならギリギリ動作可能で す(要 4bit/EXL2 等)。\nKV キャッシュに関する重要な注意点:\nVRAM に余裕があっても、長文コンテキスト( 32k 〜 128k )を扱うと KV キャッシュが VRAM を大量に消費し、 OOM ( Out Of Memory )の原因となります。 G-16/G-24 であっても、長文を扱う際は「 KV 量子化( FP8/INT4 )」や「コンテキスト設計(要約・分割)」によるメモリ管理が必須です。\nPage 56 | ローカル AI 技術調査レポート 2026 Based on Weight Memory Theory + Reproduction Benchmarks",
      "normalized_text": "Windows Tier (完全版表 2 ) 高品質・実務用: G-16 (16GB) / G-24 (24GB) 定義\nTier 想定ハード 現実的な LLM 規模 常駐可否 / 同時実行 設計指針\nG-16 VRAM 16GB GeForce 4080 Laptop RTX 4070 Ti Super 等\n14B 〜 27B (4bit 量子化 )\n常駐 : 可 同時 : 高品質寄り成立 ASR/TTS/VLM との マルチモデル運用が可能\n高品質なマルチモデル運用ライン 長文コンテキストは KV メモリを圧迫するため、 FP8 KV キャッシュ等の採用余地があります。 20B 級モデルの運用も視野に入ります。\nG-24 VRAM 24GB GeForce RTX 3090/4090 RTX 6000 Ada 等\n27B 〜 34B (4bit 量子化 )\n常駐 : 可 同時 : 余裕あり 重い画像生成や大型 VLM も 実務的に動作可能\n実務における高品質ローカルの基準点 30B 級 LLM に加え、 SDXL/FLUX などの重い画像生 成や大型 VLM を同時に回せるポテンシャルがあ ります。 70B 級は単体ならギリギリ動作可能で す(要 4bit/EXL2 等)。\nKV キャッシュに関する重要な注意点:\nVRAM に余裕があっても、長文コンテキスト( 32k 〜 128k )を扱うと KV キャッシュが VRAM を大量に消費し、 OOM ( Out Of Memory )の原因となります。 G-16/G-24 であっても、長文を扱う際は「 KV 量子化( FP8/INT4 )」や「コンテキスト設計(要約・分割)」によるメモリ管理が必須です。\nPage 56 | ローカル AI 技術調査レポート 2026 Based on Weight Memory Theory + Reproduction Benchmarks",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "16 ",
        "16",
        "24 ",
        "24",
        "16 ",
        "16",
        "4080 ",
        "4070 ",
        "14",
        "27",
        "4",
        "8 ",
        "20",
        "24 ",
        "24",
        "3090",
        "4090 ",
        "6000 ",
        "27",
        "34",
        "4",
        "30",
        "70",
        "4",
        "2 ",
        "32",
        "128",
        "16",
        "24 ",
        "8",
        "4 ",
        "56 ",
        "2026 "
      ],
      "hash": "124c49c324d19600",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p057_c00222",
      "block_type": "text",
      "page_no": 57,
      "order": 222,
      "bbox": [
        58.57,
        24.28,
        917.71,
        162.41
      ],
      "text": "補足: AMD/Intel GPU の現実ライン 公式最適化とエコシステムの現状\n結論: AMD/Intel 環境でも公式最適化による選択肢が存在しますが、運用は実装・ドライバ依存となり、 個別検証が必要です。",
      "normalized_text": "補足: AMD/Intel GPU の現実ライン 公式最適化とエコシステムの現状\n結論: AMD/Intel 環境でも公式最適化による選択肢が存在しますが、運用は実装・ドライバ依存となり、 個別検証が必要です。",
      "heading_level": 3,
      "numbers": [],
      "hash": "7d626cea0a1ecc1a",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p057_c00223",
      "block_type": "text",
      "page_no": 57,
      "order": 223,
      "bbox": [
        63.76,
        242.87,
        450.26,
        524.2
      ],
      "text": "AMD Radeon GPU (ROCm)\nROCm HIP SDK ZLUDA (Deprecated)\nROCm の Windows サポートが進展しており、一部の WSL2 環境やネイティ ブ Windows での動作が可能になりつつあります。 [97]\nllama.cpp の HIP BLAS バックエンドや、 MLC LLM などが AMD GPU をサポー ト。 VRAM 容量あたりのコストパフォーマンスに優れるケースがありま す。\nNVIDIA CUDA エコシステムと比較すると、ライブラリの対応状況やトラ ブルシューティング情報が限定的です。",
      "normalized_text": "AMD Radeon GPU (ROCm)\nROCm HIP SDK ZLUDA (Deprecated)\nROCm の Windows サポートが進展しており、一部の WSL2 環境やネイティ ブ Windows での動作が可能になりつつあります。 [97]\nllama.cpp の HIP BLAS バックエンドや、 MLC LLM などが AMD GPU をサポー ト。 VRAM 容量あたりのコストパフォーマンスに優れるケースがありま す。\nNVIDIA CUDA エコシステムと比較すると、ライブラリの対応状況やトラ ブルシューティング情報が限定的です。",
      "heading_level": 2,
      "numbers": [
        "2 ",
        "97"
      ],
      "hash": "03a1750b0d0c9052",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p057_c00224",
      "block_type": "text",
      "page_no": 57,
      "order": 224,
      "bbox": [
        525.25,
        242.87,
        912.75,
        504.94
      ],
      "text": "Intel Arc / iGPU (OpenVINO/SYCL)\nOpenVINO IPEX SYCL / oneAPI\nPyTorch 拡張として XPU ( Intel GPU )サポートを提供。 Arc GPU での推論 加速が可能。 [13]\nllama.cpp の SYCL バックエンドや OpenVINO 最適化により、 iGPU ( Core Ultra 等)を含めた幅広いハードウェアで動作。 [135][136]\nドライババージョンへの依存性が高く、セットアップ手順が頻繁に更新 される傾向があります。",
      "normalized_text": "Intel Arc / iGPU (OpenVINO/SYCL)\nOpenVINO IPEX SYCL / oneAPI\nPyTorch 拡張として XPU ( Intel GPU )サポートを提供。 Arc GPU での推論 加速が可能。 [13]\nllama.cpp の SYCL バックエンドや OpenVINO 最適化により、 iGPU ( Core Ultra 等)を含めた幅広いハードウェアで動作。 [135][136]\nドライババージョンへの依存性が高く、セットアップ手順が頻繁に更新 される傾向があります。",
      "heading_level": 3,
      "numbers": [
        "13",
        "135",
        "136"
      ],
      "hash": "7e8a35cb64236a65",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p058_c00225",
      "block_type": "text",
      "page_no": 58,
      "order": 225,
      "bbox": [
        30.04,
        19.66,
        920.34,
        531.4
      ],
      "text": "常駐運用 vs オンデマンド運用 TTFT (初速)、安定性、リソース効率のトレードオフ\n基本運用モデルの比較\n常駐( Always-on ) 推奨 : チャット Bot\nモデルをメモリに保持し、 Prompt Cache を維持。\n利点: TTFT が最短、応答が安定。\n欠点:メモリを常時占有(他アプリと競合)。\nオンデマンド( On-demand ) 推奨 : 翻訳 / 要約\nリクエスト時のみロードし、完了後に解放。\n利点:アイドル時のメモリ節約、複数モデル併用。\n欠点:毎回ロード時間が発生( TTFT 増大)。\n負荷分散と決定基準\n夜間バッチ( Nightly Batch )\n重い処理をオフピークに移動し、前計算する。\n用途: RAG の Embedding 生成、 OCR 、議事録再要約。\n効果:日中のリソースを LLM の即時応答に集中。\n運用方針の決定基準\nTier (ハード制約)とユースケース要件で決定。\np95 要件:即応が必要なら「常駐」。\n同時実行数:リソース不足なら「オンデマンド」か「バッチ」。\nKey Takeaways: 常駐は「即応性」、オンデマンドは「リソース効率」。重い 処理( OCR/Embedding )は夜間バッチへ逃がすのが鉄則です。\nPage 58 | ローカル AI 技術調査レポート",
      "normalized_text": "常駐運用 vs オンデマンド運用 TTFT (初速)、安定性、リソース効率のトレードオフ\n基本運用モデルの比較\n常駐( Always-on ) 推奨 : チャット Bot\nモデルをメモリに保持し、 Prompt Cache を維持。\n利点: TTFT が最短、応答が安定。\n欠点:メモリを常時占有(他アプリと競合)。\nオンデマンド( On-demand ) 推奨 : 翻訳 / 要約\nリクエスト時のみロードし、完了後に解放。\n利点:アイドル時のメモリ節約、複数モデル併用。\n欠点:毎回ロード時間が発生( TTFT 増大)。\n負荷分散と決定基準\n夜間バッチ( Nightly Batch )\n重い処理をオフピークに移動し、前計算する。\n用途: RAG の Embedding 生成、 OCR 、議事録再要約。\n効果:日中のリソースを LLM の即時応答に集中。\n運用方針の決定基準\nTier (ハード制約)とユースケース要件で決定。\np95 要件:即応が必要なら「常駐」。\n同時実行数:リソース不足なら「オンデマンド」か「バッチ」。\nKey Takeaways: 常駐は「即応性」、オンデマンドは「リソース効率」。重い 処理( OCR/Embedding )は夜間バッチへ逃がすのが鉄則です。\nPage 58 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "95 ",
        "58 "
      ],
      "hash": "4d97f931ca3970d3",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 20
      }
    },
    {
      "chunk_id": "document_p059_c00226",
      "block_type": "text",
      "page_no": 59,
      "order": 226,
      "bbox": [
        58.57,
        7.23,
        927.48,
        212.68
      ],
      "text": "ユースケース別推奨スタック:音声メモ → 整文化 → タスク抽出\n用途に応じて「低コスト/高品質/ハード制約」の 3 構 成を選択\n結論: ASR 精度と LLM 推論能力のバランスで構成を決定します。 ※誤転記防止には VAD (無音除去)が必須。タスク抽出には JSON スキーマ固定 が有効です。\n低コスト構成",
      "normalized_text": "ユースケース別推奨スタック:音声メモ → 整文化 → タスク抽出\n用途に応じて「低コスト/高品質/ハード制約」の 3 構 成を選択\n結論: ASR 精度と LLM 推論能力のバランスで構成を決定します。 ※誤転記防止には VAD (無音除去)が必須。タスク抽出には JSON スキーマ固定 が有効です。\n低コスト構成",
      "heading_level": 3,
      "numbers": [
        "3 "
      ],
      "hash": "f7eb55257ab970ff",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p059_c00227",
      "block_type": "text",
      "page_no": 59,
      "order": 227,
      "bbox": [
        60.18,
        247.84,
        300.77,
        299.5
      ],
      "text": "Target HW\nA-16 / W-CPU1 / G-8 想定 (Apple 16GB / CPU / VRAM\n8GB)",
      "normalized_text": "Target HW\nA-16 / W-CPU1 / G-8 想定 (Apple 16GB / CPU / VRAM\n8GB)",
      "heading_level": 3,
      "numbers": [
        "16 ",
        "1 ",
        "8 ",
        "16",
        "8"
      ],
      "hash": "2643c400dfadfbc4",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p059_c00228",
      "block_type": "text",
      "page_no": 59,
      "order": 228,
      "bbox": [
        68.3,
        323.34,
        289.31,
        444.14
      ],
      "text": "VAD & ASR\nSilero VAD ( 無音除去 ) faster-whisper (CPU/INT8)\nLLM 整形・抽出\n7B 級 (Qwen2.5-7B 等 ) 4bit (GGUF 系 )",
      "normalized_text": "VAD & ASR\nSilero VAD ( 無音除去 ) faster-whisper (CPU/INT8)\nLLM 整形・抽出\n7B 級 (Qwen2.5-7B 等 ) 4bit (GGUF 系 )",
      "heading_level": 3,
      "numbers": [
        "8",
        "7",
        "2.5",
        "7",
        "4"
      ],
      "hash": "a08c7a1addb2984a",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p059_c00229",
      "block_type": "text",
      "page_no": 59,
      "order": 229,
      "bbox": [
        55.49,
        197.53,
        533.46,
        519.64
      ],
      "text": "Output Interface\nJSON (タスク配列) Ollama/LM Studio (OpenAI 互換 )\n高品質構成",
      "normalized_text": "Output Interface\nJSON (タスク配列) Ollama/LM Studio (OpenAI 互換 )\n高品質構成",
      "heading_level": 3,
      "numbers": [],
      "hash": "cc977b7a6391e1d6",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p059_c00230",
      "block_type": "text",
      "page_no": 59,
      "order": 230,
      "bbox": [
        354.12,
        247.84,
        606.53,
        293.13
      ],
      "text": "Target HW\nA-64+ / G-16+ / G-24 想定 (Apple 64GB+ / VRAM 16GB+)",
      "normalized_text": "Target HW\nA-64+ / G-16+ / G-24 想定 (Apple 64GB+ / VRAM 16GB+)",
      "heading_level": 3,
      "numbers": [
        "64",
        "16",
        "24 ",
        "64",
        "16"
      ],
      "hash": "54af8468efb9058e",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p059_c00231",
      "block_type": "text",
      "page_no": 59,
      "order": 231,
      "bbox": [
        369.41,
        323.34,
        590.89,
        444.14
      ],
      "text": "VAD & ASR\nfaster-whisper (GPU) 必要なら大型 Whisper 系列\nLLM 整形・抽出\n14B 〜 32B 級 (Phi-4/Qwen 上位 ) 4bit / GPU 実行",
      "normalized_text": "VAD & ASR\nfaster-whisper (GPU) 必要なら大型 Whisper 系列\nLLM 整形・抽出\n14B 〜 32B 級 (Phi-4/Qwen 上位 ) 4bit / GPU 実行",
      "heading_level": 3,
      "numbers": [
        "14",
        "32",
        "4",
        "4"
      ],
      "hash": "fecd74196c6aefc7",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p059_c00232",
      "block_type": "text",
      "page_no": 59,
      "order": 232,
      "bbox": [
        359.93,
        197.53,
        849.92,
        519.64
      ],
      "text": "Output Interface\nFunction Calling 前提 Schema 固定で JSON 破壊率低減\nハード制約構成",
      "normalized_text": "Output Interface\nFunction Calling 前提 Schema 固定で JSON 破壊率低減\nハード制約構成",
      "heading_level": 3,
      "numbers": [],
      "hash": "fab7aba54168ae02",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p059_c00233",
      "block_type": "text",
      "page_no": 59,
      "order": 233,
      "bbox": [
        658.43,
        247.84,
        904.86,
        299.5
      ],
      "text": "Target HW\nW-CPU1 / A-16 ( ギリギリ ) ( 低スペック PC / メモリ不\n足 )",
      "normalized_text": "Target HW\nW-CPU1 / A-16 ( ギリギリ ) ( 低スペック PC / メモリ不\n足 )",
      "heading_level": 3,
      "numbers": [
        "1 ",
        "16 "
      ],
      "hash": "ff739a32ba5bc4e1",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p059_c00234",
      "block_type": "text",
      "page_no": 59,
      "order": 234,
      "bbox": [
        657.53,
        323.34,
        905.38,
        444.14
      ],
      "text": "VAD & ASR\n短音声限定 or 夜間バッチ処理リアルタイム性は犠\n牲にする\nLLM 整形・抽出\n3B 〜 4B 級 (Phi-3 mini 等 ) 「抽出のみ」に機能限定",
      "normalized_text": "VAD & ASR\n短音声限定 or 夜間バッチ処理リアルタイム性は犠\n牲にする\nLLM 整形・抽出\n3B 〜 4B 級 (Phi-3 mini 等 ) 「抽出のみ」に機能限定",
      "heading_level": 3,
      "numbers": [
        "3",
        "4",
        "3 "
      ],
      "hash": "67e4d20b393404cd",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p059_c00235",
      "block_type": "text",
      "page_no": 59,
      "order": 235,
      "bbox": [
        670.37,
        474.34,
        892.59,
        518.46
      ],
      "text": "Output Interface\n抽出テンプレート固定自由生成を極力減らす",
      "normalized_text": "Output Interface\n抽出テンプレート固定自由生成を極力減らす",
      "heading_level": 3,
      "numbers": [],
      "hash": "4a2b6f60f3de8e27",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p060_c00236",
      "block_type": "text",
      "page_no": 60,
      "order": 236,
      "bbox": [
        62.69,
        24.28,
        919.47,
        155.36
      ],
      "text": "音声メモ → 整文化 → タスク抽出(失敗モードと回避策) ユースケース別推奨スタック\n結論:誤転記・要点漏れ・推論不足は「設計」で抑え込む。 VAD による区間分割、逐次要約、テンプレ固定が鍵となります。",
      "normalized_text": "音声メモ → 整文化 → タスク抽出(失敗モードと回避策) ユースケース別推奨スタック\n結論:誤転記・要点漏れ・推論不足は「設計」で抑え込む。 VAD による区間分割、逐次要約、テンプレ固定が鍵となります。",
      "heading_level": 3,
      "numbers": [],
      "hash": "c013ab5f74504853",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p060_c00237",
      "block_type": "text",
      "page_no": 60,
      "order": 237,
      "bbox": [
        69.54,
        225.18,
        309.24,
        299.6
      ],
      "text": "ASR 誤転記 → 抽出ミス\n失敗モード\nASR が固有名詞や数値を誤認識し、そのままタスクと して抽出されてしまう。",
      "normalized_text": "ASR 誤転記 → 抽出ミス\n失敗モード\nASR が固有名詞や数値を誤認識し、そのままタスクと して抽出されてしまう。",
      "heading_level": 3,
      "numbers": [],
      "hash": "f8a1bf837971ca74",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p060_c00238",
      "block_type": "text",
      "page_no": 60,
      "order": 238,
      "bbox": [
        60.63,
        225.18,
        603.91,
        436.53
      ],
      "text": "回避策( Design )\n1. VAD で区間分割:無音除去で認識精度向上。 2. 重要箇所再確認 UI :抽出されたタスクの元音声を ワンクリック再生できる UI を提供。\nSilero VAD Playback UI\n長文での要点漏れ\n失敗モード\nコンテキスト長超過や Attention の分散により、後半 の重要事項が無視される。",
      "normalized_text": "回避策( Design )\n1. VAD で区間分割:無音除去で認識精度向上。 2. 重要箇所再確認 UI :抽出されたタスクの元音声を ワンクリック再生できる UI を提供。\nSilero VAD Playback UI\n長文での要点漏れ\n失敗モード\nコンテキスト長超過や Attention の分散により、後半 の重要事項が無視される。",
      "heading_level": 3,
      "numbers": [
        "1. ",
        "2. "
      ],
      "hash": "547ed08e08e158dc",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p060_c00239",
      "block_type": "text",
      "page_no": 60,
      "order": 239,
      "bbox": [
        365.14,
        322.05,
        610.04,
        436.53
      ],
      "text": "回避策( Design )\n1. 逐次要約( Rolling Summary ):一定区間ごとに要 約し、次区間の入力に含める。\n2. KV 節約:量子化 KV やウインドウ制限でメモリ枯渇 を防ぐ。\nRolling Context Quantized KV",
      "normalized_text": "回避策( Design )\n1. 逐次要約( Rolling Summary ):一定区間ごとに要 約し、次区間の入力に含める。\n2. KV 節約:量子化 KV やウインドウ制限でメモリ枯渇 を防ぐ。\nRolling Context Quantized KV",
      "heading_level": 3,
      "numbers": [
        "1. ",
        "2. "
      ],
      "hash": "e88ca76cd68b2ac0",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p060_c00240",
      "block_type": "text",
      "page_no": 60,
      "order": 240,
      "bbox": [
        673.85,
        225.18,
        911.13,
        299.6
      ],
      "text": "LLM の推論不足\n失敗モード\n小型モデル( 3B-7B )が指示に従わず、タスク以外の 雑談や幻覚を出力する。",
      "normalized_text": "LLM の推論不足\n失敗モード\n小型モデル( 3B-7B )が指示に従わず、タスク以外の 雑談や幻覚を出力する。",
      "heading_level": 3,
      "numbers": [
        "3",
        "7"
      ],
      "hash": "108340b67405e606",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p060_c00241",
      "block_type": "text",
      "page_no": 60,
      "order": 241,
      "bbox": [
        75.05,
        322.05,
        913.46,
        527.57
      ],
      "text": "回避策( Design )\n1. テンプレ固定:自由生成を禁止し、抽出テンプレ ート( JSON Schema 等)を強制。\n2. Few-shot 提示:プロンプトに抽出成功例を含める 。\nJSON Schema Few-shot\nシステム連携のポイント\nOllama/LM Studio の OpenAI 互換 API を活用して業務アプリと接続する場合、受け取った JSON 出力の型チェック(バリデーション)をクライアント側で厳格に行うことで、モデルの不安定さを 吸収できます。",
      "normalized_text": "回避策( Design )\n1. テンプレ固定:自由生成を禁止し、抽出テンプレ ート( JSON Schema 等)を強制。\n2. Few-shot 提示:プロンプトに抽出成功例を含める 。\nJSON Schema Few-shot\nシステム連携のポイント\nOllama/LM Studio の OpenAI 互換 API を活用して業務アプリと接続する場合、受け取った JSON 出力の型チェック(バリデーション)をクライアント側で厳格に行うことで、モデルの不安定さを 吸収できます。",
      "heading_level": 3,
      "numbers": [
        "1. ",
        "2. "
      ],
      "hash": "6c09067219453c54",
      "meta": {
        "body_font_size": 10.1,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p061_c00242",
      "block_type": "text",
      "page_no": 61,
      "order": 242,
      "bbox": [
        36.03,
        21.66,
        922.54,
        531.79
      ],
      "text": "ユースケース:会議議事録(話者分離+要約+アクション) 推奨スタック 3 構成( Tier 別)\n結論:規模と要件に応じて 3 構成を選択。無音除去( VAD )と発話ターン統合ルールが品質下限を決定します。 ※評価は p50/p95 、 RTF 、意味改変率で監視することを推奨します。\n低コスト構成\n前処理 (VAD)\nSilero VAD [82] 無音区間を確実に除去\n[82]\nASR ( 音声認識 )\nfaster-whisper [50] (CPU / INT8 量子化 )\n[50]\n話者分離 (Diarization)\npyannote.audio [84] ( 必要時のみ適用、負荷高 )\n[84]\nLLM ( 要約・抽出 )\n7B 級 (Qwen2.5 等 ) 要約・論点・ ToDo 抽出\n個人利用 / 小規模 MTG\n高品質構成\nASR (GPU 加速 )\nfaster-whisper (GPU) 大型 Whisper も視野 [48,50] [48,50]\nタイムスタンプ\nASR タイムスタンプ活用各要点に元発話時刻を付与\nLLM ( 高度な分析 )\n32B 〜 70B 級 KV 最適化 (FP8 KV 等 ) [5–7] [5–7]\n検証・品質\n発話と要約の突合検証文脈保持とアクション紐付け\n公式議事録 / 業務記録\nハード制約構成\n機能縮退\n話者分離を省略「話者なし要約 → タスク」へ縮退\n処理タイミング\n夜間バッチ処理長時間の音声ファイルは非同期\n軽量モデル\nWhisper tiny/base + 3B 級 LLM (Phi-3 mini 等 )\n目的限定\n「決定事項のみ」抽出に特化全文書き起こしは放棄\nリソース極小 / バッチ処理\nLocal AI Technical Survey Report 2026 61 / 80",
      "normalized_text": "ユースケース:会議議事録(話者分離+要約+アクション) 推奨スタック 3 構成( Tier 別)\n結論:規模と要件に応じて 3 構成を選択。無音除去( VAD )と発話ターン統合ルールが品質下限を決定します。 ※評価は p50/p95 、 RTF 、意味改変率で監視することを推奨します。\n低コスト構成\n前処理 (VAD)\nSilero VAD [82] 無音区間を確実に除去\n[82]\nASR ( 音声認識 )\nfaster-whisper [50] (CPU / INT8 量子化 )\n[50]\n話者分離 (Diarization)\npyannote.audio [84] ( 必要時のみ適用、負荷高 )\n[84]\nLLM ( 要約・抽出 )\n7B 級 (Qwen2.5 等 ) 要約・論点・ ToDo 抽出\n個人利用 / 小規模 MTG\n高品質構成\nASR (GPU 加速 )\nfaster-whisper (GPU) 大型 Whisper も視野 [48,50] [48,50]\nタイムスタンプ\nASR タイムスタンプ活用各要点に元発話時刻を付与\nLLM ( 高度な分析 )\n32B 〜 70B 級 KV 最適化 (FP8 KV 等 ) [5–7] [5–7]\n検証・品質\n発話と要約の突合検証文脈保持とアクション紐付け\n公式議事録 / 業務記録\nハード制約構成\n機能縮退\n話者分離を省略「話者なし要約 → タスク」へ縮退\n処理タイミング\n夜間バッチ処理長時間の音声ファイルは非同期\n軽量モデル\nWhisper tiny/base + 3B 級 LLM (Phi-3 mini 等 )\n目的限定\n「決定事項のみ」抽出に特化全文書き起こしは放棄\nリソース極小 / バッチ処理\nLocal AI Technical Survey Report 2026 61 / 80",
      "heading_level": 0,
      "numbers": [
        "3 ",
        "3 ",
        "50",
        "95 ",
        "82",
        "82",
        "50",
        "8 ",
        "50",
        "84",
        "84",
        "7",
        "2.5 ",
        "48,50",
        "48,50",
        "32",
        "70",
        "8 ",
        "5",
        "7",
        "5",
        "7",
        "3",
        "3 ",
        "2026 ",
        "61 ",
        "80"
      ],
      "hash": "19ab2787a92fb66c",
      "meta": {
        "body_font_size": 10.08,
        "body_line_count": 33
      }
    },
    {
      "chunk_id": "document_p062_c00243",
      "block_type": "text",
      "page_no": 62,
      "order": 243,
      "bbox": [
        58.57,
        24.28,
        920.58,
        154.15
      ],
      "text": "会議議事録:失敗モードと回避策 ユースケース詳細分析\n結論:話者誤割当・ ASR 幻覚・長時間処理の負荷を設計で抑止。 VAD (無音除去)の厳格化とタイムスタンプベースの検証が品質の防波堤となります。",
      "normalized_text": "会議議事録:失敗モードと回避策 ユースケース詳細分析\n結論:話者誤割当・ ASR 幻覚・長時間処理の負荷を設計で抑止。 VAD (無音除去)の厳格化とタイムスタンプベースの検証が品質の防波堤となります。",
      "heading_level": 3,
      "numbers": [],
      "hash": "61460b3e524b6402",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p062_c00244",
      "block_type": "text",
      "page_no": 62,
      "order": 244,
      "bbox": [
        73.99,
        227.97,
        456.93,
        450.93
      ],
      "text": "主な失敗モード( Failure Modes )\n話者誤割当( Diarization Error ) 高頻度\n発話の切れ目が不明確で、 A さんの発言が B さんとして記録される。特に割り 込み発話で多発。\nASR 幻覚( Hallucination ) 致命的\n無音区間やノイズに対して、存在しない文章(「ご視聴ありがとうございま した」等)を生成してしまう現象。\n長文による要点漏れ\nコンテキスト長超過により、議論の後半や中間部分の重要決定事項が LLM の要 約から欠落する。",
      "normalized_text": "主な失敗モード( Failure Modes )\n話者誤割当( Diarization Error ) 高頻度\n発話の切れ目が不明確で、 A さんの発言が B さんとして記録される。特に割り 込み発話で多発。\nASR 幻覚( Hallucination ) 致命的\n無音区間やノイズに対して、存在しない文章(「ご視聴ありがとうございま した」等)を生成してしまう現象。\n長文による要点漏れ\nコンテキスト長超過により、議論の後半や中間部分の重要決定事項が LLM の要 約から欠落する。",
      "heading_level": 3,
      "numbers": [],
      "hash": "f783b236ec82e916",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 6
      }
    },
    {
      "chunk_id": "document_p062_c00245",
      "block_type": "text",
      "page_no": 62,
      "order": 245,
      "bbox": [
        71.31,
        227.97,
        905.45,
        541.58
      ],
      "text": "回避策と設計( Mitigation )\n短い切れ目のマージ+手動マッピング 設計\n極端に短い発話区間を前後の発話者に統合するルールを適用。参加者名と ID の紐付け UI を用意。\nVAD 厳格化+無音区間破棄 前処理\nSilero VAD 等の閾値を調整し、確実に音声がある区間のみ ASR へ渡す。「要審 査フラグ」で怪しい出力をマーク。\n逐次要約( Rolling Summary )+ KV 節約 LLM 最適化\n議論を短めのチャンクに分割して逐次要約し、最終的に統合。 KV 量子化やコ ンテキスト分割でメモリ節約。\n評価指標( Monitoring Metrics )\n運用時は p50/p95 レイテンシに加え、 RTF ( Real-Time Factor )と意味改変率を監視し、品質劣化の兆候を早期検知します(ベンチ設計参照)。",
      "normalized_text": "回避策と設計( Mitigation )\n短い切れ目のマージ+手動マッピング 設計\n極端に短い発話区間を前後の発話者に統合するルールを適用。参加者名と ID の紐付け UI を用意。\nVAD 厳格化+無音区間破棄 前処理\nSilero VAD 等の閾値を調整し、確実に音声がある区間のみ ASR へ渡す。「要審 査フラグ」で怪しい出力をマーク。\n逐次要約( Rolling Summary )+ KV 節約 LLM 最適化\n議論を短めのチャンクに分割して逐次要約し、最終的に統合。 KV 量子化やコ ンテキスト分割でメモリ節約。\n評価指標( Monitoring Metrics )\n運用時は p50/p95 レイテンシに加え、 RTF ( Real-Time Factor )と意味改変率を監視し、品質劣化の兆候を早期検知します(ベンチ設計参照)。",
      "heading_level": 3,
      "numbers": [
        "50",
        "95 "
      ],
      "hash": "0a024f5cdf42ec28",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 8
      }
    },
    {
      "chunk_id": "document_p063_c00246",
      "block_type": "text",
      "page_no": 63,
      "order": 246,
      "bbox": [
        58.57,
        21.26,
        910.23,
        213.4
      ],
      "text": "ユースケース:文書 RAG PDF/ スキャン → OCR → 検索 → 回答\n結論: RAG 品質は Embedding で上限が決まるため、「 Embedding で recall 確保 →Reranker で precision 向上」の二段構えが基本戦略で す。\n低コスト構成",
      "normalized_text": "ユースケース:文書 RAG PDF/ スキャン → OCR → 検索 → 回答\n結論: RAG 品質は Embedding で上限が決まるため、「 Embedding で recall 確保 →Reranker で precision 向上」の二段構えが基本戦略で す。\n低コスト構成",
      "heading_level": 3,
      "numbers": [],
      "hash": "1d9bf05c56a33de4",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p063_c00247",
      "block_type": "text",
      "page_no": 63,
      "order": 247,
      "bbox": [
        51.79,
        248.63,
        278.5,
        291.3
      ],
      "text": "OCR / Pre-process\nTesseract OCR ※スキャン品質が高い場合",
      "normalized_text": "OCR / Pre-process\nTesseract OCR ※スキャン品質が高い場合",
      "heading_level": 3,
      "numbers": [],
      "hash": "e48a18b0c8b5edec",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p063_c00248",
      "block_type": "text",
      "page_no": 63,
      "order": 248,
      "bbox": [
        51.79,
        318.87,
        276.94,
        361.53
      ],
      "text": "Embedding / Reranker\nBGE-M3 ( 多用途 ) bge-reranker-base ( 軽量 )",
      "normalized_text": "Embedding / Reranker\nBGE-M3 ( 多用途 ) bge-reranker-base ( 軽量 )",
      "heading_level": 3,
      "numbers": [
        "3 "
      ],
      "hash": "9d81a58fcb94ddf2",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p063_c00249",
      "block_type": "text",
      "page_no": 63,
      "order": 249,
      "bbox": [
        51.79,
        198.25,
        533.46,
        492.76
      ],
      "text": "LLM Runtime\n7B 級 4bit (CPU) llama.cpp / Ollama\nテキスト中心の標準 PDF\n高品質構成",
      "normalized_text": "LLM Runtime\n7B 級 4bit (CPU) llama.cpp / Ollama\nテキスト中心の標準 PDF\n高品質構成",
      "heading_level": 3,
      "numbers": [
        "7",
        "4"
      ],
      "hash": "2338adcf8b6d85de",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p063_c00250",
      "block_type": "text",
      "page_no": 63,
      "order": 250,
      "bbox": [
        352.9,
        248.63,
        591.16,
        305.52
      ],
      "text": "OCR / DocAI\nPaddleOCR ( 構造化 ) Donut (OCR-free 情報抽出 )\nLayoutLMv3 ( レイアウト保持 )",
      "normalized_text": "OCR / DocAI\nPaddleOCR ( 構造化 ) Donut (OCR-free 情報抽出 )\nLayoutLMv3 ( レイアウト保持 )",
      "heading_level": 3,
      "numbers": [
        "3 "
      ],
      "hash": "f1af79e5c89a8a83",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p063_c00251",
      "block_type": "text",
      "page_no": 63,
      "order": 251,
      "bbox": [
        352.9,
        334.66,
        570.81,
        377.26
      ],
      "text": "Embedding / Reranker\nBGE-M3 + 大型 reranker (GPU 推論推奨 )",
      "normalized_text": "Embedding / Reranker\nBGE-M3 + 大型 reranker (GPU 推論推奨 )",
      "heading_level": 3,
      "numbers": [
        "3 "
      ],
      "hash": "a323d8a0954c1309",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p063_c00252",
      "block_type": "text",
      "page_no": 63,
      "order": 252,
      "bbox": [
        352.9,
        198.25,
        849.92,
        491.74
      ],
      "text": "LLM Runtime\n14B 〜 32B 級 (GPU) 図表・レイアウト情報を加味\n図表・帳票を含む文書\nハード制約構成",
      "normalized_text": "LLM Runtime\n14B 〜 32B 級 (GPU) 図表・レイアウト情報を加味\n図表・帳票を含む文書\nハード制約構成",
      "heading_level": 3,
      "numbers": [
        "14",
        "32"
      ],
      "hash": "bc444c8963693a30",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p063_c00253",
      "block_type": "text",
      "page_no": 63,
      "order": 253,
      "bbox": [
        654.08,
        248.63,
        894.22,
        291.3
      ],
      "text": "Processing Strategy\n夜間バッチ処理 OCR ・ Embedding を夜間に生成",
      "normalized_text": "Processing Strategy\n夜間バッチ処理 OCR ・ Embedding を夜間に生成",
      "heading_level": 3,
      "numbers": [],
      "hash": "5083d968684150dc",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p063_c00254",
      "block_type": "text",
      "page_no": 63,
      "order": 254,
      "bbox": [
        654.08,
        318.87,
        896.69,
        361.53
      ],
      "text": "Daytime Operation\n検索と短い回答のみ実行 LLM 推論負荷を最小化",
      "normalized_text": "Daytime Operation\n検索と短い回答のみ実行 LLM 推論負荷を最小化",
      "heading_level": 3,
      "numbers": [],
      "hash": "9b8b79badb74123c",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p063_c00255",
      "block_type": "text",
      "page_no": 63,
      "order": 255,
      "bbox": [
        36.03,
        389.11,
        922.54,
        548.32
      ],
      "text": "LLM Runtime\n3B 〜 7B 級 (4bit/INT8) コンテキスト長を制限\nリソース極小環境\nLocal AI Technical Survey Report 2026 63 / 80",
      "normalized_text": "LLM Runtime\n3B 〜 7B 級 (4bit/INT8) コンテキスト長を制限\nリソース極小環境\nLocal AI Technical Survey Report 2026 63 / 80",
      "heading_level": 3,
      "numbers": [
        "3",
        "7",
        "4",
        "8",
        "2026 ",
        "63 ",
        "80"
      ],
      "hash": "3895042ab1452e04",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p064_c00256",
      "block_type": "text",
      "page_no": 64,
      "order": 256,
      "bbox": [
        58.57,
        24.28,
        911.83,
        509.54
      ],
      "text": "文書 RAG :失敗モードと回避策 OCR 誤読・検索失敗・幻覚への対策設計\n結論:文書 RAG の失敗は「 OCR 誤読」「検索精度不足」「幻覚引用」に大別され、 前処理の標準化 と 原文スニペット引用 の UX 要件化で回避します。\n主な失敗モード( Failure Modes )\nスキャン品質低下や傾きにより、固有名詞や数値が誤認識され、正しい文書が ヒットしない。 Recall 低下\n表組みや段組みが崩れてテキスト化され、文脈が断絶。 LLM が意味を誤解釈す る。 Context 喪失\n検索結果に含まれない情報を、さも引用したかのように回答する。\n信頼性毀損\n回避策と設計( Mitigation )\n傾き補正・二値化をパイプライン化。構造化には PaddleOCR 、レイアウト保持に は Donut/LayoutLMv3 を採用。 PaddleOCR/Donut\n回答の根拠となった OCR テキストの断片(スニペット)を必ず提示し、ユーザ ーが原文を確認できる UI にする。 Verify UI\nOCR と Embedding は夜間バッチで済ませ、検索時は Reranker で精度向上に集中。 LLM には「引用外は回答不可」を指示。 Batch/Rerank\n実務上のポイント( Key Takeaway )\nPDF/ 画像 RAG では、 Embedding モデルの性能以前に「テキスト抽出の品質」が上限を決めます。 Tesseract 等の古典 OCR で限界を感じたら、早めに視覚モデル( VLM や Document AI )への切り替 えを検討すべきです。",
      "normalized_text": "文書 RAG :失敗モードと回避策 OCR 誤読・検索失敗・幻覚への対策設計\n結論:文書 RAG の失敗は「 OCR 誤読」「検索精度不足」「幻覚引用」に大別され、 前処理の標準化 と 原文スニペット引用 の UX 要件化で回避します。\n主な失敗モード( Failure Modes )\nスキャン品質低下や傾きにより、固有名詞や数値が誤認識され、正しい文書が ヒットしない。 Recall 低下\n表組みや段組みが崩れてテキスト化され、文脈が断絶。 LLM が意味を誤解釈す る。 Context 喪失\n検索結果に含まれない情報を、さも引用したかのように回答する。\n信頼性毀損\n回避策と設計( Mitigation )\n傾き補正・二値化をパイプライン化。構造化には PaddleOCR 、レイアウト保持に は Donut/LayoutLMv3 を採用。 PaddleOCR/Donut\n回答の根拠となった OCR テキストの断片(スニペット)を必ず提示し、ユーザ ーが原文を確認できる UI にする。 Verify UI\nOCR と Embedding は夜間バッチで済ませ、検索時は Reranker で精度向上に集中。 LLM には「引用外は回答不可」を指示。 Batch/Rerank\n実務上のポイント( Key Takeaway )\nPDF/ 画像 RAG では、 Embedding モデルの性能以前に「テキスト抽出の品質」が上限を決めます。 Tesseract 等の古典 OCR で限界を感じたら、早めに視覚モデル( VLM や Document AI )への切り替 えを検討すべきです。",
      "heading_level": 3,
      "numbers": [
        "3 "
      ],
      "hash": "344ad928732977e9",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 12
      }
    },
    {
      "chunk_id": "document_p065_c00257",
      "block_type": "text",
      "page_no": 65,
      "order": 257,
      "bbox": [
        58.57,
        21.26,
        913.67,
        213.4
      ],
      "text": "ユースケース:画像理解( VLM ) — 3 構成 スクショ / 写真の説明・抽出:推奨スタック比較\n結論: 2B 〜 7B 級 VLM がローカルの現実ライン。 72B 級は上位機前提となります。 ※視覚トークン肥大による速度 / メモリ急落を防ぐため、解像度・ max-pixels の制御が必須です。\n低コスト構成",
      "normalized_text": "ユースケース:画像理解( VLM ) — 3 構成 スクショ / 写真の説明・抽出:推奨スタック比較\n結論: 2B 〜 7B 級 VLM がローカルの現実ライン。 72B 級は上位機前提となります。 ※視覚トークン肥大による速度 / メモリ急落を防ぐため、解像度・ max-pixels の制御が必須です。\n低コスト構成",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "2",
        "7",
        "72"
      ],
      "hash": "b2aaf4ae3b962280",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p065_c00258",
      "block_type": "text",
      "page_no": 65,
      "order": 258,
      "bbox": [
        51.79,
        248.63,
        240.21,
        283.4
      ],
      "text": "Target HW\nApple 16GB / VRAM 8GB 級",
      "normalized_text": "Target HW\nApple 16GB / VRAM 8GB 級",
      "heading_level": 3,
      "numbers": [
        "16",
        "8"
      ],
      "hash": "9f01c685b151bcc9",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p065_c00259",
      "block_type": "text",
      "page_no": 65,
      "order": 259,
      "bbox": [
        51.79,
        303.08,
        265.3,
        345.74
      ],
      "text": "VLM Model\nQwen2-VL 2B 軽量・高速な視覚理解",
      "normalized_text": "VLM Model\nQwen2-VL 2B 軽量・高速な視覚理解",
      "heading_level": 3,
      "numbers": [
        "2",
        "2"
      ],
      "hash": "cfb1dd5f7d469cba",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p065_c00260",
      "block_type": "text",
      "page_no": 65,
      "order": 260,
      "bbox": [
        51.79,
        373.32,
        278.74,
        422.35
      ],
      "text": "Runtime\nmac: MLX-VLM ( 公式例あり ) Win: GPU 推論\n(transformers 等 )",
      "normalized_text": "Runtime\nmac: MLX-VLM ( 公式例あり ) Win: GPU 推論\n(transformers 等 )",
      "heading_level": 3,
      "numbers": [],
      "hash": "1099d0014e55e210",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p065_c00261",
      "block_type": "text",
      "page_no": 65,
      "order": 261,
      "bbox": [
        51.79,
        198.25,
        533.46,
        531.4
      ],
      "text": "Key Point\n解像度を制限しメモリ圧迫を回避\nスクショ説明・簡易 OCR\n高品質構成",
      "normalized_text": "Key Point\n解像度を制限しメモリ圧迫を回避\nスクショ説明・簡易 OCR\n高品質構成",
      "heading_level": 3,
      "numbers": [],
      "hash": "779183b1c0c8c3fe",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p065_c00262",
      "block_type": "text",
      "page_no": 65,
      "order": 262,
      "bbox": [
        352.9,
        248.63,
        551.7,
        282.68
      ],
      "text": "Target HW\nApple 32GB+ / VRAM 16-24GB+",
      "normalized_text": "Target HW\nApple 32GB+ / VRAM 16-24GB+",
      "heading_level": 3,
      "numbers": [
        "32",
        "16",
        "24"
      ],
      "hash": "1805c4bba66c7e08",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p065_c00263",
      "block_type": "text",
      "page_no": 65,
      "order": 263,
      "bbox": [
        352.9,
        303.08,
        602.31,
        345.74
      ],
      "text": "VLM Model\nQwen2-VL 7B / InternVL2 ( 必要に応じて上位モデル )",
      "normalized_text": "VLM Model\nQwen2-VL 7B / InternVL2 ( 必要に応じて上位モデル )",
      "heading_level": 3,
      "numbers": [
        "2",
        "7",
        "2 "
      ],
      "hash": "078b0153389aa14f",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p065_c00264",
      "block_type": "text",
      "page_no": 65,
      "order": 264,
      "bbox": [
        352.9,
        373.32,
        606.21,
        422.35
      ],
      "text": "Strategy\n画像 → テキスト抽出 →Embedding →RAG 融合 ( ハイブ\nリッド検索 )",
      "normalized_text": "Strategy\n画像 → テキスト抽出 →Embedding →RAG 融合 ( ハイブ\nリッド検索 )",
      "heading_level": 3,
      "numbers": [],
      "hash": "fbdebd8be6d86585",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p065_c00265",
      "block_type": "text",
      "page_no": 65,
      "order": 265,
      "bbox": [
        352.9,
        198.25,
        849.92,
        530.37
      ],
      "text": "Key Point\nOCR-free の図表理解能力を活用\n図表理解・情報抽出\nハード制約構成",
      "normalized_text": "Key Point\nOCR-free の図表理解能力を活用\n図表理解・情報抽出\nハード制約構成",
      "heading_level": 3,
      "numbers": [],
      "hash": "fc3ff8c4bd454d5d",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p065_c00266",
      "block_type": "text",
      "page_no": 65,
      "order": 266,
      "bbox": [
        654.08,
        248.63,
        843.81,
        283.4
      ],
      "text": "Target HW\nVRAM 不足 / メモリ制約大",
      "normalized_text": "Target HW\nVRAM 不足 / メモリ制約大",
      "heading_level": 3,
      "numbers": [],
      "hash": "324d1302033ac27f",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p065_c00267",
      "block_type": "text",
      "page_no": 65,
      "order": 267,
      "bbox": [
        654.08,
        303.08,
        906.64,
        350.93
      ],
      "text": "Substitute Strategy\nOCR (PaddleOCR 等 ) + テキスト LLM VLM モデルを使用\nしない",
      "normalized_text": "Substitute Strategy\nOCR (PaddleOCR 等 ) + テキスト LLM VLM モデルを使用\nしない",
      "heading_level": 3,
      "numbers": [],
      "hash": "1f8a8a9499bcc111",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p065_c00268",
      "block_type": "text",
      "page_no": 65,
      "order": 268,
      "bbox": [
        654.08,
        373.32,
        906.19,
        421.17
      ],
      "text": "Operation\nOCR でテキスト化し、 LLM で整形画像入力自体を避\nける",
      "normalized_text": "Operation\nOCR でテキスト化し、 LLM で整形画像入力自体を避\nける",
      "heading_level": 3,
      "numbers": [],
      "hash": "1d2ffb825cedbd4c",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p065_c00269",
      "block_type": "text",
      "page_no": 65,
      "order": 269,
      "bbox": [
        654.08,
        443.56,
        903.59,
        530.37
      ],
      "text": "Compromise\n空間認識・文脈理解を諦め文字情報の抽出・整理\nに限定\n文字主体の処理",
      "normalized_text": "Compromise\n空間認識・文脈理解を諦め文字情報の抽出・整理\nに限定\n文字主体の処理",
      "heading_level": 3,
      "numbers": [],
      "hash": "0fa3de3bd31b10a3",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p066_c00270",
      "block_type": "text",
      "page_no": 66,
      "order": 270,
      "bbox": [
        58.57,
        24.28,
        917.29,
        511.52
      ],
      "text": "画像理解:失敗モードと回避策 ユースケース別推奨スタック(詳細)\n結論: UI 文字の誤読と視覚トークン肥大による速度低下を、 OCR 併用 と 解像度・ max-pixels 制御 で回避します。\n主な失敗モード( Failure Modes )\n2B/7B 級 VLM では、スクリーンショット内の小さなフォントや密集した 情報を正確に読み取れないケースが頻発。\n高解像度画像をそのまま入力すると、視覚トークン数が数千に達し、 推論速度( TTFT/ 生成)が急激に悪化。\n複雑な表組みやグラフの空間関係を誤認し、存在しない数値や関係性 を捏造する。\n回避策・設計( Mitigation )\n文字情報は PaddleOCR 等で別経路からテキストとして供給し、 VLM は「 状況説明」に専念させる。\nMLX-VLM 等の max-pixels パラメータで上限を設定し、トークン数を抑制 。実務的には 1024px 程度にリサイズ。\n構造化が必要な帳票類は、レイアウト情報を保持できる専用モデル( LayoutLMv3 )や Donut を活用。\nPaddleOCR max-pixels 制御 LayoutLMv3",
      "normalized_text": "画像理解:失敗モードと回避策 ユースケース別推奨スタック(詳細)\n結論: UI 文字の誤読と視覚トークン肥大による速度低下を、 OCR 併用 と 解像度・ max-pixels 制御 で回避します。\n主な失敗モード( Failure Modes )\n2B/7B 級 VLM では、スクリーンショット内の小さなフォントや密集した 情報を正確に読み取れないケースが頻発。\n高解像度画像をそのまま入力すると、視覚トークン数が数千に達し、 推論速度( TTFT/ 生成)が急激に悪化。\n複雑な表組みやグラフの空間関係を誤認し、存在しない数値や関係性 を捏造する。\n回避策・設計( Mitigation )\n文字情報は PaddleOCR 等で別経路からテキストとして供給し、 VLM は「 状況説明」に専念させる。\nMLX-VLM 等の max-pixels パラメータで上限を設定し、トークン数を抑制 。実務的には 1024px 程度にリサイズ。\n構造化が必要な帳票類は、レイアウト情報を保持できる専用モデル( LayoutLMv3 )や Donut を活用。\nPaddleOCR max-pixels 制御 LayoutLMv3",
      "heading_level": 0,
      "numbers": [
        "2",
        "7",
        "1024",
        "3 ",
        "3"
      ],
      "hash": "3dfb899210fb3836",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p067_c00271",
      "block_type": "text",
      "page_no": 67,
      "order": 271,
      "bbox": [
        58.57,
        21.26,
        909.42,
        213.4
      ],
      "text": "ユースケース:コーディング補助 推奨スタック 3 パターン( IDE 支援・リポジトリ理解)\n結論:補完は小型モデル、設計レビューや長距離依存解決は中〜大型モデルで役割分担します。 ※生成コードは自動コンパイル / テスト実行を “ ツール ” 化し、検証ループに組み込むことが重要です。\n低コスト構成",
      "normalized_text": "ユースケース:コーディング補助 推奨スタック 3 パターン( IDE 支援・リポジトリ理解)\n結論:補完は小型モデル、設計レビューや長距離依存解決は中〜大型モデルで役割分担します。 ※生成コードは自動コンパイル / テスト実行を “ ツール ” 化し、検証ループに組み込むことが重要です。\n低コスト構成",
      "heading_level": 0,
      "numbers": [
        "3 "
      ],
      "hash": "100b017894848084",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p067_c00272",
      "block_type": "text",
      "page_no": 67,
      "order": 272,
      "bbox": [
        51.79,
        248.63,
        283.44,
        291.3
      ],
      "text": "Code LLM\nQwen2.5-Coder 7B サイズ展開が豊富で軽量",
      "normalized_text": "Code LLM\nQwen2.5-Coder 7B サイズ展開が豊富で軽量",
      "heading_level": 3,
      "numbers": [
        "2.5",
        "7"
      ],
      "hash": "63ce9c971eb7d8c0",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p067_c00273",
      "block_type": "text",
      "page_no": 67,
      "order": 273,
      "bbox": [
        51.79,
        318.87,
        298.58,
        367.9
      ],
      "text": "RAG / Embedding\nリポジトリ Embedding (BGE/E5) 関連ファイル抽出\n→LLM 回答",
      "normalized_text": "RAG / Embedding\nリポジトリ Embedding (BGE/E5) 関連ファイル抽出\n→LLM 回答",
      "heading_level": 3,
      "numbers": [
        "5"
      ],
      "hash": "e0de2948bcc5a4df",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p067_c00274",
      "block_type": "text",
      "page_no": 67,
      "order": 274,
      "bbox": [
        51.79,
        198.25,
        533.46,
        476.95
      ],
      "text": "Connection\nOllama / LM Studio OpenAI 互換 API で IDE 拡張接続\n一般的な IDE 補完・ Q&A\n高品質構成",
      "normalized_text": "Connection\nOllama / LM Studio OpenAI 互換 API で IDE 拡張接続\n一般的な IDE 補完・ Q&A\n高品質構成",
      "heading_level": 3,
      "numbers": [],
      "hash": "d0ff627bb9aba723",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p067_c00275",
      "block_type": "text",
      "page_no": 67,
      "order": 275,
      "bbox": [
        352.9,
        248.63,
        577.07,
        291.3
      ],
      "text": "Code LLM\nStarCoder2 (7B/15B) + 32B 級汎用 LLM 併用",
      "normalized_text": "Code LLM\nStarCoder2 (7B/15B) + 32B 級汎用 LLM 併用",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "7",
        "15",
        "32"
      ],
      "hash": "a80dee797e8774e6",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p067_c00276",
      "block_type": "text",
      "page_no": 67,
      "order": 276,
      "bbox": [
        352.9,
        318.87,
        605.65,
        366.72
      ],
      "text": "Optimization\nprefix caching (vLLM 等 ) 長文コンテキストでの遅延抑\n制",
      "normalized_text": "Optimization\nprefix caching (vLLM 等 ) 長文コンテキストでの遅延抑\n制",
      "heading_level": 3,
      "numbers": [],
      "hash": "cdbf92beef21be18",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p067_c00277",
      "block_type": "text",
      "page_no": 67,
      "order": 277,
      "bbox": [
        352.9,
        198.25,
        849.92,
        475.93
      ],
      "text": "Capability\n設計レビュー / 長距離依存複雑なリファクタリング\n提案\n大規模開発・設計支援\nハード制約構成",
      "normalized_text": "Capability\n設計レビュー / 長距離依存複雑なリファクタリング\n提案\n大規模開発・設計支援\nハード制約構成",
      "heading_level": 3,
      "numbers": [],
      "hash": "bf7956e2e44c9f1d",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p067_c00278",
      "block_type": "text",
      "page_no": 67,
      "order": 278,
      "bbox": [
        654.08,
        248.63,
        878.89,
        291.3
      ],
      "text": "Target HW\nメモリ 8GB / エントリー機古い開発環境",
      "normalized_text": "Target HW\nメモリ 8GB / エントリー機古い開発環境",
      "heading_level": 3,
      "numbers": [
        "8"
      ],
      "hash": "3d0ebbbdb6324cb4",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p067_c00279",
      "block_type": "text",
      "page_no": 67,
      "order": 279,
      "bbox": [
        654.08,
        318.87,
        870.39,
        361.53
      ],
      "text": "Code LLM\n3B 級モデル Phi-3 mini / StarCoder2-3B",
      "normalized_text": "Code LLM\n3B 級モデル Phi-3 mini / StarCoder2-3B",
      "heading_level": 3,
      "numbers": [
        "3",
        "3 ",
        "2",
        "3"
      ],
      "hash": "48dd3e748a6a631c",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p067_c00280",
      "block_type": "text",
      "page_no": 67,
      "order": 280,
      "bbox": [
        36.03,
        389.11,
        922.54,
        536.13
      ],
      "text": "Strategy\n短い支援に限定「行補完」+「関数リファクタ提\n案」\nスニペット補完のみ\nLocal AI Technical Survey Report 2026 67 / 80",
      "normalized_text": "Strategy\n短い支援に限定「行補完」+「関数リファクタ提\n案」\nスニペット補完のみ\nLocal AI Technical Survey Report 2026 67 / 80",
      "heading_level": 3,
      "numbers": [
        "2026 ",
        "67 ",
        "80"
      ],
      "hash": "fd3663bab7671777",
      "meta": {
        "body_font_size": 10.8,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p068_c00281",
      "block_type": "text",
      "page_no": 68,
      "order": 281,
      "bbox": [
        62.69,
        24.28,
        911.55,
        549.77
      ],
      "text": "コーディング支援:失敗モードと回避策 コーディング補助( IDE 支援・リポジトリ理解)\n結論:幻覚 API 生成と長文遅延を「検証ループ」と「キャッシュ活用」で抑止し、 コード品質とセキュリティを自動化プロセスで担保します。\n主な失敗モード( Failure Modes )\nライブラリのバージョン不一致や、もっともらしいが実在しない関数 ( Hallucination )を生成。\nリポジトリ全体を読み込むと KV キャッシュが肥大化し、補完のレスポ ンスが低下する。\n古い構文や廃止されたメソッドを使用し、コンパイルエラーやセキュ リティリスクを招く。\n回避策・設計( Mitigation & Design )\nビルド・単体テスト実行を function calling として LLM に提供。失敗時は エラーログをフィードバックして再生成させる。\nvLLM や llama.cpp のキャッシュ機能を活用し、共通コンテキスト(ライ ブラリ定義等)の再計算を回避。 [88][90][112]\nLinter や依存関係解決ツールを “ ツール ” として組み込み、生成コードを 静的解析してから提示する。\nパラメータ設定の含意\nコード生成では「創造性」よりも「正確性」が優先されます。温度()を低く設定し、や最大トークン数を固定することで、挙動のばらつきを抑制します。また、サンド",
      "normalized_text": "コーディング支援:失敗モードと回避策 コーディング補助( IDE 支援・リポジトリ理解)\n結論:幻覚 API 生成と長文遅延を「検証ループ」と「キャッシュ活用」で抑止し、 コード品質とセキュリティを自動化プロセスで担保します。\n主な失敗モード( Failure Modes )\nライブラリのバージョン不一致や、もっともらしいが実在しない関数 ( Hallucination )を生成。\nリポジトリ全体を読み込むと KV キャッシュが肥大化し、補完のレスポ ンスが低下する。\n古い構文や廃止されたメソッドを使用し、コンパイルエラーやセキュ リティリスクを招く。\n回避策・設計( Mitigation & Design )\nビルド・単体テスト実行を function calling として LLM に提供。失敗時は エラーログをフィードバックして再生成させる。\nvLLM や llama.cpp のキャッシュ機能を活用し、共通コンテキスト(ライ ブラリ定義等)の再計算を回避。 [88][90][112]\nLinter や依存関係解決ツールを “ ツール ” として組み込み、生成コードを 静的解析してから提示する。\nパラメータ設定の含意\nコード生成では「創造性」よりも「正確性」が優先されます。温度()を低く設定し、や最大トークン数を固定することで、挙動のばらつきを抑制します。また、サンド",
      "heading_level": 0,
      "numbers": [
        "88",
        "90",
        "112"
      ],
      "hash": "0da999165a6360a7",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 12
      }
    },
    {
      "chunk_id": "document_p069_c00282",
      "block_type": "text",
      "page_no": 69,
      "order": 282,
      "bbox": [
        58.57,
        21.26,
        914.48,
        214.91
      ],
      "text": "ユースケース:画像生成( Diffusion ) — 3 構成 推奨スタックと VRAM 要件の最適化\n結論: SDXL を軸に ComfyUI でワークフローを資産化。 FLUX 等はライセンス精査が必須です。 ※ VRAM 要件は解像度・バッチ・ステップ数に依存するため、プロファイル固定が重要です。\n低コスト構成",
      "normalized_text": "ユースケース:画像生成( Diffusion ) — 3 構成 推奨スタックと VRAM 要件の最適化\n結論: SDXL を軸に ComfyUI でワークフローを資産化。 FLUX 等はライセンス精査が必須です。 ※ VRAM 要件は解像度・バッチ・ステップ数に依存するため、プロファイル固定が重要です。\n低コスト構成",
      "heading_level": 3,
      "numbers": [
        "3 "
      ],
      "hash": "3a1454c4cb106d29",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p069_c00283",
      "block_type": "text",
      "page_no": 69,
      "order": 283,
      "bbox": [
        62.32,
        251.66,
        152.61,
        301.04
      ],
      "text": "Target VRAM\nVRAM 8GB 〜 12GB (Apple Silicon 16GB)",
      "normalized_text": "Target VRAM\nVRAM 8GB 〜 12GB (Apple Silicon 16GB)",
      "heading_level": 3,
      "numbers": [
        "8",
        "12",
        "16"
      ],
      "hash": "271c28abc23e6f82",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p069_c00284",
      "block_type": "text",
      "page_no": 69,
      "order": 284,
      "bbox": [
        62.32,
        330.12,
        190.73,
        379.57
      ],
      "text": "Model / UI\nSDXL base 1.0 + Refiner なし ComfyUI (Win/Linux/macOS)",
      "normalized_text": "Model / UI\nSDXL base 1.0 + Refiner なし ComfyUI (Win/Linux/macOS)",
      "heading_level": 3,
      "numbers": [
        "1.0 "
      ],
      "hash": "1708cbb6cf1e568e",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p069_c00285",
      "block_type": "text",
      "page_no": 69,
      "order": 285,
      "bbox": [
        62.32,
        408.65,
        168.47,
        458.02
      ],
      "text": "Optimization\nAttention Slicing 有効化 FP16 / Tiled VAE 活用",
      "normalized_text": "Optimization\nAttention Slicing 有効化 FP16 / Tiled VAE 活用",
      "heading_level": 3,
      "numbers": [
        "16 "
      ],
      "hash": "cc18096c3c4d0dc1",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p069_c00286",
      "block_type": "text",
      "page_no": 69,
      "order": 286,
      "bbox": [
        62.32,
        199.76,
        534.9,
        536.57
      ],
      "text": "Resolution\n1024x1024 固定 Batch size = 1\n高品質構成",
      "normalized_text": "Resolution\n1024x1024 固定 Batch size = 1\n高品質構成",
      "heading_level": 3,
      "numbers": [
        "1024",
        "1024 ",
        "1\n"
      ],
      "hash": "bb060f4ed951f508",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p069_c00287",
      "block_type": "text",
      "page_no": 69,
      "order": 287,
      "bbox": [
        363.43,
        251.66,
        459.47,
        301.04
      ],
      "text": "Target VRAM\nVRAM 24GB 以上 (Apple Silicon 64GB+)",
      "normalized_text": "Target VRAM\nVRAM 24GB 以上 (Apple Silicon 64GB+)",
      "heading_level": 3,
      "numbers": [
        "24",
        "64"
      ],
      "hash": "de0faf060d5b3136",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p069_c00288",
      "block_type": "text",
      "page_no": 69,
      "order": 288,
      "bbox": [
        363.43,
        330.12,
        510.16,
        379.57
      ],
      "text": "Model / UI\nFLUX.1 [dev/pro] 等の上位系 ComfyUI ( ワークフロー資産化 )",
      "normalized_text": "Model / UI\nFLUX.1 [dev/pro] 等の上位系 ComfyUI ( ワークフロー資産化 )",
      "heading_level": 3,
      "numbers": [
        "1 "
      ],
      "hash": "71bd069708c82ea9",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p069_c00289",
      "block_type": "text",
      "page_no": 69,
      "order": 289,
      "bbox": [
        363.43,
        408.65,
        473.14,
        458.02
      ],
      "text": "Optimization\nSeed 固定で再現性担保 Refiner / LoRA 多重適用",
      "normalized_text": "Optimization\nSeed 固定で再現性担保 Refiner / LoRA 多重適用",
      "heading_level": 3,
      "numbers": [],
      "hash": "025fb6b984358b38",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p069_c00290",
      "block_type": "text",
      "page_no": 69,
      "order": 290,
      "bbox": [
        363.43,
        199.76,
        851.43,
        535.39
      ],
      "text": "License Check\nFLUX.1-dev 等の商用条件 ライセンス精査が必須\nハード制約構成",
      "normalized_text": "License Check\nFLUX.1-dev 等の商用条件 ライセンス精査が必須\nハード制約構成",
      "heading_level": 3,
      "numbers": [
        "1"
      ],
      "hash": "99c32cc610469124",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p069_c00291",
      "block_type": "text",
      "page_no": 69,
      "order": 291,
      "bbox": [
        664.61,
        251.66,
        770.3,
        301.04
      ],
      "text": "Target VRAM\nVRAM 8GB 未満 ( メインメモリ共有等 )",
      "normalized_text": "Target VRAM\nVRAM 8GB 未満 ( メインメモリ共有等 )",
      "heading_level": 3,
      "numbers": [
        "8"
      ],
      "hash": "75390aabcdad2507",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p069_c00292",
      "block_type": "text",
      "page_no": 69,
      "order": 292,
      "bbox": [
        664.61,
        330.12,
        802.36,
        379.57
      ],
      "text": "Model / UI\nSD 1.5 系 / LCM-LoRA ( 高速化 ) Stable Diffusion WebUI (Forge)",
      "normalized_text": "Model / UI\nSD 1.5 系 / LCM-LoRA ( 高速化 ) Stable Diffusion WebUI (Forge)",
      "heading_level": 3,
      "numbers": [
        "1.5 "
      ],
      "hash": "1d9b0232678bba09",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p069_c00293",
      "block_type": "text",
      "page_no": 69,
      "order": 293,
      "bbox": [
        664.61,
        408.65,
        825.43,
        458.02
      ],
      "text": "Offloading strategy\n夜間バッチ処理 または LAN 内別筐体へオフロード",
      "normalized_text": "Offloading strategy\n夜間バッチ処理 または LAN 内別筐体へオフロード",
      "heading_level": 3,
      "numbers": [],
      "hash": "b5e636739b257c04",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p069_c00294",
      "block_type": "text",
      "page_no": 69,
      "order": 294,
      "bbox": [
        664.61,
        487.15,
        793.41,
        536.57
      ],
      "text": "Limitation\n解像度 512x512 等に制限 日中は LLM にリソース集中",
      "normalized_text": "Limitation\n解像度 512x512 等に制限 日中は LLM にリソース集中",
      "heading_level": 3,
      "numbers": [
        "512",
        "512 "
      ],
      "hash": "fdce2cc94c2a11ac",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p070_c00295",
      "block_type": "text",
      "page_no": 70,
      "order": 295,
      "bbox": [
        62.69,
        24.28,
        915.03,
        543.74
      ],
      "text": "画像生成:失敗モードと回避策 VRAM 制約と品質・ライセンス管理\n結論: VRAM 不足と生成結果のブレを 「標準プロファイル」 で抑制し、 ライセンス違反リスクを 「事前精査と監査」 で排除します。\n主な失敗モード( Failure Modes )\n高解像度や大バッチ指定時にプロセスがクラッシュ。特に SDXL/FLUX 等 の大型モデルで頻発。\n同じプロンプトでも Seed や設定の違いで出力が変動し、 UI モック等の修 正サイクルが回らない。\nFLUX.1-dev 等の非商用 / 開発用ライセンスを、誤って商用プロダクトや 社内資料に利用してしまう。\n回避策と設計( Mitigation Strategies )\nVRAM 容量に応じた上限テーブルを作成。 Attention Slicing や VAE Tiling 等 の最適化を強制適用。\nSeed を固定し、 ComfyUI ワークフローを JSON として資産化・バージョン 管理する。 [72]\n使用モデルのライセンス条項(商用可否)をリスト化し、生成ログに モデルハッシュを紐付ける。 [76]\n設計含意( Design Implication )",
      "normalized_text": "画像生成:失敗モードと回避策 VRAM 制約と品質・ライセンス管理\n結論: VRAM 不足と生成結果のブレを 「標準プロファイル」 で抑制し、 ライセンス違反リスクを 「事前精査と監査」 で排除します。\n主な失敗モード( Failure Modes )\n高解像度や大バッチ指定時にプロセスがクラッシュ。特に SDXL/FLUX 等 の大型モデルで頻発。\n同じプロンプトでも Seed や設定の違いで出力が変動し、 UI モック等の修 正サイクルが回らない。\nFLUX.1-dev 等の非商用 / 開発用ライセンスを、誤って商用プロダクトや 社内資料に利用してしまう。\n回避策と設計( Mitigation Strategies )\nVRAM 容量に応じた上限テーブルを作成。 Attention Slicing や VAE Tiling 等 の最適化を強制適用。\nSeed を固定し、 ComfyUI ワークフローを JSON として資産化・バージョン 管理する。 [72]\n使用モデルのライセンス条項(商用可否)をリスト化し、生成ログに モデルハッシュを紐付ける。 [76]\n設計含意( Design Implication )",
      "heading_level": 0,
      "numbers": [
        "1",
        "72",
        "76"
      ],
      "hash": "0a14eeeaf7808f24",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 11
      }
    },
    {
      "chunk_id": "document_p071_c00296",
      "block_type": "text",
      "page_no": 71,
      "order": 296,
      "bbox": [
        60.01,
        114.56,
        159.84,
        215.07
      ],
      "text": "10",
      "normalized_text": "10",
      "heading_level": 1,
      "numbers": [
        "10"
      ],
      "hash": "5b573e916f492c19",
      "meta": {
        "body_font_size": 12.98
      }
    },
    {
      "chunk_id": "document_p071_c00297",
      "block_type": "text",
      "page_no": 71,
      "order": 297,
      "bbox": [
        60.01,
        264.23,
        308.33,
        411.78
      ],
      "text": "測定と比較の 方法\nベンチマーク設計と 再現性の担保",
      "normalized_text": "測定と比較の 方法\nベンチマーク設計と 再現性の担保",
      "heading_level": 3,
      "numbers": [],
      "hash": "dd2c23c98a2f6eb8",
      "meta": {
        "body_font_size": 12.98,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p071_c00298",
      "block_type": "text",
      "page_no": 71,
      "order": 298,
      "bbox": [
        492.23,
        83.75,
        879.46,
        452.86
      ],
      "text": "KE Y TAKEAWAYS\n本章の要点\n指標の厳密定義:\np50/p95 レイテンシ、 TTFT 、 tok/s 、 RTF 等を定義し、 JSON 破 壊率や意味改変率も定量化します。\n測定手順の標準化:\n条件固定(温度・量子化等)、ウォームアップ分離、 KV 影 響の切り分け(短文 / 長文)を徹底します。\n再現性のコア要素:\nASR/TTS の前処理固定と、最小テストセット(短文 / 長文 / 構 造化 / 音声)による比較を実施します。\n指標定義 測定手順 ログ保存 テストセット",
      "normalized_text": "KE Y TAKEAWAYS\n本章の要点\n指標の厳密定義:\np50/p95 レイテンシ、 TTFT 、 tok/s 、 RTF 等を定義し、 JSON 破 壊率や意味改変率も定量化します。\n測定手順の標準化:\n条件固定(温度・量子化等)、ウォームアップ分離、 KV 影 響の切り分け(短文 / 長文)を徹底します。\n再現性のコア要素:\nASR/TTS の前処理固定と、最小テストセット(短文 / 長文 / 構 造化 / 音声)による比較を実施します。\n指標定義 測定手順 ログ保存 テストセット",
      "heading_level": 3,
      "numbers": [
        "50",
        "95 "
      ],
      "hash": "e498aae716389b4f",
      "meta": {
        "body_font_size": 12.98,
        "body_line_count": 5
      }
    },
    {
      "chunk_id": "document_p072_c00299",
      "block_type": "text",
      "page_no": 72,
      "order": 299,
      "bbox": [
        59.16,
        20.38,
        908.16,
        543.09
      ],
      "text": "測定指標の定義 p50/p95 ・ TTFT ・ tok/s ・ RTF ・品質指標\n結論: LLM の「初速( TTFT )」と「生成速度( tok/s )」を分離して計測し、 品質( JSON 破壊率等)とリソース消費( RAM/VRAM )を定量化します。\n速度・リソース指標( Latency & Resource )\nTTFT ( Time To First Token )と生成完了時間を分離して計測。 KV reuse や prompt cache の効果は TTFT に現れるため分離が必須。\n生成トークン数 ÷ 生成時間。ユーザーの体感速度やバッチ処理能力の指標。\nASR/TTS 用指標。処理時間 ÷ 音声長。\nRTF < 1.0 なら実時間より高速\nOS 監視またはプロセス単位( nvidia-smi 等)で計測。 Apple Silicon 等の統合メモリ 環境ではシステム全体の圧迫度も注視。\n品質・堅牢性指標( Quality & Robustness )\n指定スキーマに対する「パース失敗」や「スキーマ不一致」の発生率。ツール 呼び出し( Tool Use )の安定性を左右する重要指標。\n要約やフォーマット変換において、元の意味が変わってしまった割合。 Embedding 類似度による自動評価+人手監査で測定。\n同一条件下(温度、シード等)での結果の安定性。推論ごとのブレ幅を確認。\n測定上の注意( Apple Silicon ):",
      "normalized_text": "測定指標の定義 p50/p95 ・ TTFT ・ tok/s ・ RTF ・品質指標\n結論: LLM の「初速( TTFT )」と「生成速度( tok/s )」を分離して計測し、 品質( JSON 破壊率等)とリソース消費( RAM/VRAM )を定量化します。\n速度・リソース指標( Latency & Resource )\nTTFT ( Time To First Token )と生成完了時間を分離して計測。 KV reuse や prompt cache の効果は TTFT に現れるため分離が必須。\n生成トークン数 ÷ 生成時間。ユーザーの体感速度やバッチ処理能力の指標。\nASR/TTS 用指標。処理時間 ÷ 音声長。\nRTF < 1.0 なら実時間より高速\nOS 監視またはプロセス単位( nvidia-smi 等)で計測。 Apple Silicon 等の統合メモリ 環境ではシステム全体の圧迫度も注視。\n品質・堅牢性指標( Quality & Robustness )\n指定スキーマに対する「パース失敗」や「スキーマ不一致」の発生率。ツール 呼び出し( Tool Use )の安定性を左右する重要指標。\n要約やフォーマット変換において、元の意味が変わってしまった割合。 Embedding 類似度による自動評価+人手監査で測定。\n同一条件下(温度、シード等)での結果の安定性。推論ごとのブレ幅を確認。\n測定上の注意( Apple Silicon ):",
      "heading_level": 0,
      "numbers": [
        "50",
        "95 ",
        "1.0 "
      ],
      "hash": "964d30960150c9b7",
      "meta": {
        "body_font_size": 10.82,
        "body_line_count": 13
      }
    },
    {
      "chunk_id": "document_p073_c00300",
      "block_type": "text",
      "page_no": 73,
      "order": 300,
      "bbox": [
        30.04,
        19.66,
        922.48,
        531.84
      ],
      "text": "測定手順(条件固定・ウォームアップ・ KV 切り分け) 再現可能なベンチマーク設計の要件\n再現条件の固定\n生成パラメータの統一\n同一プロンプト、最大トークン数、温度( temperature )、 top_p を固 定し、ランダム性を排除または制御します。\nモデル環境の固定\n同一の量子化形式( GGUF Q4_K_M など)、コンテキスト長設定を使 用します。\n音声・画像系の前提\nASR/TTS は同一音声・同一前処理( VAD 有無)、同一話者設定で品質 と速度を分離測定します。\nKey Takeaway: 条件のわずかな違い( VAD 有無など)が結果を大きく左右するた め、メタデータとして全条件を記録します。\n測定手順\nウォームアップ分離\n初回ロード(モデル読込+コンパイル)と 2 回目以降の推論を分離。 常駐運用とオンデマンド運用の性能差を明確化します。\nKV 影響の切り分け\n短文( <512 tokens )と長文( 4k/8k/32k )を分けて測定し、 paged/quantized KV の効果を評価します。\nログ保存要件\nモデルハッシュ、量子化方式、ランタイム Ver 、ハード情報( CPU 命令 、 VRAM )を記録し、再現性を担保します。\n条件固定 ウォームアップ KV 分離\nPage 73 | ローカル AI 技術調査レポート",
      "normalized_text": "測定手順(条件固定・ウォームアップ・ KV 切り分け) 再現可能なベンチマーク設計の要件\n再現条件の固定\n生成パラメータの統一\n同一プロンプト、最大トークン数、温度( temperature )、 top_p を固 定し、ランダム性を排除または制御します。\nモデル環境の固定\n同一の量子化形式( GGUF Q4_K_M など)、コンテキスト長設定を使 用します。\n音声・画像系の前提\nASR/TTS は同一音声・同一前処理( VAD 有無)、同一話者設定で品質 と速度を分離測定します。\nKey Takeaway: 条件のわずかな違い( VAD 有無など)が結果を大きく左右するた め、メタデータとして全条件を記録します。\n測定手順\nウォームアップ分離\n初回ロード(モデル読込+コンパイル)と 2 回目以降の推論を分離。 常駐運用とオンデマンド運用の性能差を明確化します。\nKV 影響の切り分け\n短文( <512 tokens )と長文( 4k/8k/32k )を分けて測定し、 paged/quantized KV の効果を評価します。\nログ保存要件\nモデルハッシュ、量子化方式、ランタイム Ver 、ハード情報( CPU 命令 、 VRAM )を記録し、再現性を担保します。\n条件固定 ウォームアップ KV 分離\nPage 73 | ローカル AI 技術調査レポート",
      "heading_level": 3,
      "numbers": [
        "4",
        "2 ",
        "512 ",
        "4",
        "8",
        "32",
        "73 "
      ],
      "hash": "0492b7c00554a2e7",
      "meta": {
        "body_font_size": 12.27,
        "body_line_count": 17
      }
    },
    {
      "chunk_id": "document_p074_c00301",
      "block_type": "text",
      "page_no": 74,
      "order": 301,
      "bbox": [
        58.57,
        19.33,
        925.8,
        531.4
      ],
      "text": "リスクとコンプライアンス ライセンス管理・プライバシー・安全性対策\n結論:「コードの OSS ライセンス」と「モデル重みの利用条件」を分離管理し、ローカル完結の利点を活かしつつ、検証ループ による安全性確保が必須です。\nライセンス管理\nコードと重みの分離\nランタイム (MIT/Apache) とモデル重み (Community/ 非商用 ) は別 条件。利用範囲を台帳化する。\n個別確認の義務化\nQwen2.5 のファミリー内例外や、 FLUX.1-dev 等の派生モデル条 件を一次ソースで確認する。 [33]\nプライバシー・安全\nローカル完結の徹底\n入力データがデフォルトで外部送信されない構成を物理 / 論理 的に担保。 LAN 内 API も管理対象。\n入力ガードレール\nプロンプトインジェクション対策や PII (個人情報)フィルタ リングを前段に実装する。\n幻覚対策・監視\n検証ループの実装\n生成コードの自動コンパイル / テスト実行や、 RAG 引用元の存 在確認を自動化し、幻覚をフィルタする。\nログ監査\n入出力、パラメータ、実行環境情報をログ保存し、事後監査 や精度改善サイクルに活用する。\nPage 74 | ローカル AI 技術調査レポート\nライセンス確認\nコードと重みを\n分離して精査\n商用利用判定\n規約・例外条件 (Family 内差分 ) 確認\nローカル完結\nデータ送信遮断 プライバシー保護\n検証ループ\n幻覚・攻撃の\n自動検知\n継続監視\nログ監査と 是正サイクル",
      "normalized_text": "リスクとコンプライアンス ライセンス管理・プライバシー・安全性対策\n結論:「コードの OSS ライセンス」と「モデル重みの利用条件」を分離管理し、ローカル完結の利点を活かしつつ、検証ループ による安全性確保が必須です。\nライセンス管理\nコードと重みの分離\nランタイム (MIT/Apache) とモデル重み (Community/ 非商用 ) は別 条件。利用範囲を台帳化する。\n個別確認の義務化\nQwen2.5 のファミリー内例外や、 FLUX.1-dev 等の派生モデル条 件を一次ソースで確認する。 [33]\nプライバシー・安全\nローカル完結の徹底\n入力データがデフォルトで外部送信されない構成を物理 / 論理 的に担保。 LAN 内 API も管理対象。\n入力ガードレール\nプロンプトインジェクション対策や PII (個人情報)フィルタ リングを前段に実装する。\n幻覚対策・監視\n検証ループの実装\n生成コードの自動コンパイル / テスト実行や、 RAG 引用元の存 在確認を自動化し、幻覚をフィルタする。\nログ監査\n入出力、パラメータ、実行環境情報をログ保存し、事後監査 や精度改善サイクルに活用する。\nPage 74 | ローカル AI 技術調査レポート\nライセンス確認\nコードと重みを\n分離して精査\n商用利用判定\n規約・例外条件 (Family 内差分 ) 確認\nローカル完結\nデータ送信遮断 プライバシー保護\n検証ループ\n幻覚・攻撃の\n自動検知\n継続監視\nログ監査と 是正サイクル",
      "heading_level": 0,
      "numbers": [
        "2.5 ",
        "1",
        "33",
        "74 "
      ],
      "hash": "75a36278b0e06be6",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 22
      }
    },
    {
      "chunk_id": "document_p075_c00302",
      "block_type": "text",
      "page_no": 75,
      "order": 302,
      "bbox": [
        72.03,
        19.33,
        922.51,
        40.24
      ],
      "text": "まとめ(意思決定フロー) ローカル AI 導入の 5 ステップと判断基準",
      "normalized_text": "まとめ(意思決定フロー) ローカル AI 導入の 5 ステップと判断基準",
      "heading_level": 0,
      "numbers": [
        "5 "
      ],
      "hash": "c7cd6a43e2ffb6ad",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p075_c00303",
      "block_type": "text",
      "page_no": 75,
      "order": 303,
      "bbox": [
        48.76,
        96.45,
        175.09,
        362.39
      ],
      "text": "1 要件定義 Requirements Requirements\n業務課題から技術要件へ変換す る始点。\n判断基準 (Criteria)\n品質 : 日本語、 JSON 、専門性\n速度 : p95 レイテンシ、 tok/s\n機能 : Tool-use 、 RAG 有無\nアクション\n許容レイテンシ目標(例 : 10 tok/s )と JSON スキーマの厳格 さを決定。",
      "normalized_text": "1 要件定義 Requirements Requirements\n業務課題から技術要件へ変換す る始点。\n判断基準 (Criteria)\n品質 : 日本語、 JSON 、専門性\n速度 : p95 レイテンシ、 tok/s\n機能 : Tool-use 、 RAG 有無\nアクション\n許容レイテンシ目標(例 : 10 tok/s )と JSON スキーマの厳格 さを決定。",
      "heading_level": 3,
      "numbers": [
        "1 ",
        "95 ",
        "10 "
      ],
      "hash": "c79a399091040028",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p075_c00304",
      "block_type": "text",
      "page_no": 75,
      "order": 304,
      "bbox": [
        232.46,
        96.45,
        358.49,
        347.76
      ],
      "text": "2 Tier 選定 Hardware Tier Hardware Tier\nメモリ制約に基づく現実ライン の把握。\n判断基準 (Criteria)\nApple: 16 〜 128GB (UMA)\nWindows: VRAM 8 〜 24GB\nCPU: AVX/RAM 容量\nアクション\n「重み 4bit + KV キャッシュ」 の理論値で Tier (A-16 〜 G-24) を特定。",
      "normalized_text": "2 Tier 選定 Hardware Tier Hardware Tier\nメモリ制約に基づく現実ライン の把握。\n判断基準 (Criteria)\nApple: 16 〜 128GB (UMA)\nWindows: VRAM 8 〜 24GB\nCPU: AVX/RAM 容量\nアクション\n「重み 4bit + KV キャッシュ」 の理論値で Tier (A-16 〜 G-24) を特定。",
      "heading_level": 3,
      "numbers": [
        "2 ",
        "16 ",
        "128",
        "8 ",
        "24",
        "4",
        "16 ",
        "24"
      ],
      "hash": "35a31fb5644e4af9",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p075_c00305",
      "block_type": "text",
      "page_no": 75,
      "order": 305,
      "bbox": [
        416.16,
        96.45,
        547.7,
        333.12
      ],
      "text": "3 モデル選定 Model Select Model Select\nTier 内で動く最適モデルの選定。\n判断基準 (Criteria)\n規模 : 7B-14B ( 汎用 ), 32B+ ( 高品 質 )\n形式 : GGUF, EXL2, AWQ\n権利 : ライセンス条件\nアクション\nLlama/Qwen/Phi 等からサイズ 適合候補を選び、ライセンス を確認。",
      "normalized_text": "3 モデル選定 Model Select Model Select\nTier 内で動く最適モデルの選定。\n判断基準 (Criteria)\n規模 : 7B-14B ( 汎用 ), 32B+ ( 高品 質 )\n形式 : GGUF, EXL2, AWQ\n権利 : ライセンス条件\nアクション\nLlama/Qwen/Phi 等からサイズ 適合候補を選び、ライセンス を確認。",
      "heading_level": 3,
      "numbers": [
        "3 ",
        "7",
        "14",
        "32",
        "2, "
      ],
      "hash": "b12e58c0e1422184",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 7
      }
    },
    {
      "chunk_id": "document_p075_c00306",
      "block_type": "text",
      "page_no": 75,
      "order": 306,
      "bbox": [
        599.86,
        96.45,
        725.89,
        195.12
      ],
      "text": "4 ランタイム Runtime Runtime\nハードウェア性能を引き出す実 行環境。\n判断基準 (Criteria)",
      "normalized_text": "4 ランタイム Runtime Runtime\nハードウェア性能を引き出す実 行環境。\n判断基準 (Criteria)",
      "heading_level": 3,
      "numbers": [
        "4 "
      ],
      "hash": "d51d645a3473a53b",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p075_c00307",
      "block_type": "text",
      "page_no": 75,
      "order": 307,
      "bbox": [
        606.61,
        202.7,
        687.08,
        212.09
      ],
      "text": "Mac: MLX / llama.cpp",
      "normalized_text": "Mac: MLX / llama.cpp",
      "heading_level": 3,
      "numbers": [],
      "hash": "b1caf9f17c7515d4",
      "meta": {
        "body_font_size": 9.38
      }
    },
    {
      "chunk_id": "document_p075_c00308",
      "block_type": "text",
      "page_no": 75,
      "order": 308,
      "bbox": [
        606.61,
        218.27,
        725.0,
        333.12
      ],
      "text": "Win: Ollama / LM Studio / vLLM\nAPI: OpenAI 互換性\nアクション\nOS とモデル形式に合ったラン タイムを選択。 Server 機能の 有無を確認。",
      "normalized_text": "Win: Ollama / LM Studio / vLLM\nAPI: OpenAI 互換性\nアクション\nOS とモデル形式に合ったラン タイムを選択。 Server 機能の 有無を確認。",
      "heading_level": 3,
      "numbers": [],
      "hash": "fcd96e6cb2ca085a",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p075_c00309",
      "block_type": "text",
      "page_no": 75,
      "order": 309,
      "bbox": [
        36.03,
        96.45,
        922.54,
        534.82
      ],
      "text": "5 検証・最適化 Verify Verify\n実機ベンチによる実用性の確定 。\n判断基準 (Criteria)\n指標 : TTFT, p95, JSON 破壊率\n負荷 : メモリピーク , KV 推移\n品質 : 幻覚 , 誤転記\nアクション\n条件固定ベンチで測定。 KV 量 子化やプロンプトキャッシュ で調整。\n意思決定の要点 (Key Takeaways)\n「ランタイム先行」ではなく「要件 → メモリ (Tier)→ モデル」の順で決定することで、手戻りを防ぎます。 特に長文コンテキストでは KV キャッシュが支配的になるため、 KV 最適化( paged/quantized KV )の活用が成否を分けます。\nLocal AI Technical Survey Report 2026 75 / 80",
      "normalized_text": "5 検証・最適化 Verify Verify\n実機ベンチによる実用性の確定 。\n判断基準 (Criteria)\n指標 : TTFT, p95, JSON 破壊率\n負荷 : メモリピーク , KV 推移\n品質 : 幻覚 , 誤転記\nアクション\n条件固定ベンチで測定。 KV 量 子化やプロンプトキャッシュ で調整。\n意思決定の要点 (Key Takeaways)\n「ランタイム先行」ではなく「要件 → メモリ (Tier)→ モデル」の順で決定することで、手戻りを防ぎます。 特に長文コンテキストでは KV キャッシュが支配的になるため、 KV 最適化( paged/quantized KV )の活用が成否を分けます。\nLocal AI Technical Survey Report 2026 75 / 80",
      "heading_level": 3,
      "numbers": [
        "5 ",
        "95, ",
        "2026 ",
        "75 ",
        "80"
      ],
      "hash": "febd5addadfe3b4f",
      "meta": {
        "body_font_size": 9.38,
        "body_line_count": 10
      }
    },
    {
      "chunk_id": "document_p076_c00310",
      "block_type": "text",
      "page_no": 76,
      "order": 310,
      "bbox": [
        62.69,
        25.41,
        918.88,
        301.3
      ],
      "text": "総括(実務への適用) 実装の原則と次のステップ\n結論:ローカル AI は「 4bit 量子化+ KV 最適化」を前提に、 失敗モードを設計で抑え込むことで実務運用が可能です。\n実装の原則( Principles )\n重みは 4bit ( GGUF/AWQ/GPTQ )でメモリ理論値を計算し、 Tier に合わせ る。",
      "normalized_text": "総括(実務への適用) 実装の原則と次のステップ\n結論:ローカル AI は「 4bit 量子化+ KV 最適化」を前提に、 失敗モードを設計で抑え込むことで実務運用が可能です。\n実装の原則( Principles )\n重みは 4bit ( GGUF/AWQ/GPTQ )でメモリ理論値を計算し、 Tier に合わせ る。",
      "heading_level": 3,
      "numbers": [
        "4",
        "4"
      ],
      "hash": "4ed677acad7fb1a3",
      "meta": {
        "body_font_size": 12.24,
        "body_line_count": 3
      }
    },
    {
      "chunk_id": "document_p076_c00311",
      "block_type": "text",
      "page_no": 76,
      "order": 311,
      "bbox": [
        69.82,
        301.32,
        459.88,
        371.54
      ],
      "text": "Cost-Efficiency\n長文・多同時接続時は、 paged KV 、 KV 量子化、 prompt cache を活用してメ モリ爆発を防ぐ。",
      "normalized_text": "Cost-Efficiency\n長文・多同時接続時は、 paged KV 、 KV 量子化、 prompt cache を活用してメ モリ爆発を防ぐ。",
      "heading_level": 3,
      "numbers": [],
      "hash": "e2c983b6dc55c82c",
      "meta": {
        "body_font_size": 12.24,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p076_c00312",
      "block_type": "text",
      "page_no": 76,
      "order": 312,
      "bbox": [
        69.82,
        371.56,
        454.15,
        441.77
      ],
      "text": "Scalability\n幻覚・誤転記・ JSON 破壊は「起きるもの」とし、 UI 確認・検証ループ・ 再生成でカバーする。",
      "normalized_text": "Scalability\n幻覚・誤転記・ JSON 破壊は「起きるもの」とし、 UI 確認・検証ループ・ 再生成でカバーする。",
      "heading_level": 3,
      "numbers": [],
      "hash": "c9ec97faab559bb0",
      "meta": {
        "body_font_size": 12.24,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p076_c00313",
      "block_type": "text",
      "page_no": 76,
      "order": 313,
      "bbox": [
        69.82,
        221.64,
        730.09,
        511.94
      ],
      "text": "Robustness\np50/p95 レイテンシと品質( tok/s, RTF, JSON 破壊率)を定量ベンチマーク で測定し確定させる。\n次のステップ( Action Items )",
      "normalized_text": "Robustness\np50/p95 レイテンシと品質( tok/s, RTF, JSON 破壊率)を定量ベンチマーク で測定し確定させる。\n次のステップ( Action Items )",
      "heading_level": 3,
      "numbers": [
        "50",
        "95 "
      ],
      "hash": "73db7fcffb856e23",
      "meta": {
        "body_font_size": 12.24,
        "body_line_count": 2
      }
    },
    {
      "chunk_id": "document_p076_c00314",
      "block_type": "text",
      "page_no": 76,
      "order": 314,
      "bbox": [
        526.47,
        267.38,
        896.75,
        305.66
      ],
      "text": "1\nTier の確定: 手持ちハードウェア( Apple/Win )と Tier 表を照合し、現実的なモデル規模を把 握。",
      "normalized_text": "1\nTier の確定: 手持ちハードウェア( Apple/Win )と Tier 表を照合し、現実的なモデル規模を把 握。",
      "heading_level": 3,
      "numbers": [
        "1\n"
      ],
      "hash": "5098bee3dee6598a",
      "meta": {
        "body_font_size": 12.24,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p076_c00315",
      "block_type": "text",
      "page_no": 76,
      "order": 315,
      "bbox": [
        526.47,
        342.46,
        895.58,
        380.74
      ],
      "text": "2\nパイロット構築: 推奨スタック(低コスト / 高品質)に基づき、 llama.cpp/Ollama/faster-whisper 等 でプロトタイプ作成。",
      "normalized_text": "2\nパイロット構築: 推奨スタック(低コスト / 高品質)に基づき、 llama.cpp/Ollama/faster-whisper 等 でプロトタイプ作成。",
      "heading_level": 3,
      "numbers": [
        "2\n"
      ],
      "hash": "ed04dbcd2ea555d0",
      "meta": {
        "body_font_size": 12.24,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p076_c00316",
      "block_type": "text",
      "page_no": 76,
      "order": 316,
      "bbox": [
        526.47,
        417.59,
        896.85,
        444.72
      ],
      "text": "3\nベンチマーク実施: 条件固定(温度 / トークン数)でログを取り、 p95 レイテンシと実用性を計測。",
      "normalized_text": "3\nベンチマーク実施: 条件固定(温度 / トークン数)でログを取り、 p95 レイテンシと実用性を計測。",
      "heading_level": 3,
      "numbers": [
        "3\n",
        "95 "
      ],
      "hash": "a07956f5c5457a0d",
      "meta": {
        "body_font_size": 12.24,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p076_c00317",
      "block_type": "text",
      "page_no": 76,
      "order": 317,
      "bbox": [
        36.03,
        492.74,
        922.54,
        531.79
      ],
      "text": "4\nリスク監査: ライセンス(特に画像生成 /TTS )とプライバシー要件(外部送信なし)を最終 確認。 Local AI Technical Survey Report 2026 76 / 80",
      "normalized_text": "4\nリスク監査: ライセンス(特に画像生成 /TTS )とプライバシー要件(外部送信なし)を最終 確認。 Local AI Technical Survey Report 2026 76 / 80",
      "heading_level": 3,
      "numbers": [
        "4\n",
        "2026 ",
        "76 ",
        "80"
      ],
      "hash": "bd5baa9f705caa4c",
      "meta": {
        "body_font_size": 12.24,
        "body_line_count": 1
      }
    },
    {
      "chunk_id": "document_p077_c00318",
      "block_type": "text",
      "page_no": 77,
      "order": 318,
      "bbox": [
        36.03,
        34.72,
        899.54,
        531.44
      ],
      "text": "References ( 1/4 ) Technical Research Report Sources [1-34]\n[1] https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/\n[2] https://docs.openhands.dev/openhands/usage/llms/local-llms\n[3] https://lmstudio.ai/docs/developer/core/server\n[4] https://arxiv.org/abs/2306.00978\n[5] https://docs.vllm.ai/en/latest/design/paged_attention/\n[6] https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/\n[7] https://github.com/NVIDIA/TensorRT-LLM\n[8] https://github.com/ggml-org/llama.cpp\n[10] https://developer.apple.com/documentation/metal/choosing-a-resource-storage-mode- for-apple-gpus\n[12] https://huggingface.co/wangkanai/sdxl-fp16\n[13] https://github.com/intel/intel-extension-for-pytorch\n[14] https://github.com/xiph/rnnoise\n[16] https://huggingface.co/meta-llama/Llama-3.1-70B\n[17] https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\n[19] https://lmstudio.ai/docs/app\n[20] https://docs.nvidia.com/tensorrt-llm/index.html\n[22] https://github.com/ml-explore/mlx\n[23] https://github.com/ollama/ollama\n[24] https://docs.ollama.com/api/openai-compatibility\n[25] https://llama-cpp-python.readthedocs.io/en/latest/api-reference/\n[27] https://developers.openai.com/cookbook/articles/gpt-oss/run-locally-ollama/\n[28] https://github.com/Blaizzy/mlx-vlm\n[31] https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\n[32] https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n[33] https://qwenlm.github.io/blog/qwen2.5/\nPage 77 | ローカル AI 技術調査レポート 2026 Primary Information Source: Attached Report File",
      "normalized_text": "References ( 1/4 ) Technical Research Report Sources [1-34]\n[1] https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/\n[2] https://docs.openhands.dev/openhands/usage/llms/local-llms\n[3] https://lmstudio.ai/docs/developer/core/server\n[4] https://arxiv.org/abs/2306.00978\n[5] https://docs.vllm.ai/en/latest/design/paged_attention/\n[6] https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/\n[7] https://github.com/NVIDIA/TensorRT-LLM\n[8] https://github.com/ggml-org/llama.cpp\n[10] https://developer.apple.com/documentation/metal/choosing-a-resource-storage-mode- for-apple-gpus\n[12] https://huggingface.co/wangkanai/sdxl-fp16\n[13] https://github.com/intel/intel-extension-for-pytorch\n[14] https://github.com/xiph/rnnoise\n[16] https://huggingface.co/meta-llama/Llama-3.1-70B\n[17] https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\n[19] https://lmstudio.ai/docs/app\n[20] https://docs.nvidia.com/tensorrt-llm/index.html\n[22] https://github.com/ml-explore/mlx\n[23] https://github.com/ollama/ollama\n[24] https://docs.ollama.com/api/openai-compatibility\n[25] https://llama-cpp-python.readthedocs.io/en/latest/api-reference/\n[27] https://developers.openai.com/cookbook/articles/gpt-oss/run-locally-ollama/\n[28] https://github.com/Blaizzy/mlx-vlm\n[31] https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\n[32] https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n[33] https://qwenlm.github.io/blog/qwen2.5/\nPage 77 | ローカル AI 技術調査レポート 2026 Primary Information Source: Attached Report File",
      "heading_level": 3,
      "numbers": [
        "1",
        "4 ",
        "1",
        "34",
        "1",
        "2020",
        "11",
        "1",
        "2",
        "3",
        "4",
        "2306.00978\n",
        "5",
        "6",
        "7",
        "8",
        "10",
        "12",
        "16\n",
        "13",
        "14",
        "16",
        "3.1",
        "70",
        "17",
        "3",
        "128",
        "19",
        "20",
        "22",
        "23",
        "24",
        "25",
        "27",
        "28",
        "31",
        "3",
        "70",
        "32",
        "2.5",
        "7",
        "33",
        "2.5",
        "77 ",
        "2026 "
      ],
      "hash": "aa16995edb32f384",
      "meta": {
        "body_font_size": 9.36,
        "body_line_count": 26
      }
    },
    {
      "chunk_id": "document_p078_c00319",
      "block_type": "text",
      "page_no": 78,
      "order": 319,
      "bbox": [
        45.73,
        34.72,
        883.36,
        253.65
      ],
      "text": "References (参考文献リスト 2/4 ) [35] ~ [68] Llama / Qwen / Gemma / Phi / VLM / Whisper / TTS / OCR\nNO. S OU RC E U R L / D ES CR IPTION\n[35] G em m a 2 M odel C ard https://ai.google.dev/gemma/docs/core/model_card_2\n[37] Mixtral 8x7B Instruct https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\n[40] Phi- 4 Technical R eport (Arxiv) https://arxiv.org/abs/2412.08905",
      "normalized_text": "References (参考文献リスト 2/4 ) [35] ~ [68] Llama / Qwen / Gemma / Phi / VLM / Whisper / TTS / OCR\nNO. S OU RC E U R L / D ES CR IPTION\n[35] G em m a 2 M odel C ard https://ai.google.dev/gemma/docs/core/model_card_2\n[37] Mixtral 8x7B Instruct https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\n[40] Phi- 4 Technical R eport (Arxiv) https://arxiv.org/abs/2412.08905",
      "heading_level": 3,
      "numbers": [
        "2",
        "4 ",
        "35",
        "68",
        "35",
        "2 ",
        "2\n",
        "37",
        "8",
        "7",
        "8",
        "7",
        "0.1\n",
        "40",
        "4 ",
        "2412.08905"
      ],
      "hash": "e309773864b1c344",
      "meta": {
        "body_font_size": 7.94,
        "body_line_count": 4
      }
    },
    {
      "chunk_id": "document_p078_c00320",
      "block_type": "text",
      "page_no": 78,
      "order": 320,
      "bbox": [
        36.03,
        124.65,
        914.64,
        549.75
      ],
      "text": "[41]\nOp en AI C ook bo ok (R un locally LM S tu dio) https://developers.openai.com/cookbook/articles/gpt-oss/run-locally- lmstudio/\n[42] Qw en2- VL 72B https://huggingface.co/Qwen/Qwen2-VL-72B\n[43] Phi- 3- vision 128k In struct https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\n[45] In ternVL2 4B https://huggingface.co/OpenGVLab/InternVL2-4B\n[47] LLaVA R eposito ry https://github.com/haotian-liu/LLaVA\n[48] Op en AI W hisper https://openai.com/index/whisper/\n[49] AP N ews (W hisp er H allucinatio n Risk) https://apnews.com/article/90020cdf5fa16c79ca2e5b6c4c9bbb14\nN O . S OU RC E U R L / D ES CR IPTION\n[52] Pip er TTS https://github.com/rhasspy/piper\n[53] Ko koro TTS https://github.com/hexgrad/kokoro\n[54] C oqu i XTTS- v2 https://huggingface.co/coqui/XTTS-v2\n[55] S tyleTTS2 https://github.com/yl4579/StyleTTS2\n[58] BG E- M3 Em b ed din g https://huggingface.co/BAAI/bge-m3\n[59] Multiling ual- E5 Larg e https://huggingface.co/intfloat/multilingual-e5-large\n[60] Multiling ual- E5 Larg e Instruct https://huggingface.co/intfloat/multilingual-e5-large-instruct\n[61] BG E Reranker Larg e https://huggingface.co/BAAI/bge-reranker-large\n[62] BG E Reranker v 2 M 3 https://huggingface.co/BAAI/bge-reranker-v2-m3\n[64] Tesseract OC R https://github.com/tesseract-ocr/tesseract\n[66] PaddleO CR https://github.com/PaddlePaddle/PaddleOCR\nPage 78 | ローカル AI 技術調査レポート 2026 Reference List [35-68]",
      "normalized_text": "[41]\nOp en AI C ook bo ok (R un locally LM S tu dio) https://developers.openai.com/cookbook/articles/gpt-oss/run-locally- lmstudio/\n[42] Qw en2- VL 72B https://huggingface.co/Qwen/Qwen2-VL-72B\n[43] Phi- 3- vision 128k In struct https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\n[45] In ternVL2 4B https://huggingface.co/OpenGVLab/InternVL2-4B\n[47] LLaVA R eposito ry https://github.com/haotian-liu/LLaVA\n[48] Op en AI W hisper https://openai.com/index/whisper/\n[49] AP N ews (W hisp er H allucinatio n Risk) https://apnews.com/article/90020cdf5fa16c79ca2e5b6c4c9bbb14\nN O . S OU RC E U R L / D ES CR IPTION\n[52] Pip er TTS https://github.com/rhasspy/piper\n[53] Ko koro TTS https://github.com/hexgrad/kokoro\n[54] C oqu i XTTS- v2 https://huggingface.co/coqui/XTTS-v2\n[55] S tyleTTS2 https://github.com/yl4579/StyleTTS2\n[58] BG E- M3 Em b ed din g https://huggingface.co/BAAI/bge-m3\n[59] Multiling ual- E5 Larg e https://huggingface.co/intfloat/multilingual-e5-large\n[60] Multiling ual- E5 Larg e Instruct https://huggingface.co/intfloat/multilingual-e5-large-instruct\n[61] BG E Reranker Larg e https://huggingface.co/BAAI/bge-reranker-large\n[62] BG E Reranker v 2 M 3 https://huggingface.co/BAAI/bge-reranker-v2-m3\n[64] Tesseract OC R https://github.com/tesseract-ocr/tesseract\n[66] PaddleO CR https://github.com/PaddlePaddle/PaddleOCR\nPage 78 | ローカル AI 技術調査レポート 2026 Reference List [35-68]",
      "heading_level": 3,
      "numbers": [
        "41",
        "42",
        "2",
        "72",
        "2",
        "72",
        "43",
        "3",
        "128",
        "3",
        "128",
        "45",
        "2 ",
        "4",
        "2",
        "4",
        "47",
        "48",
        "49",
        "90020",
        "5",
        "16",
        "79",
        "2",
        "5",
        "6",
        "4",
        "9",
        "14\n",
        "52",
        "53",
        "54",
        "2 ",
        "2\n",
        "55",
        "2 ",
        "4579",
        "2\n",
        "58",
        "3 ",
        "3\n",
        "59",
        "5 ",
        "5",
        "60",
        "5 ",
        "5",
        "61",
        "62",
        "2 ",
        "3 ",
        "2",
        "3\n",
        "64",
        "66",
        "78 ",
        "2026 ",
        "35",
        "68"
      ],
      "hash": "7ae394882e452311",
      "meta": {
        "body_font_size": 7.94,
        "body_line_count": 20
      }
    },
    {
      "chunk_id": "document_p079_c00321",
      "block_type": "text",
      "page_no": 79,
      "order": 321,
      "bbox": [
        36.03,
        34.72,
        918.18,
        531.44
      ],
      "text": "References ( 3/4 ) 参考文献リスト [69] - [102]\nN o . R eference D etails (Layout / Im ag e Gen / Voice / H W )\n[69] LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking https://arxiv.org/pdf/2204.08387\n[71] Stable Diffusion XL Base 1.0 https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\n[72] ComfyUI (A powerful and modular stable diffusion GUI) https://github.com/Comfy-Org/ComfyUI\n[73] Stable Diffusion WebUI (AUTOMATIC1111) https://github.com/AUTOMATIC1111/stable-diffusion-webui\n[75] FLUX (Black Forest Labs) https://github.com/black-forest-labs/flux\n[76] FLUX.1-dev Model License https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev\n[82] Silero VAD (Pre-trained enterprise-grade Voice Activity Detector) https://github.com/snakers4/silero-vad\n[84] pyannote.audio (Neural building blocks for speaker diarization) https://github.com/pyannote/pyannote-audio\nN o . R eference D etails (Voice / Op tim ization / H ardw are)\n[85] openWakeWord (Open-source wake word detection) https://github.com/dscripka/openWakeWord\n[87] ExLlamaV2 (Fast inference library for modern LLMs) https://github.com/turboderp-org/exllamav2\n[88] llama.cpp Discussion: KV Cache Management https://github.com/ggerganov/llama.cpp/discussions/7949\n[90] llama.cpp Discussion: Prompt Caching https://github.com/ggml-org/llama.cpp/discussions/8860\n[92] Apple MacBook Pro Specs https://www.apple.com/macbook-pro/specs/\n[93] Apple Support: Mac Memory Configurations https://support.apple.com/en-us/111901\n[94] Apple Support: Mac Studio Memory Specs https://support.apple.com/ja-jp/117736\n[97] AMD ROCm Documentation for Windows (Radeon/Ryzen) https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/index.html\nPage 79 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report",
      "normalized_text": "References ( 3/4 ) 参考文献リスト [69] - [102]\nN o . R eference D etails (Layout / Im ag e Gen / Voice / H W )\n[69] LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking https://arxiv.org/pdf/2204.08387\n[71] Stable Diffusion XL Base 1.0 https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\n[72] ComfyUI (A powerful and modular stable diffusion GUI) https://github.com/Comfy-Org/ComfyUI\n[73] Stable Diffusion WebUI (AUTOMATIC1111) https://github.com/AUTOMATIC1111/stable-diffusion-webui\n[75] FLUX (Black Forest Labs) https://github.com/black-forest-labs/flux\n[76] FLUX.1-dev Model License https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev\n[82] Silero VAD (Pre-trained enterprise-grade Voice Activity Detector) https://github.com/snakers4/silero-vad\n[84] pyannote.audio (Neural building blocks for speaker diarization) https://github.com/pyannote/pyannote-audio\nN o . R eference D etails (Voice / Op tim ization / H ardw are)\n[85] openWakeWord (Open-source wake word detection) https://github.com/dscripka/openWakeWord\n[87] ExLlamaV2 (Fast inference library for modern LLMs) https://github.com/turboderp-org/exllamav2\n[88] llama.cpp Discussion: KV Cache Management https://github.com/ggerganov/llama.cpp/discussions/7949\n[90] llama.cpp Discussion: Prompt Caching https://github.com/ggml-org/llama.cpp/discussions/8860\n[92] Apple MacBook Pro Specs https://www.apple.com/macbook-pro/specs/\n[93] Apple Support: Mac Memory Configurations https://support.apple.com/en-us/111901\n[94] Apple Support: Mac Studio Memory Specs https://support.apple.com/ja-jp/117736\n[97] AMD ROCm Documentation for Windows (Radeon/Ryzen) https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/index.html\nPage 79 | ローカル AI 技術調査レポート 2026 Data Source: Technical Survey Report",
      "heading_level": 3,
      "numbers": [
        "3",
        "4 ",
        "69",
        "102",
        "69",
        "3",
        "2204.08387\n",
        "71",
        "1.0 ",
        "1.0\n",
        "72",
        "73",
        "1111",
        "1111",
        "75",
        "76",
        "1",
        "1",
        "82",
        "4",
        "84",
        "85",
        "87",
        "2 ",
        "2\n",
        "88",
        "7949\n",
        "90",
        "8860\n",
        "92",
        "93",
        "111901\n",
        "94",
        "117736\n",
        "97",
        "79 ",
        "2026 "
      ],
      "hash": "5b1d6e51e1fc8346",
      "meta": {
        "body_font_size": 7.94,
        "body_line_count": 19
      }
    },
    {
      "chunk_id": "document_p080_c00322",
      "block_type": "text",
      "page_no": 80,
      "order": 322,
      "bbox": [
        36.03,
        34.72,
        917.73,
        547.09
      ],
      "text": "References (参考文献リスト 4/4 ) 音声周辺・ベンチマーク・最適化技術・ハードウェア関連 [103] - [136]\n[103\n]\nPhi-3-m ini-128k-instruct M odel C ard https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\n[104\n]\nvLLM: Qua ntized KV Cache https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/\n[105\n]\nO p enAI W hisper https://openai.com/index/whisper/\n[106\n]\nLayoutLMv3: Pre-training for D ocument AI https://arxiv.org/pdf/2204.08387\n[107\n]\nQwen2-V L C ollectio n https://huggingface.co/collections/Qwen/qwen2-vl\n[108\n]\nQwen2-V L-7B-Instruct https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct\n[109\n]\nQwen2.5-C oder-7B-Instruct https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct\n[110\n]\nO lla ma R epo sitory https://github.com/ollama/ollama\n[111\n]\nS tarC od er2 D ocumentation https://huggingface.co/docs/transformers/en/model_doc/starcoder2\n[112\n]\nvLLM: Autom atic Prefix C aching https://docs.vllm.ai/en/v0.9.2/design/automatic_prefix_caching.html\n[113\n]\nS DX L-FP16 Mo del C ard https://huggingface.co/wangkanai/sdxl-fp16\n[114\n]\nTensorRT-LLM : KV Ca che Reus e https://nvidia.github.io/TensorRT-LLM/advanced/kv-cache-reuse.html\n[115\n]\nllam a.cp p Reposito ry https://github.com/ggml-org/llama.cpp\n[116\n]\nG GU F Form at S pecification https://github.com/ggml-org/ggml/blob/master/docs/gguf.md\n[120\n]\nMLX-LM Rep osito ry https://github.com/ml-explore/mlx-lm\n[121\n]\nvLLM S upp orted Platforms https://docs.vllm.ai/en/latest/api/vllm/platforms/\n[122\n]\nN V ID IA TensorRT-LLM https://github.com/NVIDIA/TensorRT-LLM\n[123\n]\nG PTQ: Accurate Po st-Training Qua ntization https://arxiv.org/pdf/2210.17323\n[124\n]\nAWQ : Activatio n-aware W eig ht Q uantiza tion https://arxiv.org/abs/2306.00978\n[125\n]\nAutoAW Q Repos itory https://github.com/casper-hansen/AutoAWQ\n[126\n]\nAutoG PTQ Rep ository https://github.com/AutoGPTQ/AutoGPTQ\n[127\n]\nS moo thQuant: Accurate and Efficient 8 -bit LLMs https://arxiv.org/abs/2211.10438\n[128\n]\nQwen2.5-7B-Instruct https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n[129\n]\nFLUX Rep ository https://github.com/black-forest-labs/flux\n[130\n]\nC hoosing a Res ource S torag e Mode fo r App le GPU s https://developer.apple.com/documentation/metal/...\n[131\n]\nApp le M1 Anno uncement https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/\n[132\n]\nMLX U nified Mem ory Guide https://ml-explore.github.io/mlx/build/html/usage/unified_memory.html\n[133\n]\nC heck if yo ur Mac has unified memory https://support.apple.com/en-us/111901\nPage 80 | ローカル AI 技術調査レポート 2026 Technical Survey Completed.",
      "normalized_text": "References (参考文献リスト 4/4 ) 音声周辺・ベンチマーク・最適化技術・ハードウェア関連 [103] - [136]\n[103\n]\nPhi-3-m ini-128k-instruct M odel C ard https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\n[104\n]\nvLLM: Qua ntized KV Cache https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/\n[105\n]\nO p enAI W hisper https://openai.com/index/whisper/\n[106\n]\nLayoutLMv3: Pre-training for D ocument AI https://arxiv.org/pdf/2204.08387\n[107\n]\nQwen2-V L C ollectio n https://huggingface.co/collections/Qwen/qwen2-vl\n[108\n]\nQwen2-V L-7B-Instruct https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct\n[109\n]\nQwen2.5-C oder-7B-Instruct https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct\n[110\n]\nO lla ma R epo sitory https://github.com/ollama/ollama\n[111\n]\nS tarC od er2 D ocumentation https://huggingface.co/docs/transformers/en/model_doc/starcoder2\n[112\n]\nvLLM: Autom atic Prefix C aching https://docs.vllm.ai/en/v0.9.2/design/automatic_prefix_caching.html\n[113\n]\nS DX L-FP16 Mo del C ard https://huggingface.co/wangkanai/sdxl-fp16\n[114\n]\nTensorRT-LLM : KV Ca che Reus e https://nvidia.github.io/TensorRT-LLM/advanced/kv-cache-reuse.html\n[115\n]\nllam a.cp p Reposito ry https://github.com/ggml-org/llama.cpp\n[116\n]\nG GU F Form at S pecification https://github.com/ggml-org/ggml/blob/master/docs/gguf.md\n[120\n]\nMLX-LM Rep osito ry https://github.com/ml-explore/mlx-lm\n[121\n]\nvLLM S upp orted Platforms https://docs.vllm.ai/en/latest/api/vllm/platforms/\n[122\n]\nN V ID IA TensorRT-LLM https://github.com/NVIDIA/TensorRT-LLM\n[123\n]\nG PTQ: Accurate Po st-Training Qua ntization https://arxiv.org/pdf/2210.17323\n[124\n]\nAWQ : Activatio n-aware W eig ht Q uantiza tion https://arxiv.org/abs/2306.00978\n[125\n]\nAutoAW Q Repos itory https://github.com/casper-hansen/AutoAWQ\n[126\n]\nAutoG PTQ Rep ository https://github.com/AutoGPTQ/AutoGPTQ\n[127\n]\nS moo thQuant: Accurate and Efficient 8 -bit LLMs https://arxiv.org/abs/2211.10438\n[128\n]\nQwen2.5-7B-Instruct https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\n[129\n]\nFLUX Rep ository https://github.com/black-forest-labs/flux\n[130\n]\nC hoosing a Res ource S torag e Mode fo r App le GPU s https://developer.apple.com/documentation/metal/...\n[131\n]\nApp le M1 Anno uncement https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/\n[132\n]\nMLX U nified Mem ory Guide https://ml-explore.github.io/mlx/build/html/usage/unified_memory.html\n[133\n]\nC heck if yo ur Mac has unified memory https://support.apple.com/en-us/111901\nPage 80 | ローカル AI 技術調査レポート 2026 Technical Survey Completed.",
      "heading_level": 3,
      "numbers": [
        "4",
        "4 ",
        "103",
        "136",
        "103\n",
        "3",
        "128",
        "3",
        "128",
        "104\n",
        "105\n",
        "106\n",
        "3",
        "2204.08387\n",
        "107\n",
        "2",
        "2",
        "108\n",
        "2",
        "7",
        "2",
        "7",
        "109\n",
        "2.5",
        "7",
        "2.5",
        "7",
        "110\n",
        "111\n",
        "2 ",
        "2\n",
        "112\n",
        "0.9",
        "2",
        "113\n",
        "16 ",
        "16\n",
        "114\n",
        "115\n",
        "116\n",
        "120\n",
        "121\n",
        "122\n",
        "123\n",
        "2210.17323\n",
        "124\n",
        "2306.00978\n",
        "125\n",
        "126\n",
        "127\n",
        "8 ",
        "2211.10438\n",
        "128\n",
        "2.5",
        "7",
        "2.5",
        "7",
        "129\n",
        "130\n",
        "131\n",
        "1 ",
        "2020",
        "11",
        "1",
        "132\n",
        "133\n",
        "111901\n",
        "80 ",
        "2026 "
      ],
      "hash": "67959cf487d8e489",
      "meta": {
        "body_font_size": 7.92,
        "body_line_count": 29
      }
    }
  ]
}
<!DOCTYPE html><html><head><meta charset="utf-8"><title>Athra Debug: document</title><style>
body { font-family: sans-serif; background: #ecf0f1; margin: 0; padding: 16px; }
h1 { font-size: 1.1em; color: #2c3e50; margin: 0 0 4px 0; }
.meta { font-size: 11px; color: #7f8c8d; margin-bottom: 12px; }
.legend { margin-bottom: 16px; }
.legend span { display: inline-block; padding: 2px 8px; margin: 2px;
               border-radius: 3px; font-size: 11px; font-weight: bold; }
.page-block { margin-bottom: 32px; }
.page-block h2 { font-size: 0.95em; color: #555; margin: 0 0 4px 0; }
.canvas { position: relative; background: white; border: 1px solid #bbb;
          overflow: hidden; box-shadow: 0 1px 4px rgba(0,0,0,.15); }
.bbox { position: absolute; border: 2px solid; box-sizing: border-box;
        opacity: 0.65; cursor: default; transition: opacity .1s; }
.bbox:hover { opacity: 1; z-index: 50; }
.lbl { font-size: 9px; font-weight: bold; padding: 1px 4px;
       display: inline-block; white-space: nowrap; }
</style></head><body><h1>Athra PDF Debug — document</h1><p class="meta">Source: input/テスト.pdf | Pages: 80 | Chunks: 322</p><div class="legend">Legend: <span style="background:#5dade2;border:1px solid #999;">body</span><span style="background:#e74c3c;border:1px solid #999;">h1</span><span style="background:#e67e22;border:1px solid #999;">h2</span><span style="background:#f1c40f;border:1px solid #999;">h3</span></div><div class="page-block"><h2>Page 1 (4 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:301.3px;top:398.1px;width:165.4px;height:9.0px;border-color:#5dade2;" title="document_p001_c00001 [body]
2026 年 2 月 15 日 | Generated by Genspark AI Slides"><span class="lbl" style="background:#5dade2;">body</span></div><div class="bbox level-3" style="left:163.4px;top:111.5px;width:441.0px;height:65.3px;border-color:#f1c40f;" title="document_p001_c00002 [h3]
Technical Research Report ローカル AI 技術調査レポート"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:153.7px;top:189.3px;width:460.5px;height:67.9px;border-color:#f1c40f;" title="document_p001_c00003 [h3]
オンデバイス/ローカル推論の現実解と推奨スタック
LLM VLM ASR TTS RAG Apple Silicon Windows GPU"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:159.7px;top:299.3px;width:427.7px;height:39.9px;border-color:#f1c40f;" title="document_p001_c00004 [h3]
Report Purpose
Apple Silicon および Windows 環境におけるローカル AI の実運用ラインと推奨スタックを提示。 「意思決定のための要点」と「根拠となる完全な技術詳細」を二層構造で網羅。"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 2 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:24.0px;top:15.7px;width:713.8px;height:95.7px;border-color:#f1c40f;" title="document_p002_c00005 [h3]
このスライドの読み方 概要 → 詳細の二層構造ガイド
構造:二層設計"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:59.4px;top:147.6px;width:298.4px;height:38.0px;border-color:#f1c40f;" title="document_p002_c00006 [h3]
1. 概要スライド( Executive Summary )
意思決定に必要な「結論」と「要点」を最初に提示します。時間がな い場合はここだけ読めば全体像が掴めます。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:59.4px;top:98.7px;width:666.3px;height:326.4px;border-color:#f1c40f;" title="document_p002_c00007 [h3]
2. 完全版スライド( Full Detail )
レポート内の表、数値、グラフ、注釈を省略せずに掲載します。エン ジニアや実装担当者が参照するための詳細情報です。
相互リンクと参照
概要から詳細へ、詳細から参考文献へ、論理的に接続されています 。
設計原則と表記
省略しない
元の技術レポートに含まれる情報は、脚注や URL に至るまで全てスラ イド内に保持します。
根拠の明示
「推測」と「事実("><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 3 (14 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:30.0px;top:21.0px;width:708.6px;height:40.5px;border-color:#f1c40f;" title="document_p003_c00008 [h3]
Local AI Technical Report 目次( Agenda ) Total 13 Chapters"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:37.2px;top:93.8px;width:201.4px;height:23.1px;border-color:#f1c40f;" title="document_p003_c00009 [h3]
01 タイトル/目的/読み方
レポートの目的と、概要 → 詳細の二層構造の活用方法。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:37.2px;top:141.5px;width:187.9px;height:22.3px;border-color:#f1c40f;" title="document_p003_c00010 [h3]
02 エグゼクティブサマリ
実務要点、中核技術、推奨スタックの概要を提示。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:37.2px;top:189.3px;width:177.9px;height:23.1px;border-color:#f1c40f;" title="document_p003_c00011 [h3]
03 ローカル AI の定義・前提
オンデバイス/ローカル LAN 推論の定義と範囲。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:37.2px;top:237.0px;width:188.3px;height:23.1px;border-color:#f1c40f;" title="document_p003_c00012 [h3]
04 ローカル AI ランドスケープ
LLM から Agent まで、技術スタックの全体像を図解。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:37.2px;top:284.7px;width:195.6px;height:23.1px;border-color:#f1c40f;" title="document_p003_c00013 [h3]
05 特徴軸(評価軸)の定義
指示追従、 JSON 堅牢性、メモリ効率などの評価基準。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:37.2px;top:332.5px;width:171.1px;height:23.1px;border-color:#f1c40f;" title="document_p003_c00014 [h3]
06 カテゴリ別モデルカタログ
LLM/VLM/ASR/TTS 等の代表モデルと採用判断。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:37.2px;top:380.2px;width:181.2px;height:23.1px;border-color:#f1c40f;" title="document_p003_c00015 [h3]
07 メモリ設計のコア
重み理論値、 KV キャッシュ、最適化技術の詳解。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:403.4px;top:93.8px;width:190.4px;height:23.1px;border-color:#f1c40f;" title="document_p003_c00016 [h3]
08 Tier 定義と “ 現実ライン ”
Apple Silicon / Windows GPU / CPU ごとの実用ライン。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:403.4px;top:141.5px;width:205.6px;height:23.1px;border-color:#f1c40f;" title="document_p003_c00017 [h3]
09 ユースケース別推奨スタック
低コスト/高品質/ハード制約別の構成案( 6 ケース)。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:403.4px;top:189.3px;width:198.3px;height:23.1px;border-color:#f1c40f;" title="document_p003_c00018 [h3]
10 測定と比較の方法
再現可能なベンチマーク手順と指標( tok/s, TTFT 等)。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:403.4px;top:237.0px;width:187.9px;height:22.3px;border-color:#f1c40f;" title="document_p003_c00019 [h3]
11 リスク/コンプライアンス
ライセンス条件、商用利用、セキュリティリスク。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:403.4px;top:284.7px;width:187.9px;height:22.3px;border-color:#f1c40f;" title="document_p003_c00020 [h3]
12 まとめ(意思決定フロー)
モデル選定と環境構築のための意思決定チャート。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-2" style="left:52.8px;top:333.6px;width:695.2px;height:86.1px;border-color:#e67e22;" title="document_p003_c00021 [h2]
13 References
一次情報源および参照 URL 一覧。
各章は「概要(要点)」 → 「詳細(完全版)」の順で構成されています
Page 3"><span class="lbl" style="background:#e67e22;">h2</span></div></div></div><div class="page-block"><h2>Page 4 (4 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:36.0px;top:29.2px;width:598.7px;height:241.7px;border-color:#f1c40f;" title="document_p004_c00022 [h3]
エグゼクティブサマリ(実務要点)
ローカル AI 導入における意思決定の重要ポイント
結論:ローカル AI は「 4bit 量子化+ KV 管理」を前提に、 GGUF/llama.cpp 系と Ollama/LM Studio/MLX で実務化可能です。
定義とスコープ
推論が端末 or ローカル LAN 内で完結
入力データは外部へ送信されない
LAN サーブ含む( LM Studio, Oll"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:75.0px;top:242.1px;width:550.6px;height:88.7px;border-color:#f1c40f;" title="document_p004_c00023 [h3]
Ollama / LM Studio : UI + API
MLX : Apple Silicon 特化
ハードウェア実用ライン (4bit 量子化前提 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:48.6px;top:343.1px;width:133.2px;height:46.8px;border-color:#f1c40f;" title="document_p004_c00024 [h3]
Apple Silicon (Unified Memory)
16GB : 3B 〜 7/8B 級
24-32GB : 7B 〜 14B 級 ( 業務最小ライン )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:259.9px;top:319.2px;width:472.8px;height:111.6px;border-color:#f1c40f;" title="document_p004_c00025 [h3]
Windows (NVIDIA GPU VRAM)
8GB : 7B 級 / 12GB : 14B 級
24GB : 27B 〜 34B 級 ( 実務域 )
推奨スタック
低コスト : GGUF + llama.cpp
高品質 : GPU + TensorRT-LLM
ハード制約 : 3B 級 + RAG 工夫
補足:長文コンテキストは KV キャッシュがメモリを支配します。最終判断は再現ベンチマ"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 5 (2 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:19.4px;width:686.2px;height:112.1px;border-color:#f1c40f;" title="document_p005_c00026 [h3]
エグゼクティブサマリ(詳細 1 ) ローカル AI の定義と中核技術
結論:ローカル AI は「推論がユーザー管理下で完結する構成」と定義され、 重み量子化( 4bit ) と KV キャッシュ最適化 が実運用の技術的基盤です。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:55.1px;top:193.0px;width:668.0px;height:204.8px;border-color:#f1c40f;" title="document_p005_c00027 [h3]
ローカル AI の定義と射程
ユーザー端末(オンデバイス)、 PC 、またはローカル LAN 内サーバで完 結。
入力データ(文書・音声・画像)がデフォルトでクラウドへ送出されな い構成。
LM Studio や Ollama の「 localhost/network 公開」機能を含みます。
[1][2]
Localhost On-Premise LAN Privacy First
中核技術( E"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 6 (4 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:44.4px;top:16.3px;width:695.6px;height:313.3px;border-color:#f1c40f;" title="document_p006_c00028 [h3]
エグゼクティブサマリ(詳細 2 ) 実運用ランタイムとハードウェア実用ライン
結論:ランタイムは llama.cpp/GGUF 系・ MLX が第一選択となり、ハードウェアはメモリ容量で Tier 化されます(長文は KV 支配)。
実運用ランタイム(第一選択)
クロスプラットフォーム標準: llama.cpp
GGUF 形式必須。最小セットアップで CPU/GPU 推論が可能。
軽量・汎用で多く"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:416.6px;top:164.8px;width:202.1px;height:80.7px;border-color:#f1c40f;" title="document_p006_c00029 [h3]
ハードウェア実用ライン( 4bit 中心概算)
Apple Silicon (Unified Memory):
3B 〜 7/8B 級(インタラクティブ) 16GB
7B 〜 14B 級(業務実用・ RAG 最小ライン) 24-32GB
14B 〜 32B 級以上(高品質・長文要約可) 64GB+"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:417.8px;top:256.2px;width:185.0px;height:53.8px;border-color:#f1c40f;" title="document_p006_c00030 [h3]
Windows (Discrete GPU):
7B 級(最小ライン) VRAM 8GB
14B 級〜 27B 級(品質寄り) VRAM 12-16GB
32-34B 級・重い画像生成が実務域 VRAM 24GB"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:53.4px;top:320.9px;width:677.9px;height:85.9px;border-color:#f1c40f;" title="document_p006_c00031 [h3]
Windows CPU-only:
7B 級が「使える」境界( AVX 活用)。 32GB RAM+
注意:長文コンテキストとメモリ
上記は重み( weight-only )の概算です。長文( Long Context )を扱う場合、 KV キャッシュがメモリを線形に圧迫するため、別途 KV 量子化やコンテキスト長制限( 4k/8k 等)の設計が必要です。"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 7 (13 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:17.0px;width:681.9px;height:153.7px;border-color:#f1c40f;" title="document_p007_c00032 [h3]
エグゼクティブサマリ(詳細 3 ) 推奨スタック 3 パターン( Tier 別構成案)
結論:用途とリソースに応じて「低コスト/高品質/ハード制約」の 3 構成を選択します。 ※各構成の具体的な失敗モードと回避策は、後述の「ユースケース別推奨スタック」章で詳細に展開します。
低コスト構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:198.9px;width:176.4px;height:27.8px;border-color:#f1c40f;" title="document_p007_c00033 [h3]
Target HW
CPU のみ / エントリー GPU / Apple 16GB"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:242.5px;width:178.8px;height:33.6px;border-color:#f1c40f;" title="document_p007_c00034 [h3]
LLM Runtime
llama.cpp (GGUF Q4/K) Ollama / LM Studio"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:298.7px;width:179.2px;height:34.1px;border-color:#f1c40f;" title="document_p007_c00035 [h3]
ASR / TTS
faster-whisper (INT8) Piper / Kokoro ( 軽量 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:158.6px;width:385.3px;height:278.3px;border-color:#f1c40f;" title="document_p007_c00036 [h3]
RAG / Embedding
BGE-M3 ( 多言語・多用途 ) bge-reranker-base
最低限の業務自動化
高品質構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:198.9px;width:181.1px;height:34.1px;border-color:#f1c40f;" title="document_p007_c00037 [h3]
Target HW
GPU 搭載機 (VRAM 16-24GB+) Apple 64GB+"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:255.1px;width:197.9px;height:34.1px;border-color:#f1c40f;" title="document_p007_c00038 [h3]
LLM Runtime
GPU 優先 (TensorRT-LLM / vLLM) 32B 〜 70B 級モデル"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:311.3px;width:186.9px;height:34.1px;border-color:#f1c40f;" title="document_p007_c00039 [h3]
ASR / TTS
faster-whisper (GPU) XTTS / StyleTTS2 ( 高品質 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:158.6px;width:397.6px;height:278.3px;border-color:#f1c40f;" title="document_p007_c00040 [h3]
RAG / VLM
bge-reranker-large Qwen2-VL 7B / InternVL2
リッチな体験・高精度
ハード制約構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:198.9px;width:183.8px;height:34.1px;border-color:#f1c40f;" title="document_p007_c00041 [h3]
Target HW
低スペック PC / 古い Mac メモリ 8GB 以下等"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:255.1px;width:180.4px;height:34.1px;border-color:#f1c40f;" title="document_p007_c00042 [h3]
LLM Runtime
Phi-3 mini (3.8B) 等 GGUF Q4 ( 極小モデル )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:311.3px;width:201.2px;height:38.3px;border-color:#f1c40f;" title="document_p007_c00043 [h3]
Strategy
短いコンテキストで完結させる RAG は Embedding 検
索を重視"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:367.5px;width:203.0px;height:69.4px;border-color:#f1c40f;" title="document_p007_c00044 [h3]
Compromise
LLM 自体を軽くし、検索精度で補う生成タスクを限
定する
極限環境での動作"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 8 (2 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:60.0px;top:17.6px;width:663.4px;height:21.1px;border-color:#f1c40f;" title="document_p008_c00045 [h3]
ローカル AI の定義とスコープ Definition &amp; Scope"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:52.9px;top:95.8px;width:672.8px;height:343.6px;border-color:#f1c40f;" title="document_p008_c00046 [h3]
DEFINITION
本資料の「ローカル AI 」とは、推論がユーザー管理下(端末/ローカル PC /ローカル LAN )で完結し、 入力データが外部へ送信されない構成を指します。
スコープの射程(範囲)
ローカル API サーバを含む : localhost だけでなく、 LAN 公開( network )された推論サーバも対象 。
LM Studio / Ollama
実装例 : Window"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 9 (2 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:49.8px;top:20.3px;width:680.4px;height:107.7px;border-color:#5dade2;" title="document_p009_c00047 [body]
未指定事項の扱い(前提条件) オフライン要件・対象 OS ・ライセンス
結論:「完全オフライン」は推論実行時のみを必須要件とし、 導入・更新時のネットワーク利用は許容する現実的な設計を前提とします。"><span class="lbl" style="background:#5dade2;">body</span></div><div class="bbox level-3" style="left:28.8px;top:186.6px;width:709.8px;height:242.9px;border-color:#f1c40f;" title="document_p009_c00048 [h3]
オフライン要件と対象 OS
“ 完全オフライン ” の定義: 「モデル推論自体はオフラインで成立する」レベルを基本とします。モ デルの初回ダウンロードや RAG 文書の取り込みプロセスにはネットワー ク接続が必要になり得る点を前提とします。
[19]
対象 OS のスコープ: macOS Ventura 以降および Windows 10/11 を主要ターゲットとします。 Linux は「 Wind"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 10 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-1" style="left:48.0px;top:90.2px;width:79.9px;height:80.4px;border-color:#e74c3c;" title="document_p010_c00049 [h1]
04"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:48.0px;top:209.9px;width:231.7px;height:118.1px;border-color:#e74c3c;" title="document_p010_c00050 [h1]
ローカル AI ランドスケープ
オンデバイス実行環境の 全体像と技術スタック"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-3" style="left:393.8px;top:69.4px;width:312.7px;height:290.5px;border-color:#f1c40f;" title="document_p010_c00051 [h3]
KE Y TAKEAWAYS
本章の要点
テキスト LLM の二大潮流:
GGUF ( llama.cpp 系)と GPU 量子化形式( GPTQ/AWQ/EXL2 等 )に大別されます。
プラットフォームの最適解:
macOS は Metal/Unified Memory 、 Windows は Ollama/LM Studio が実務的選択肢です。
構成要素の多様性:
LLM だけでなく、 A"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 11 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:28.8px;top:15.5px;width:711.4px;height:410.0px;border-color:#f1c40f;" title="document_p011_c00052 [h3]
ローカル AI ランドスケープ カテゴリ網羅・代表モデルとランタイムの関係図
Local AI Technical Survey Report 2026 11 / 80
結論:ローカル AI は 9 カテゴリ( LLM/VLM/ASR/TTS/Embedding/OCR/ 画像生成 /Agent/ 音声前処理)で構成され、 モデル系とランタイム系が実務上の結節点となります。
Core (AI Ca"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 12 (8 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:18.5px;width:693.2px;height:84.8px;border-color:#f1c40f;" title="document_p012_c00053 [h3]
ローカル AI ランドスケープ(詳細) カテゴリ別代表モデル・ランタイム・主用途
結論:実務上の収束点は「 LLM は GGUF (llama.cpp) か GPU 量子化系 」「 macOS は MLX/Unified Memory 最適化」「 Windows は Ollama/LM Studio + 必要に 応じ WSL2 」。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:37.8px;top:147.0px;width:190.4px;height:70.1px;border-color:#f1c40f;" title="document_p012_c00054 [h3]
テキスト LLM
形式 : GGUF (llama.cpp 必須 ), GPTQ/AWQ/EXL2 (GPU 向け ) [21]
代表 : Llama 3.1, Qwen2.5, Gemma 2, Phi-3/4
用途 : 汎用対話 , 要約 , RAG, 翻訳"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:278.0px;top:147.4px;width:167.0px;height:59.0px;border-color:#f1c40f;" title="document_p012_c00055 [h3]
VLM / LMM
代表 : Qwen2-VL, Phi-3-Vision, LLaVA
実行 : MLX-VLM (Mac 最適化 ), Transformers, vLLM [28]
用途 : 画像理解 , OCR 代替 , UI 解析"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:518.1px;top:147.0px;width:198.5px;height:59.3px;border-color:#f1c40f;" title="document_p012_c00056 [h3]
ASR (音声認識)
方式 : Whisper 系 (faster-whisper, whisper.cpp) [50] [51]
特徴 : CTranslate2 で高速・省メモリ , C++ 軽量実装
用途 : 議事録 , 音声コマンド , 文字起こし"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:37.8px;top:251.3px;width:188.4px;height:59.4px;border-color:#f1c40f;" title="document_p012_c00057 [h3]
TTS (音声合成)
軽量 : Piper / Kokoro ( 高速・ CPU 向け ) [52][53]
高品質 : XTTS / StyleTTS2 ( クローン・ GPU 推奨 ) [54][55]
用途 : 読み上げ , 通知 , ボイスボット"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:278.0px;top:251.6px;width:186.8px;height:59.0px;border-color:#f1c40f;" title="document_p012_c00058 [h3]
Embedding/Reranker
代表 : bge-m3, multilingual-e5, bge-reranker [58-62]
重要性 : RAG の検索精度( Recall/Precision )を決定
用途 : ベクトル検索 , 再ランキング"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:37.8px;top:251.6px;width:645.1px;height:152.4px;border-color:#f1c40f;" title="document_p012_c00059 [h3]
OCR / Document AI
古典 : Tesseract / PaddleOCR ( 構造化 ) [64][66]
AI 型 : Donut / LayoutLMv3 ( 文書理解 ) [68][69]
用途 : PDF 解析 , 請求書読取 , スキャン処理
画像生成
代表 : SDXL, FLUX.1 ( 要 VRAM 確認 ) [71][75]
実行 : ComfyUI, SD We"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:344.7px;width:708.7px;height:82.9px;border-color:#f1c40f;" title="document_p012_c00060 [h3]
Agent / Tool-use
方式 : OpenAI 互換 API での Function Calling [24]
実行 : Ollama / LM Studio が実務的 [23][41]
用途 : タスク自動化 , 外部ツール連携
音声前処理
代表 : Silero VAD, pyannote, RNNoise [82][84]
役割 : 品質の下限担保(無音除去・話者分離)
用途 : "><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 13 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-1" style="left:48.0px;top:71.7px;width:79.9px;height:80.4px;border-color:#e74c3c;" title="document_p013_c00061 [h1]
05"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-3" style="left:48.0px;top:193.7px;width:231.7px;height:152.8px;border-color:#f1c40f;" title="document_p013_c00062 [h3]
特徴軸(評価軸 ) の定義
評価の共通物差しと 測定基準"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:393.8px;top:67.0px;width:312.9px;height:296.2px;border-color:#f1c40f;" title="document_p013_c00063 [h3]
KE Y TAKEAWAYS
本章の要点
13 の評価軸:
指示追従、 JSON 堅牢性、長文耐性、速度、メモリなど、多 角的な視点でモデルを比較評価します。
KV キャッシュ最適化:
paged/quantized KV や reuse 機能が、長文コンテキストや多同 時リクエスト処理の鍵となります。
JSON 制約の重要性:
ツール利用において JSON 出力の安定性は必須ですが、機能 はラン"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 14 (2 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:30.0px;top:18.0px;width:108.7px;height:8.1px;border-color:#5dade2;" title="document_p014_c00064 [body]
Evaluation Metrics Overview"><span class="lbl" style="background:#5dade2;">body</span></div><div class="bbox level-3" style="left:30.0px;top:36.3px;width:717.4px;height:386.5px;border-color:#f1c40f;" title="document_p014_c00065 [h3]
特徴軸(評価軸)の定義 Total 13 Metrics
01 指示追従 (Instruction Following) プロンプトの制約・禁止事項・形式指定(箇条書き禁止等)を遵守する能力。
02 幻覚耐性 (Hallucination Resistance) 根拠のない固有名詞や数値を捏造せず、不確実性を提示できる性質。
03 JSON 堅牢性 (Structured Output) スキーマ"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 15 (6 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:24.0px;top:15.7px;width:712.0px;height:16.7px;border-color:#5dade2;" title="document_p015_c00066 [body]
特徴軸(詳細):測定と設計含意 13 の評価軸における計測手順と技術的背景"><span class="lbl" style="background:#5dade2;">body</span></div><div class="bbox level-3" style="left:72.6px;top:87.1px;width:119.8px;height:12.1px;border-color:#f1c40f;" title="document_p015_c00067 [h3]
機能・性能評価のコア"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:54.6px;top:127.9px;width:308.1px;height:161.5px;border-color:#f1c40f;" title="document_p015_c00068 [h3]
JSON 堅牢性と指示追従
ツール呼び出し等の業務連携ではスキーマ破壊が致命的。 Ollama/LM Studio の OpenAI 互換 API を用いて同一条件で比較検証を行う。 llama-cpp-python 等のランタイム依存機 能( response_format )も活用。
[24]
[25]
長文耐性とメモリ管理
コンテキスト長に応じて KV キャッシュメモリは線形に増加し、支配的"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:54.6px;top:307.1px;width:308.5px;height:37.0px;border-color:#f1c40f;" title="document_p015_c00069 [h3]
画像理解 (VLM)
視覚トークン数が増加すると速度・メモリ効率が急落するため、解像度や max-pixels 制 御が重要。 [28]"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:441.8px;top:87.1px;width:119.8px;height:12.1px;border-color:#f1c40f;" title="document_p015_c00070 [h3]
技術的詳細と設計含意"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:322.5px;top:127.9px;width:410.0px;height:297.3px;border-color:#f1c40f;" title="document_p015_c00071 [h3]
KV キャッシュ最適化技術
vLLM は Paged KV Cache 設計でメモリ断片化を防ぎ、 KV キャッシュを FP8 等に量子化して フットプリントを削減。 NVIDIA TensorRT-LLM も KV reuse (再利用)を最適化として強調 。長文・多同時接続時の鍵となる。
[5][6]
[7]
量子化耐性と相性
4bit 化による品質劣化はモデルと量子化方式( AWQ, GPT"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 16 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-1" style="left:48.0px;top:90.2px;width:79.9px;height:80.4px;border-color:#e74c3c;" title="document_p016_c00072 [h1]
06"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-3" style="left:48.0px;top:209.9px;width:231.7px;height:118.1px;border-color:#f1c40f;" title="document_p016_c00073 [h3]
カテゴリ別 モデルカタログ
主要 9 カテゴリの 詳細データと採用判断"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:393.8px;top:63.4px;width:307.4px;height:303.4px;border-color:#f1c40f;" title="document_p016_c00074 [h3]
SECTION OVERVIEW
提示方針:三層構造
概要( Overview ):
カテゴリごとの主要トレンドと代表モデルの要点を概説。
完全版表( Full Data ):
規模・量子化・得意不得意・ランタイム相性・リスク・参 考文献を列落ちなく全掲載。
採用判断( Decision ):
「どう選ぶか」の意思決定基準と失敗回避策を提示。
テキスト LLM VLM/LMM ASR TTS Em"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 17 (2 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-1" style="left:49.2px;top:20.3px;width:684.8px;height:294.1px;border-color:#e74c3c;" title="document_p017_c00075 [h1]
テキスト LLM (汎用)概要 主要 7 モデルと実運用環境
結論:ローカル実務のボリュームゾーンは 7B 〜 14B ( 4bit ) 。 形式は GGUF ( llama.cpp 系)か GPU 量子化( GPTQ/AWQ/EXL2 )の二択が現実解です。
代表モデル群(要点)
Llama 3.1 Instruct: 8B/70B/405B 系。多言語対話最適化を明記。 [29-31]
Qwe"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-3" style="left:28.8px;top:173.2px;width:709.2px;height:252.2px;border-color:#f1c40f;" title="document_p017_c00076 [h3]
実行環境と形式( Runtime &amp; Format )
クロスプラットフォーム標準。 CPU/GPU ハイブリッド実行が可能。 GGUF 形式が必須。
[21][116]
MLX / MLX-LM による変換・実行、または GGUF (llama.cpp) が最適。 Unified Memory を活用 。 [22][120]
vLLM / ExLlamaV2 / AutoGPTQ 等。 GPU "><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 18 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:691.5px;height:244.6px;border-color:#f1c40f;" title="document_p018_c00077 [h3]
テキスト LLM (完全版表 1 ) 主要モデル詳細比較: Llama 3.1 / Qwen2.5 / Gemma 2 / Mixtral
代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断
Llama 3.1 Instruct Meta [29-31]
8B 70B 405B
4bit 中心 (GGUF 等 ) ※方式は環境依存
得意 多言語対"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:306.2px;width:690.3px;height:27.4px;border-color:#f1c40f;" title="document_p018_c00078 [h3]
Gemma 2 Google [35]
2B 9B 27B
2B/9B はローカル向き 27B は VRAM/ メモリ要求 増
得意 モデルカード更新日が明示されて おり、責任ある利用を前提に整理さ れている [35]
Mac: MLX / GGUF 変換 Win: GGUF / 各 GPU
Risk “ 事実質問用途での誤生成 成 ” は一般に起こり得る(モ モデル一般リスク)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:366.7px;width:704.4px;height:58.4px;border-color:#f1c40f;" title="document_p018_c00079 [h3]
Mixtral 8x7B Mistral [36-37]
MoE (8×7B)
GPU 向けで真価が出やす い (量子化 / 実装依存)
得意 「 Llama2 70B を多くのベンチで上 上回る」等をモデル説明で明記 [37]
Win GPU: 適合ランタイム( vLLM 等)前提 Mac: 難易度高め
Risk MoE は実装・ VRAM ・スル ループットのブレが大きい
採用判断の要点:ロー"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 19 (4 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:685.8px;height:95.1px;border-color:#f1c40f;" title="document_p019_c00080 [h3]
テキスト LLM (完全版表 2 ) 主要モデル詳細比較: Phi-3 Mini / Phi-4 / gpt-oss
代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:156.8px;width:690.2px;height:26.6px;border-color:#f1c40f;" title="document_p019_c00081 [h3]
Phi-3 Mini Microsoft [38]
3.8B 4bit でローカル適性高い 長文版 (128K) 運用は KV が 支配
得意 「 128K 文脈」をうたうが長文はメ メモリ設計前提 [38]
Mac/Win: 比較的回しやすい( 形式依存)
Risk “ 小さい=万能 ” ではなく く、専門領域は RAG 前提で補 補う判断"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:218.7px;width:690.5px;height:27.4px;border-color:#f1c40f;" title="document_p019_c00082 [h3]
Phi-4 Microsoft [39-40]
14B 4bit でローカル上位ライ ン ( 32GB 級以上推奨)
得意 データ品質重視・合成データ活用 を明示 [40]
Win GPU: 最適化次第 Mac: MLX / 変換次第
Risk 14B は “ 重いが現実的 ” の境界。
の境 KV/ 長文設計が重要"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:280.6px;width:705.4px;height:144.6px;border-color:#f1c40f;" title="document_p019_c00083 [h3]
gpt-oss OpenAI [41]
20B 120B
MXFP4 量子化で出荷を明 明記 (他量子化なし)
得意 ツール呼び出し・ローカル API 利 用までガイドあり [27]
Mac/Win: LM Studio / Ollama で 手順が提示される [41]
Risk 20B は 16GB 以上、 120B は
は 60GB 以上推奨を明記 [41]
採用判断の要点: 14B クラス("><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 20 (6 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:15.5px;width:693.2px;height:409.7px;border-color:#f1c40f;" title="document_p020_c00084 [h3]
採用判断基準(テキスト LLM ) 選定フローと実務上の重要ポイント
結論:モデル選定は「必要品質 → 許容レイテンシ → 許容メモリ → 形式 → ランタイム」の順に行うのが、実務上最も破綻しにくい フローです。
実務の現実ライン
7B 〜 14B がボリュームゾーン
一般業務や RAG において、品質と速度のバランスが良いスイー トスポット。
3B 級はハード制約モード
メモリ不足時のフォール"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-1" style="left:67.3px;top:154.5px;width:70.3px;height:64.4px;border-color:#e74c3c;" title="document_p020_c00085 [h1]
1
必要品質
タスク難易度でサイズ決
定 ( 例 : 70B vs 8B)"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:215.4px;top:158.8px;width:55.9px;height:56.1px;border-color:#e74c3c;" title="document_p020_c00086 [h1]
2
許容レイテンシ
リアルタイム性か
バッチ処理か"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:352.5px;top:158.8px;width:63.2px;height:56.8px;border-color:#e74c3c;" title="document_p020_c00087 [h1]
3
許容メモリ
4bit 量子化前提で VRAM/RAM に収まるか"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:498.0px;top:158.8px;width:54.3px;height:56.1px;border-color:#e74c3c;" title="document_p020_c00088 [h1]
4
形式選定
GGUF or GPTQ/AWQ
要件先行で決定"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:633.9px;top:158.8px;width:64.0px;height:56.8px;border-color:#e74c3c;" title="document_p020_c00089 [h1]
5
ランタイム
形式に合うものを選択
(llama.cpp / vLLM 等 )"><span class="lbl" style="background:#e74c3c;">h1</span></div></div></div><div class="page-block"><h2>Page 21 (5 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:49.8px;top:19.4px;width:685.5px;height:163.0px;border-color:#f1c40f;" title="document_p021_c00090 [h3]
VLM/LMM 概要(画像理解・マルチモーダル) カテゴリ別モデルカタログ
結論:ローカル運用の現実ラインは 2B/7B 級 が中心。 画像トークン化の前処理依存が強く、ランタイムの明示サポートが重要です。
代表モデルと特徴"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:58.2px;top:199.6px;width:286.8px;height:23.0px;border-color:#f1c40f;" title="document_p021_c00091 [h3]
Qwen2-VL (2B/7B/72B) [42]
ローカル現実ラインの主力。 2B/7B が実用的。 72B は上位ハード( 96GB+ )前提。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:58.2px;top:172.0px;width:490.8px;height:190.9px;border-color:#f1c40f;" title="document_p021_c00092 [h3]
Phi-3-Vision (128K) [43]
軽量マルチモーダル+長文対応を標榜。テキスト重視の画像理解に適正。
InternVL2 (4B 等 ) [44-45]
画像理解の系列として設計。モデル群が複数サイズ展開。
LLaVA (7B/13B 系 ) [46-47]
画像チャット用途の先駆者。 4bit 運用で 12GB VRAM でも動作可能。
ランタイム相性と技術課題"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:420.8px;top:199.6px;width:246.7px;height:32.8px;border-color:#f1c40f;" title="document_p021_c00093 [h3]
macOS / Apple Silicon
MLX-VLM: Qwen2-VL 等の利用例を具体コマンドで提示。
Unified Memory を活用し、システムメモリで大型モデルを実行可能。
[28]"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:54.6px;top:254.3px;width:675.8px;height:175.6px;border-color:#f1c40f;" title="document_p021_c00094 [h3]
Windows / GPU
transformers / vLLM: GPU 推論が主流。モデル形式( Safetensors/AWQ 等)とランタイム の対応に依存。
技術的ボトルネック
画像トークンが増えるとメモリ / 速度が急落。解像度制御や max-pixels 設定が運用の肝と なる。
Visual Tokens Multi-modal RAG Resolution Control
設計"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 22 (5 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:676.0px;height:96.9px;border-color:#f1c40f;" title="document_p022_c00095 [h3]
VLM/LMM (完全版表) Qwen2-VL / Phi-3-Vision / InternVL2 / LLaVA 詳細比較
代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:160.3px;width:686.8px;height:27.4px;border-color:#f1c40f;" title="document_p022_c00096 [h3]
Qwen2-VL Qwen [42]
2B 7B 72B
2B/7B がローカル現実ライン 72B は上位ハード前提
得意 画像+テキストの汎用 画像トークン増でメモリ / 速度急 落 → 解像度制御が肝
Mac: MLX-VLM が利用例を明 明示 [28] Win: transformers / vLLM
Risk 視覚トークン肥大による リソース枯渇"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:225.7px;width:687.6px;height:27.4px;border-color:#f1c40f;" title="document_p022_c00097 [h3]
Phi-3-Vision Microsoft [43]
4.2B (128K)
形式依存でローカル適性あり 128K 長文対応
得意 軽量マルチモーダル+長文を 標榜 “ 実務的な OCR 読取 ” に強み
Mac: MLX 変換次第 Win: GPU 推論
Risk 長文は KV が支配しやすく 、メモリ管理が必須 [43]"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:291.1px;width:686.8px;height:27.4px;border-color:#f1c40f;" title="document_p022_c00098 [h3]
InternVL2 OpenGVLab [44-45]
1B/2B 4B/8B 等
4B 等がローカル向き モデル群が複数サイズ展開
得意 画像理解の系列として設計 構成要素をモデルカードで詳細説 明 [45]
Win: GPU / 形式次第 Mac: 実装依存
Risk エコシステム差(変換・ プロセッサ依存)が運用の 難所"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:356.4px;width:704.7px;height:68.7px;border-color:#f1c40f;" title="document_p022_c00099 [h3]
LLaVA Community [46-47]
7B 13B
4bit での運用示唆 13B で 12GB VRAM 動作可 [47]
得意 画像チャット用途 Win GPU で構築しやすい
Win: GPU 推論で実績多 Mac: llama.cpp 等
Risk ベース LLM や実装差が大 きく、モデルカードとの整 合確認が必要 採用判断の要点:画像理解はテキスト LLM より前処理・プロセ"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 23 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:15.5px;width:693.1px;height:378.1px;border-color:#f1c40f;" title="document_p023_c00100 [h3]
採用判断基準( VLM/LMM ) ランタイム適合性と運用設計
結論:画像理解は前処理・プロセッサ依存が強いため、「ランタイムの明示サポート確認」が最優先です。 2B/7B 級を基本とし 、不足分を RAG/OCR で補うのが安全策です。
ランタイム適合性
一次情報の確認
ランタイムがモデルを明示サポートしているか確認が安全( 例 : MLX-VLM の Qwen2-VL 対応コマンド提示)。
プ"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:288.3px;top:276.5px;width:202.2px;height:117.1px;border-color:#f1c40f;" title="document_p023_c00101 [h3]
運用設計( 2B/7B 級)
現実的なライン
72B 級は 96GB+ メモリや複数 GPU が必要なため、実務では 2B/7B 級での運用設計に寄せる。
視覚トークン制御
長文・多画像時は KV 支配とトークン肥大に注意。 max-pixels/ マ ルチ画像数を制御する。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:86.4px;top:186.1px;width:641.9px;height:239.0px;border-color:#f1c40f;" title="document_p023_c00102 [h3]
補完戦略( OCR/RAG )
不得意のカバー
微細な文字認識や構造化は OCR ( PaddleOCR 等)に任せ、 VLM は「意味理解」に集中させる。
ハイブリッド構成
画像から情報をテキスト抽出し、 Embedding 化して RAG で検索 可能にする設計が堅牢。
Page 23 | ローカル AI 技術調査レポート
サポート確認
README 等でランタイム (MLX/vLLM) の対"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 24 (2 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-1" style="left:46.9px;top:18.8px;width:688.4px;height:106.3px;border-color:#e74c3c;" title="document_p024_c00103 [h1]
ASR (音声認識)概要 カテゴリ別モデルカタログ
結論:ローカル ASR は「 RTF ・メモリ・誤転記」の同時最適化が必須。 faster-whisper を基準実装とし、速度と精度(幻覚抑制)のバランスを図ります。"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-3" style="left:43.8px;top:176.8px;width:684.2px;height:244.6px;border-color:#f1c40f;" title="document_p024_c00104 [h3]
Whisper 系実装の比較
実装名 特徴・最適化 推奨用途
Whisper (Original) [48] 研究 / 公開ベース、 PyTorch 依存 精度検証・基準
faster-whisper [50] CTranslate2 、 8bit 量子化、最大 4 倍速 実務の第一選択
whisper.cpp [51] C/C+ + 軽量、 g gml 、 Apple Silicon 最 組込み"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 25 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:677.1px;height:96.6px;border-color:#f1c40f;" title="document_p025_c00105 [h3]
ASR (完全版表) 主要モデル・方式比較: Whisper / faster-whisper / whisper.cpp
代表モデル / 方式 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:159.8px;width:690.3px;height:92.3px;border-color:#f1c40f;" title="document_p025_c00106 [h3]
Whisper OpenAI [48-49]
Tiny 〜 Large-v3
実装多数 (下記派生に分岐)
得意 68 万時間規模の多言語データで頑 健性を示し、翻訳も可能 [48]
汎用 : 直接実装 / 各派生 Risk 高リスク領域では “ 誤転記 記(幻覚) ” が実害になる可 可能性が報道されており、 検証プロセスが必須 [49]
faster-whisper CTranslate2 ["><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:289.6px;width:702.5px;height:135.5px;border-color:#f1c40f;" title="document_p025_c00107 [h3]
whisper.cpp ggml [51]
Whisper 互換
ggml 系で ローカル最適化
得意 C/C++ で軽量運用、各種最適化例 例が豊富
Mac/Win: 共に導入可能 Info バッチ向きに設計し、ス トリーミング要件は別設計 が必要になるケースがある (一般論) [51]
採用判断の要点:ローカル ASR は「 RTF (実時間比)」「メモリ」「誤転記リスク」の 3 要素を同時に"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 26 (5 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:15.5px;width:689.4px;height:409.7px;border-color:#f1c40f;" title="document_p026_c00108 [h3]
採用判断基準( ASR ) RTF 要件と誤転記リスクに基づく選定フロー
結論: faster-whisper を基準実装とし、 RTF 要件・誤転記リスクに応じて GPU 化やモデル拡大を検討する段階的最適化が合理的で す。
前処理の固定
VAD 設定の統一
無音区間の除去は認識精度と速度に直結するため、 Silero VAD 等の設定を固定し再現性を担保する。
ベンチマーク条件
同一音声・同一"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-1" style="left:100.7px;top:154.5px;width:48.2px;height:56.9px;border-color:#e74c3c;" title="document_p026_c00109 [h1]
1
基準実装
faster-whisper (CPU/INT8) で検証"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:275.3px;top:154.5px;width:45.5px;height:56.2px;border-color:#e74c3c;" title="document_p026_c00110 [h1]
2
RTF 評価
実時間比 (RTF)&lt;1 を満たすか確認"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:449.8px;top:154.5px;width:42.2px;height:56.2px;border-color:#e74c3c;" title="document_p026_c00111 [h1]
3
品質評価
誤転記 ( 幻覚 ) の
許容範囲内か"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:618.2px;top:154.5px;width:51.3px;height:56.2px;border-color:#e74c3c;" title="document_p026_c00112 [h1]
4
最適化
不足なら GPU 化 またはモデル拡大"><span class="lbl" style="background:#e74c3c;">h1</span></div></div></div><div class="page-block"><h2>Page 27 (2 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-1" style="left:50.2px;top:19.4px;width:683.9px;height:296.9px;border-color:#e74c3c;" title="document_p027_c00113 [h1]
TTS (音声合成)概要 読み上げと音声クローンの分離設計
結論:「読み上げ」は 軽量モデル( Piper/Kokoro ) で常駐化し、 「音声クローン」は 高品質モデル( XTTS/StyleTTS2 ) で分離設計します。
代表システムと特徴
Piper: &quot;fast, local neural TTS&quot; を標榜。省リソース・常駐向き。 [52]
Kokoro: 82M パラメータで高品質・"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-3" style="left:56.3px;top:193.0px;width:671.0px;height:228.4px;border-color:#f1c40f;" title="document_p027_c00114 [h3]
Text-to-Speech Voice Cloning
運用の分離とリスク管理
軽量 TTS を採用し、システム負荷を最小化。バックグラウンドでの安定 動作を優先。
GPU リソースを割り当て品質を最大化。ただし、法務・権利リスク管理 (許諾・ログ監査)を必須要件とします。 [56]
Risk Management Resource Optimization
日本語品質への注意( Quality"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 28 (5 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:677.7px;height:94.8px;border-color:#f1c40f;" title="document_p028_c00115 [h3]
TTS (完全版表) 主要モデル詳細比較: Piper / Kokoro / XTTS-v2 / StyleTTS2
代表モデル / 方式 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:156.2px;width:687.3px;height:27.4px;border-color:#f1c40f;" title="document_p028_c00116 [h3]
Piper Fast/Local [52]
軽量 ローカル常駐 TTS 向き
得意 &quot;fast, local neural TTS&quot; を明示 得意 省リソース環境で有利
Win/Mac: 導入容易 Risk リポジトリ移転あり(運 用はフォーク / 移転先を確認 ) [52]"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:217.5px;width:689.4px;height:26.6px;border-color:#f1c40f;" title="document_p028_c00117 [h3]
Kokoro Open Weight [53]
82M 軽量 TTS として 使い分け
得意 82M で高速・コスト効率を主張 得意 Apache ライセンス重みをうたう [53]
CPU: 成立しやすい Note 日本語品質は声・辞書・ 前処理に依存(事前確認必 須)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:278.8px;width:686.8px;height:25.1px;border-color:#f1c40f;" title="document_p028_c00118 [h3]
Coqui XTTS-v2 Clone [54]
大きめ GPU 推奨 音声クローン寄り
得意 数秒の参照音声で多言語クローン をうたう [54]
GPU 環境 : 実務的 Risk 法務・倫理・権利リスク 大(同意・監査が必須) [54,56]"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:340.1px;width:697.0px;height:85.1px;border-color:#f1c40f;" title="document_p028_c00119 [h3]
StyleTTS2 Research [55]
研究系 高品質 TTS 志向 得意 &quot;human-level&quot; を目標とする研究系 [55]
GPU: 望ましい Risk 実運用は依存関係・再現 性の確認が必須
採用判断の要点:「読み上げ(通知・要約)」と「クローン(本人声)」は別物として分離設計します。前者は Piper/Kokoro のような軽量系、後者は XTTS 等で、法務・倫理リスク管理"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 29 (7 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-1" style="left:46.9px;top:15.5px;width:692.6px;height:84.3px;border-color:#e74c3c;" title="document_p029_c00120 [h1]
採用判断基準( TTS ) 品質・コスト・リスクの分離管理
結論:「読み上げ(通知・要約)」と「クローン(本人声)」は別レイヤで運用すべきです。前者は軽量化し、後者は法務・ 倫理リスクを管理します。"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-3" style="left:47.4px;top:262.2px;width:437.2px;height:125.1px;border-color:#f1c40f;" title="document_p029_c00121 [h3]
分離運用( Layered )
軽量 TTS の常駐
通知・要約読み上げは Piper/Kokoro 等で省リソース化し、シス テムに常駐させる。
クローンは別系統
XTTS 等は GPU リソースを消費するため、必要な時のみ呼び出す 別サービスとして切り出す。
リスク・コンプライアンス
権利同意の必須化
音声クローンは法務・倫理リスクが高い。業務利用時は対象 者の書面同意をプロセス化する。
ログ"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:322.5px;top:262.2px;width:403.1px;height:162.9px;border-color:#f1c40f;" title="document_p029_c00122 [h3]
品質保証( QA )
日本語品質の事前試験
モデルによりアクセントや読み間違い(数値・固有名詞)の 差が大きい。
辞書・前処理の標準化
業務特有の用語はユーザー辞書や前処理ルール(正規化)で カバーする義務を課す。
Page 29 | ローカル AI 技術調査レポート"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-1" style="left:94.5px;top:154.5px;width:51.3px;height:56.2px;border-color:#e74c3c;" title="document_p029_c00123 [h1]
1
用途定義
単なる読み上げか
本人性の再現か"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:276.2px;top:154.5px;width:40.3px;height:56.9px;border-color:#e74c3c;" title="document_p029_c00124 [h1]
2
モデル選定
軽量 (Piper) vs
高品質 (XTTS)"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:449.8px;top:154.5px;width:44.9px;height:56.2px;border-color:#e74c3c;" title="document_p029_c00125 [h1]
3
リスク判定
声の権利許諾と
ログ監査設計"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:621.5px;top:154.5px;width:53.6px;height:56.9px;border-color:#e74c3c;" title="document_p029_c00126 [h1]
4
実装・運用
常駐 (CPU) or オンデマンド (GPU)"><span class="lbl" style="background:#e74c3c;">h1</span></div></div></div><div class="page-block"><h2>Page 30 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:49.8px;top:19.4px;width:683.5px;height:397.8px;border-color:#f1c40f;" title="document_p030_c00127 [h3]
Embedding / Reranker 概要 検索・ RAG 基盤モデル
結論: RAG システムの品質上限は Embedding の検索精度 で決まります。 まず BGE-M3 等で Recall を確保し、 Reranker で Precision を向上させる二段構えが定石です。
Embedding ( ベクトル検索 )
Retrieval Vector DB
Multi-Function"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 31 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:673.9px;height:93.6px;border-color:#f1c40f;" title="document_p031_c00128 [h3]
Embedding/Reranker (完全版表) 検索・ RAG 基盤: BGE-M3 / multilingual-e5 / bge-reranker
代表モデル 規模 推奨量子化 / 形式 得意 / 不得意(要点) ランタイム相性 リスク / 採用判断"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:153.8px;width:689.3px;height:77.1px;border-color:#f1c40f;" title="document_p031_c00129 [h3]
BGE-M3 BAAI [58]
Model Card 参照
FP16/INT8 で安定運用 必要なら量子化
特徴 Multi-Functionality Multi-Linguality Multi-Granularity を明示 [58]
CPU/GPU/ONNX 等で運用可能 (環境依存)
推奨 RAG は Embedding 品質が が上限を決めるため、まず ここを堅くする
multili"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:271.6px;width:703.1px;height:153.6px;border-color:#f1c40f;" title="document_p031_c00130 [h3]
bge-reranker BAAI [61-62]
278M 〜 560M 等
large/v2 等 RAG の “ 再ランキング ” で 精度向上
機能 「クエリ+文書を入力しスコ アを直接出す」とモデルカードで 説明 [62]
CPU/GPU (遅延要件次第)
Risk reranker はレイテンシを増 増やすため、 p95 要件で採否 を判断
採用判断の要点: RAG の失敗の多くは「検索"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 32 (7 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:15.5px;width:693.0px;height:409.7px;border-color:#f1c40f;" title="document_p032_c00131 [h3]
採用判断基準( Embedding / Reranker ) 検索精度と運用安定化のための二段構え設計
結論: Embedding で網羅性 (Recall) を確保し、 Reranker で精度 (Precision) を上げる「二段構え」が実務的な最適解です。
運用の安定化
前計算(夜間バッチ)
検索対象文書の Embedding 化や OCR 処理は夜間にバッチ実行し 、日中の計算資源を L"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-1" style="left:149.5px;top:138.7px;width:7.3px;height:14.4px;border-color:#e74c3c;" title="document_p032_c00132 [h1]
1"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-2" style="left:110.1px;top:169.8px;width:86.0px;height:42.1px;border-color:#e67e22;" title="document_p032_c00133 [h2]
Embedding
広く候補を取得 (Recall 重視
) BGE-M3 / E5"><span class="lbl" style="background:#e67e22;">h2</span></div><div class="bbox level-1" style="left:380.6px;top:138.7px;width:7.3px;height:14.4px;border-color:#e74c3c;" title="document_p032_c00134 [h1]
2"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-2" style="left:342.7px;top:169.8px;width:83.1px;height:33.4px;border-color:#e67e22;" title="document_p032_c00135 [h2]
Reranker
上位を厳選 (Precision 重視 )
bge-reranker"><span class="lbl" style="background:#e67e22;">h2</span></div><div class="bbox level-1" style="left:611.7px;top:143.3px;width:7.3px;height:14.4px;border-color:#e74c3c;" title="document_p032_c00136 [h1]
3"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-3" style="left:579.6px;top:173.4px;width:72.0px;height:34.4px;border-color:#f1c40f;" title="document_p032_c00137 [h3]
LLM 生成
精度の高い文脈で回答
( 幻覚抑制 )"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 33 (5 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:19.4px;width:684.5px;height:185.7px;border-color:#f1c40f;" title="document_p033_c00138 [h3]
OCR / Document AI 概要 スキャン品質と構造化要件に基づく手法選択
結論:スキャン品質やレイアウトの複雑度に応じて、 古典的 OCR と OCR-free ( Doc 理解) を使い分けるハイブリッド戦略が推奨されます。
代表的なシステム・モデル"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:58.2px;top:225.8px;width:240.3px;height:24.6px;border-color:#f1c40f;" title="document_p033_c00139 [h3]
Tesseract OCR [64]
古典的 OCR + LSTM 。単純なテキスト化のデファクトスタンダード。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:58.2px;top:265.2px;width:240.2px;height:23.7px;border-color:#f1c40f;" title="document_p033_c00140 [h3]
PaddleOCR [66]
高精度かつ多機能。表構造や縦書きなど複雑なレイアウトに強い。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:58.2px;top:304.6px;width:200.5px;height:24.7px;border-color:#f1c40f;" title="document_p033_c00141 [h3]
Donut (OCR-free) [68]
画像を直接 Transformer で処理し、構造化データを抽出。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:54.6px;top:193.0px;width:672.5px;height:245.9px;border-color:#f1c40f;" title="document_p033_c00142 [h3]
LayoutLMv3 [69]
テキスト、画像、レイアウト情報を統合して文書理解を行う。
運用指針と使い分け
高品質スキャン・単純文書:
Tesseract を採用。高速かつ軽量で、単純なテキスト化に最適。
低品質・複雑レイアウト・構造化:
PaddleOCR 、 Donut 、 LayoutLMv3 を採用。傾きやノイズに強く、フォームや表の理解が 可能。
Hybrid Strategy Lay"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 34 (5 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:672.6px;height:95.3px;border-color:#f1c40f;" title="document_p034_c00143 [h3]
OCR / Document AI (完全版表) 主要モデル詳細比較: Tesseract / PaddleOCR / Donut / LayoutLMv3
代表モデル / 方式 方式 推奨実行形態 得意 / 不得意(要点) ランタイム相性"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:157.1px;width:660.0px;height:27.4px;border-color:#f1c40f;" title="document_p034_c00144 [h3]
Tesseract OCR Google [64]
古典 OCR + LSTM
テキスト化基盤 スキャン品質高なら CPU で十分
得意 長年の実績と安定性 不得意 複雑レイアウトや手書き文字は苦手
Cross-Platform: Win/Mac/Linux 問わず導入容易"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:219.4px;width:656.8px;height:18.2px;border-color:#f1c40f;" title="document_p034_c00145 [h3]
PaddleOCR Baidu [66]
Deep Learning OCR
高精度・多機能 構造化出力向け
得意 軽量かつ高精度な認識 特徴 表認識やレイアウト解析機能も充実
Python / ONNX: Python 環境で容易に実装可能"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:281.6px;width:658.9px;height:27.4px;border-color:#f1c40f;" title="document_p034_c00146 [h3]
Donut Clova AI [68]
OCR-free Transformer
Doc 理解・抽出 画像から直接 JSON 等へ
得意 OCR 誤りを経由せず直接情報抽出 特徴 フォームや請求書等の定型文書に強み
Transformers: Hugging Face Transformers 対応"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:343.9px;width:702.3px;height:81.2px;border-color:#f1c40f;" title="document_p034_c00147 [h3]
LayoutLMv3 Microsoft [69]
Multimodal Transformer
レイアウト保持 文脈理解統合
得意 視覚情報とテキスト情報を統合して理解 特徴 表 / 図 / レイアウト由来の欠落を補完 [69]
Transformers / ONNX: 推論エンジンでの最適化が可能
レイアウト保持の重要性: RAG において、単なるテキスト化では図表やレイアウト構造に含まれる情"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 35 (7 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:15.5px;width:691.8px;height:358.7px;border-color:#f1c40f;" title="document_p035_c00148 [h3]
OCR/Document AI 採用判断基準 リスク緩和のための UX 要件とハイブリッド構成
結論:誤読リスクをゼロにはできない前提で、前処理の標準化と「原文引用」による人間系確認の UX を要件化します。
前処理の標準化
品質の下限を担保
傾き補正、ノイズ除去、二値化を OCR 前段に固定的に組み込む ことで、認識精度のベースラインを確保。
画像の正規化
解像度やコントラストのバラつきを抑え、"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:322.5px;top:245.4px;width:409.1px;height:179.7px;border-color:#f1c40f;" title="document_p035_c00149 [h3]
UX 要件と運用
原文スニペット引用
AI 回答の根拠となった原文箇所(画像切り出し等)を提示し、 ユーザーが誤読を検証できる UX を必須化。
夜間バッチへのオフロード
重い OCR/Embedding 処理は夜間に回し、日中のリソースは検索 と短い回答生成に集中させる。
Page 35 | ローカル AI 技術調査レポート"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-1" style="left:67.3px;top:137.7px;width:70.3px;height:56.1px;border-color:#e74c3c;" title="document_p035_c00150 [h1]
1
品質評価
スキャン品質と レイアウト複雑度を確認"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:214.6px;top:137.7px;width:57.6px;height:56.1px;border-color:#e74c3c;" title="document_p035_c00151 [h1]
2
前処理
傾き補正・二値化の 標準パイプライン化"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:356.8px;top:137.7px;width:55.2px;height:56.8px;border-color:#e74c3c;" title="document_p035_c00152 [h1]
3
モデル選択
PaddleOCR( 構造化 ) +Donut( レイアウト )"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:493.0px;top:137.7px;width:64.0px;height:56.1px;border-color:#e74c3c;" title="document_p035_c00153 [h1]
4
UX 要件化
原文スニペット表示で
誤読リスクを緩和"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:640.4px;top:137.7px;width:51.4px;height:56.1px;border-color:#e74c3c;" title="document_p035_c00154 [h1]
5
運用設計
OCR/Embedding は 夜間バッチ処理へ"><span class="lbl" style="background:#e74c3c;">h1</span></div></div></div><div class="page-block"><h2>Page 36 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:50.2px;top:19.4px;width:683.2px;height:416.3px;border-color:#f1c40f;" title="document_p036_c00155 [h3]
画像生成( Diffusion )概要 モデル・ランタイム・実務ポイント
結論:画像生成は SDXL を軸に、 ComfyUI/SD WebUI で運用。 FLUX 等はライセンス精査が必須であり、 VRAM 要件はワークフローに依存します。
主要モデルと UI ( Runtime )
高品質生成の標準モデル。ライセンス確認の上、低コスト構成の軸に。
[71]
ノードベース UI 。 Window"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 37 (4 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:39.0px;top:27.8px;width:686.1px;height:174.5px;border-color:#5dade2;" title="document_p037_c00156 [body]
画像生成(完全版表) 主要モデル・ UI 詳細比較: SDXL / ComfyUI / SD WebUI / FLUX
モデル / UI 方式 / 特徴 VRAM 要件 得意 / 不得意(要点) リスク / 採用判断
SDXL base 1.0 Stability AI [71]
Diffusion ワークフロー依存 (解像度・バッチ・ステップ数 に大きく左右される)
得意 高品質生成(ベースモデ"><span class="lbl" style="background:#5dade2;">body</span></div><div class="bbox level-3" style="left:39.0px;top:237.0px;width:689.9px;height:27.4px;border-color:#f1c40f;" title="document_p037_c00157 [h3]
ComfyUI UI [72]
ノード型 UI Win/Linux/macOS 対応
構成次第 (効率的なメモリ管理が可能)
得意 再利用性・資産化(ワークフロー 保存) 得意 seed 固定による再現性担保
判断 ワークフローのブラックボックス化を 防ぐため管理が必要"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:308.0px;width:689.9px;height:26.5px;border-color:#f1c40f;" title="document_p037_c00158 [h3]
Stable Diffusion WebUI UI [73]
プラグイン豊富 設定・拡張依存 得意 導入が比較的容易
注意 設定差による品質ブレが発生しや すい
判断 簡易利用には適するが、厳密な再現性 には注意"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:366.7px;width:701.3px;height:58.4px;border-color:#f1c40f;" title="document_p037_c00159 [h3]
FLUX.1 Black Forest Labs [75]
上位モデル系 高め (高品質ゆえのリソース要求)
得意 高い品質ポテンシャル Risk ライセンス(特に dev 版の条件)の精査 が必須 [76] 商用利用可否を必ず確認
採用判断の要点: VRAM 容量が最大の制約となります。解像度、バッチサイズ、ステップ数を固定したプロファイルを作成し、 VRAM 不足を防ぐ運用設計が重要です。特に"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 38 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:4.8px;top:4.6px;width:508.4px;height:435.8px;border-color:#5dade2;" title="document_p038_c00160 [body]
\\n\\n 採用判断基準(画像生成) \\n
VRAM 制約と再現性の担保
\\n 結論: VRAM 制約に合わせて解像度 / バッチ / ステップを固定し、ワークフローを資産化して再現性を担保します。 \\n
1
VRAM 制約
ハードウェア上限を まず確認
2
解像度固定
VRAM に収まる 最大サイズを決定
3
\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\"><span class="lbl" style="background:#5dade2;">body</span></div></div></div><div class="page-block"><h2>Page 39 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:20.3px;width:685.6px;height:100.5px;border-color:#f1c40f;" title="document_p039_c00161 [h3]
Agent / Tool-use 概要 ローカル環境でのエージェント構築と実行
結論: Agent は「 LLM +ツール呼び出し(関数)+実行環境 」で構成され、 Ollama/LM Studio の OpenAI 互換 API を用いる いることで、ローカルでの実務的な構築が現実化しています。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:55.0px;top:177.0px;width:309.0px;height:235.2px;border-color:#f1c40f;" title="document_p039_c00162 [h3]
構成要素と API 基盤
エージェントの基本構成。 LLM が判断し、定義されたツール(関数)を 呼び出し、ローカル環境で実行して結果を返すループ構造。
Ollama や LM Studio は、ローカルで動作しながら OpenAI 互換のエンドポイ ポイントを提供。これにより、既存の Agent フレームワークやクライア ントツールをそのまま利用可能。
[23][24][41]
OpenHand"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-2" style="left:28.8px;top:178.3px;width:709.2px;height:247.1px;border-color:#e67e22;" title="document_p039_c00163 [h2]
Function Calling (Tool Use)
ローカル API でも tool_choice や functions パラメータを利用可能。モデ ルが JSON 形式で引数を生成し、システム側で実行します。
[24][27]
ローカルモデル(特に小型)では JSON スキーマの破壊が発生しやすいた め、型定義の厳格化と、パース失敗時の再試行ロジックが不可欠です。
生成されたコードやコマン"><span class="lbl" style="background:#e67e22;">h2</span></div></div></div><div class="page-block"><h2>Page 40 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:28.8px;top:27.8px;width:705.6px;height:412.7px;border-color:#f1c40f;" title="document_p040_c00164 [h3]
Agent/Tool-use (完全版表) OpenAI 互換 API / Function Calling / 実装基盤の詳細比較
項目・方式 詳細仕様・提供形態 利点・特徴 注意点・リスク
OpenAI 互換 API インターフェース [24,41]
提供: Ollama / LM Studio ローカル LLM を標準的な REST API として公開 Chat Completions AP"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 41 (4 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:15.5px;width:688.4px;height:378.3px;border-color:#f1c40f;" title="document_p041_c00165 [h3]
採用判断基準( Agent / Tool-use ) JSON スキーマ固定と検証ループによる安全運用
結論: JSON スキーマ固定+検証ループ(自動チェック)による安全運用を基本とし、 Ollama/LM Studio の OpenAI 互換 API で実装 を統一します。
スキーマと検証
JSON スキーマの厳格化 必須引数、型定義、 Enum 制約を厳密に 記述し、モデルの構造化出力能力を"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:288.3px;top:317.3px;width:197.9px;height:86.3px;border-color:#f1c40f;" title="document_p041_c00166 [h3]
[24,41]
ランタイム機能の活用 必要に応じて、 llama-cpp-python 等のラ ンタイム固有機能(文法制約機能など)を補助的に活用。
[25]"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:90.7px;top:188.6px;width:640.1px;height:236.6px;border-color:#f1c40f;" title="document_p041_c00167 [h3]
幻覚 API 対策
ホワイトリスト化 実行可能な関数セットを厳密にホワイトリ スト化し、存在しない API の呼び出し(幻覚)をシステム側で 遮断。
タイムアウトとリトライ 無限ループや応答遅延を防ぐため、 標準でタイムアウト設定とリトライ上限を設ける設計を義務化 。
Page 41 | ローカル AI 技術調査レポート
スキーマ定義
必須引数・型・制約を
厳格化して固定
ツール実行
Ollama"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:613.7px;top:188.6px;width:56.6px;height:32.1px;border-color:#f1c40f;" title="document_p041_c00168 [h3]
完了 / リトライ
結果確認または フォールバック"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 42 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:20.3px;width:688.4px;height:258.1px;border-color:#f1c40f;" title="document_p042_c00169 [h3]
音声周辺( VAD/ 話者分離 / ウェイクワード / ノイズ除去)概要 カテゴリ別モデルカタログ
結論:前処理( VAD/ 話者分離 / ウェイクワード / ノイズ除去)が音声 UX の品質下限を決定します。 Silero VAD による無音除去と pyannote.audio による話者分離が実務上の標準構成です。
主要コンポーネント( Primary )
VAD ( Voice Activi"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:49.5px;top:173.2px;width:680.7px;height:135.7px;border-color:#f1c40f;" title="document_p042_c00170 [h3]
Silero VAD pyannote.audio
補助コンポーネント( Secondary )
ウェイクワード( Wake Word ): openWakeWord [85]
「 Hey Siri 」等の起動語検出。オンデバイスで動作し、誤検知( False Positive )とのトレー ドオフ設計が鍵。
ノイズ除去( Noise Suppression ): RNNoise [14]
RNN"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:301.4px;width:709.2px;height:124.0px;border-color:#f1c40f;" title="document_p042_c00171 [h3]
openWakeWord RNNoise
設計含意( Design Implication )
これら前処理の設定差(閾値、モデルバージョン)で最終的な認識精度が大きく変動します。ベンチマーク時は「同一前処理条件」を固定することが再現性の担保に不可欠です。
Local AI Technical Survey Report 2026 42 / 80"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 43 (4 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:39.0px;top:27.8px;width:673.5px;height:94.0px;border-color:#5dade2;" title="document_p043_c00172 [body]
音声周辺(完全版表) Silero VAD / pyannote / openWakeWord / RNNoise 詳細比較
代表モデル / ツール 役割 実行要件 / 形式 利点 / 特徴 リスク / 注意点"><span class="lbl" style="background:#5dade2;">body</span></div><div class="bbox level-3" style="left:39.0px;top:154.5px;width:688.2px;height:77.8px;border-color:#f1c40f;" title="document_p043_c00173 [h3]
Silero VAD Snakers4 [82]
無音 / 雑音除去 (Voice Activity Detection)
軽量・ CPU 動作可 ONNX / PyTorch
標準 前処理のデファクト ASR 負荷を大幅に削減
Attention 閾値設定に依存(誤って語 語頭を切るリスクあり)
pyannote.audio HuggingFace [84]
話者分離 (Speaker Diar"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:273.7px;width:684.4px;height:26.6px;border-color:#f1c40f;" title="document_p043_c00174 [h3]
openWakeWord dscripka [85]
ウェイクワード検出 (Wake Word Detection)
ローカル動作 CPU / tflite 等
実用 カスタムウェイクワード作成が可 能 非常に軽量
Attention 誤検知( False Positive )の の調整が必要"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:333.4px;width:699.5px;height:91.8px;border-color:#f1c40f;" title="document_p043_c00175 [h3]
RNNoise Xiph.Org [14]
ノイズ除去 (Noise Suppression)
極めて軽量 C/C++ / WebAssembly
高速 リアルタイム処理向き RNN ベースで背景雑音を抑制
Risk 音質変化(声がロボットっぽく なる等)のリスク
設計の推奨:会議系ワークフローでは「 VAD → ASR → 要約」の直列処理を基本とし、負荷の高い話者分離( pyannote 等)"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 44 (6 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:15.5px;width:693.0px;height:409.7px;border-color:#f1c40f;" title="document_p044_c00176 [h3]
採用判断基準(音声周辺) 前処理パイプライン標準化と品質管理
結論:前処理パイプラインを標準化し、閾値・モデルバージョン・ RTF 目標を固定することで、 UX の品質下限を担保します。
パイプラインの標準化
基準設定のプリセット化
VAD 閾値、話者分離の有無、ノイズ除去の有無をユースケース ごとに固定セットとして定義。
再現性の担保
モデルバージョンとパラメータをコードで固定し、環境によ る挙"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-1" style="left:70.6px;top:137.7px;width:64.0px;height:56.1px;border-color:#e74c3c;" title="document_p044_c00177 [h1]
1
ユースケース定義
会議録音か ウェイクワードか"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:211.4px;top:137.7px;width:64.0px;height:56.1px;border-color:#e74c3c;" title="document_p044_c00178 [h1]
2
パイプライン構成
VAD+ASR+ 分離など
プリセット化"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:356.4px;top:137.7px;width:56.2px;height:56.1px;border-color:#e74c3c;" title="document_p044_c00179 [h1]
3
閾値固定
VAD 感度・分離閾値
を数値で管理"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:497.1px;top:137.7px;width:56.0px;height:56.1px;border-color:#e74c3c;" title="document_p044_c00180 [h1]
4
バージョン固定
Silero/pyannote の
バージョン統一"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-1" style="left:640.3px;top:137.7px;width:51.3px;height:56.1px;border-color:#e74c3c;" title="document_p044_c00181 [h1]
5
ログ監視
RTF ・エラー率の 継続モニタリング"><span class="lbl" style="background:#e74c3c;">h1</span></div></div></div><div class="page-block"><h2>Page 45 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-1" style="left:48.0px;top:90.2px;width:79.9px;height:80.4px;border-color:#e74c3c;" title="document_p045_c00182 [h1]
07"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-3" style="left:48.0px;top:209.9px;width:217.5px;height:118.1px;border-color:#f1c40f;" title="document_p045_c00183 [h3]
メモリ設計の コア
重み量子化・ KV キャッシュ管理と 最適化技術"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:393.8px;top:61.3px;width:312.9px;height:307.6px;border-color:#f1c40f;" title="document_p045_c00184 [h3]
KE Y TAKEAWAYS
本章の要点
重みメモリの現実解:
4bit 量子化( weight-only )が基本。理論値+ 10% オーバーヘ ッドで設計します。
KV キャッシュの支配性:
長文コンテキストでは KV がメモリを圧迫。 vLLM の paged/quantized KV [5,6] や TensorRT-LLM の KV reuse [7] が必須で す。
最適化技術の活用:"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 46 (4 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:46.9px;top:19.4px;width:684.4px;height:110.5px;border-color:#5dade2;" title="document_p046_c00185 [body]
メモリ設計のコア(概要) 重み量子化と KV キャッシュの支配性
結論:ローカル AI のメモリ制約は、 「重み 4bit 量子化」 と 「 KV キャッシュ最適化」 の 2 点によって決定さ れる支配的な要因です。"><span class="lbl" style="background:#5dade2;">body</span></div><div class="bbox level-3" style="left:55.2px;top:193.0px;width:304.4px;height:152.3px;border-color:#f1c40f;" title="document_p046_c00186 [h3]
重みメモリ( Weights )
パラメータ数と bit 幅で物理的な下限が決まります。
メモリ ≈ パラメータ数 × bit 幅 ÷ 8 ( + 約 10% ラ
ンタイム /CUDA オーバーヘッド)
FP16 ( 16bit )と比較して約 1/4 のサイズで、品質劣化を最小限に抑えつ つコンシューマ GPU に載せるための必須技術です。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:55.5px;top:367.8px;width:150.4px;height:8.1px;border-color:#f1c40f;" title="document_p046_c00187 [h3]
GGUF Q4_K_M AWQ 4bit GPTQ"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:55.9px;top:193.0px;width:665.1px;height:246.3px;border-color:#f1c40f;" title="document_p046_c00188 [h3]
KV キャッシュと最適化
KV キャッシュは「コンテキスト長に比例」して増加します。長文・多 同時接続では重み以上にメモリを圧迫します。
例: Llama3 8B (n_layer=32, n_head_kv=8, head_dim=128)
Paged KV: メモリ断片化を防ぐ( vLLM 等) [5]
Quantized KV: FP8/INT4 化で容量削減 [6]
KV Reuse: 計"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 47 (6 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:696.9px;height:85.5px;border-color:#f1c40f;" title="document_p047_c00189 [h3]
重みメモリ理論値(完全版表 1 : 0.5B 〜 14B ) モデル規模別推奨メモリ容量( +10% オーバーヘッド込み概算)
モデル規模 (Parameters) 4bit (Q4_K_M 等 )
ローカル推奨
INT8 (8bit) 標準的量子化
FP16 (16bit) 元精度 / 学習時"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:134.7px;width:623.7px;height:9.8px;border-color:#f1c40f;" title="document_p047_c00190 [h3]
0.5B (Qwen2.5-0.5B 等 ) 0.3 GiB 0.5 GiB 1.0 GiB"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:168.0px;width:623.7px;height:9.8px;border-color:#f1c40f;" title="document_p047_c00191 [h3]
1.5B (Qwen2.5-1.5B 等 ) 0.8 GiB 1.5 GiB 3.1 GiB"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:201.2px;width:623.7px;height:9.8px;border-color:#f1c40f;" title="document_p047_c00192 [h3]
3B (Phi-3 Mini 等 ) 1.5 GiB 3.1 GiB 6.1 GiB"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:234.4px;width:626.2px;height:9.8px;border-color:#f1c40f;" title="document_p047_c00193 [h3]
7B (Qwen2.5-7B 等 ) 3.6 GiB 7.2 GiB 14.3 GiB"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:267.6px;width:705.5px;height:157.5px;border-color:#f1c40f;" title="document_p047_c00194 [h3]
8B (Llama 3.1 8B 等 ) 4.1 GiB 8.2 GiB 16.4 GiB
14B (Qwen2.5-14B/Phi-4 等 ) 7.2 GiB 14.3 GiB 28.7 GiB
計算根拠:上記数値は「パラメータ数 × bit 幅 ÷ 8 」に、ランタイムオーバヘッドとして約 10% を加算した概算理論値です。これらは重みのみのメモリ消費であり、実運用ではこれに加えて KV キャ"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 48 (5 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:43.8px;top:27.8px;width:686.5px;height:87.9px;border-color:#f1c40f;" title="document_p048_c00195 [h3]
重みメモリ理論値(完全版表 2 : 27B 〜 70B ) 大規模モデル( 4bit/INT8/FP16 )のメモリ要件
モデル規模 4bit (Q4) 推奨 IN T8 (Q8) FP16 (Half)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:43.8px;top:148.2px;width:502.9px;height:18.0px;border-color:#f1c40f;" title="document_p048_c00196 [h3]
27B Gemma 2 27B 等 13.8 GiB 27.7 GiB 55.3 GiB"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:43.8px;top:194.2px;width:502.9px;height:18.0px;border-color:#f1c40f;" title="document_p048_c00197 [h3]
32B Qwen2.5 32B 等 16.4 GiB 32.8 GiB 65.6 GiB"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:43.8px;top:240.2px;width:502.9px;height:18.0px;border-color:#f1c40f;" title="document_p048_c00198 [h3]
34B Yi-34B 等 17.4 GiB 34.8 GiB 69.7 GiB"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:286.3px;width:701.2px;height:138.9px;border-color:#f1c40f;" title="document_p048_c00199 [h3]
70B Llama 3 70B 等 35.9 GiB 71.7 GiB 143.4 GiB
計算前提:パラメータ数 × bit 幅 ÷ 8 で基本容量を算出後、実運用におけるランタイムオーバヘッド等を考慮して約 +10% を加算した概算値です。
設計含意: 30B 級モデルの実運用には、 4bit 量子化でも約 16-18GiB の VRAM/ 統合メモリが必要です。 70B 級では 4bit で"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 49 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:28.8px;top:27.8px;width:698.8px;height:397.4px;border-color:#f1c40f;" title="document_p049_c00200 [h3]
KV キャッシュの支配性(完全版表 1 ) 2k 〜 32k tokens におけるメモリ消費量比較( Llama3 8B 相当)
コンテキスト長 (Tokens)
FP16 ( 標準 ) (16-bit)
FP8 ( 量子化 ) (8-bit)
INT4 ( 量子化 ) (4-bit)
2,048 tokens 標準的な短文対話 0.25 GiB 0.12 GiB 0.06 GiB
8,192 "><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 50 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:41.4px;top:27.8px;width:686.4px;height:124.3px;border-color:#f1c40f;" title="document_p050_c00201 [h3]
KV キャッシュの支配性(完全版表 2 :超長文域) コンテキスト長 131k tokens におけるメモリ消費量比較( Llama3 8B 相当)
コンテキスト長 FP16 (Base) FP8 (Optimized) INT4 (Highly Optimized)
131,072 tokens 128k context 16.00 GiB Critical モデル本体 (8B) と合わせると "><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:396.7px;top:132.2px;width:63.9px;height:19.9px;border-color:#f1c40f;" title="document_p050_c00202 [h3]
8.00 GiB Heavy 50% 削減(実用域へ)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:132.2px;width:698.2px;height:293.0px;border-color:#f1c40f;" title="document_p050_c00203 [h3]
4.00 GiB Manageable 75% 削減(余裕あり)
メモリ内訳の逆転現象
超長文( 128k 等)では、 KV キャッシュのメモリ消費量がモデル本体(重み)のメモリ消費 量を上回る現象が発生します。
例: Llama3 8B ( 4bit 重み ≒ 4.1GiB )に対し、 131k tokens の KV ( FP16 )は 16.00GiB に達し、総 メモリの約 80% を K"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 51 (4 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:24.0px;top:15.7px;width:685.1px;height:16.7px;border-color:#5dade2;" title="document_p051_c00204 [body]
メモリ最適化技術 paged KV / quantized KV / KV reuse / prompt cache"><span class="lbl" style="background:#5dade2;">body</span></div><div class="bbox level-3" style="left:81.0px;top:98.7px;width:130.4px;height:14.1px;border-color:#f1c40f;" title="document_p051_c00205 [h3]
KV 構造と管理の最適化"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:59.4px;top:148.7px;width:298.4px;height:49.6px;border-color:#f1c40f;" title="document_p051_c00206 [h3]
Paged KV Cache (vLLM) [5]
メモリを固定サイズのページ単位で管理し、断片化を抑制する技術。 OS の仮想メモリと同様の仕組みで、 GPU メモリの利用効率を劇的に向 上させ、スループットを高めます。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:59.4px;top:98.7px;width:671.1px;height:326.4px;border-color:#f1c40f;" title="document_p051_c00207 [h3]
Quantized KV Cache [6]
KV キャッシュを標準の FP16 から FP8 や INT4 へ量子化。精度劣化を最小 限に抑えつつメモリフットプリントを 50 〜 75% 削減し、より長いコン テキストや大きなバッチサイズでの推論を可能にします。
再利用とレイテンシ短縮
KV Cache Reuse (TensorRT-LLM) [7]
共通のプレフィックス(システムプロンプトや"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 52 (4 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:20.3px;width:686.8px;height:100.5px;border-color:#f1c40f;" title="document_p052_c00208 [h3]
Tier 定義と現実ライン(概要) ハードウェア別の実用ライン定義
結論: Tier は「重み( 4bit )+ KV ( 4k 〜 8k )+オーバーヘッド」を前提に定義。 理論計算値 をベースとしつつ、最終判断は 実機再現ベンチ で行う設計です。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:66.6px;top:174.9px;width:145.4px;height:198.2px;border-color:#f1c40f;" title="document_p052_c00209 [h3]
Apple Silicon (UMA)
16GB (A-16) 3B 〜 7/8B (4bit)
24-32GB (A-24/32) 7B 〜 14B (4bit)
64GB (A-64) 14B 〜 32B (4bit)
96-128GB (A-96+) 32B 〜 70B (4bit)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:308.4px;top:174.9px;width:151.4px;height:198.2px;border-color:#f1c40f;" title="document_p052_c00210 [h3]
Windows GPU (VRAM)
VRAM 8GB (G-8) 7B (4bit) ※短文中心
VRAM 12GB (G-12) 7B 〜 14B (4bit)
VRAM 16GB (G-16) 14B 〜 27B (4bit)
VRAM 24GB (G-24) 27B 〜 34B (4bit)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:174.9px;width:709.2px;height:259.0px;border-color:#f1c40f;" title="document_p052_c00211 [h3]
Windows CPU-only
RAM 32GB (W-CPU1) 3B 〜 7B (4bit)
7B 級が「実用的に動く」境界線。 AVX 命令等の最適化が必須。 速度は GPU 比で大幅劣後。
実務上の注意点
Local AI Technical Survey Report 2026 52 / 80"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 53 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:677.0px;height:115.8px;border-color:#f1c40f;" title="document_p053_c00212 [h3]
Apple Silicon Tier (完全版表 1 ) 実用ライン定義: A-16 / A-24/32 (Unified Memory Architecture)
Tier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-2" style="left:39.0px;top:202.2px;width:689.0px;height:50.2px;border-color:#e67e22;" title="document_p053_c00213 [h2]
A-16 Entry
Apple Silicon 16GB (M1/M2/M3/M4 等 ) [1,11,92]
3B 〜 7/8B (4bit 量子化 )
条件付 “ 短文中心 ” なら可
軽量構成 ASR/TTS は小さめなら可
文脈長を抑え、 RAG は Embedding 先計算 算。 Apple 公式の Unified Memory Architecture (UMA) 特性を理解し、シ"><span class="lbl" style="background:#e67e22;">h2</span></div><div class="bbox level-2" style="left:28.8px;top:304.7px;width:701.2px;height:120.4px;border-color:#e67e22;" title="document_p053_c00214 [h2]
A-24/32 Standard
Apple Silicon 24 〜 32GB (M3/M4 Pro/Max 等 ) [93]
7B 〜 14B (4bit 量子化 )
可 (業務最小ライン)
成立 多くのユースケースで 実用可
14B 級を軸に、 VLM は 2B 〜小型を併用。 。 「 RAG 含め業務用途の最小実用ライン」 になりやすい。 M4 世代でも統合メモリ 前提で構成されるため、 "><span class="lbl" style="background:#e67e22;">h2</span></div></div></div><div class="page-block"><h2>Page 54 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:695.1px;height:120.0px;border-color:#f1c40f;" title="document_p054_c00215 [h3]
Apple Silicon Tier (完全版表 2 ) A-64 / A-96/128 :高品質ローカルの到達点と運用指針
Tier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:207.7px;width:691.4px;height:73.8px;border-color:#f1c40f;" title="document_p054_c00216 [h3]
A-64 High-End
Apple 64GB (M1 Max 等 ) [1, 92]
14B 〜 32B (4bit)
可 余裕あり LLM+VLM 等
長文会議は要約で圧縮し、 KV 最適化を活用。 M1 Max 等で 64GB 構成が公式に提示されており、 32B 級モデルの実用的なスイートスポット。 32B モデル( 4bit )で約 16-18GB 消費、残りで VLM/ASR/OS "><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:319.3px;width:701.9px;height:105.9px;border-color:#f1c40f;" title="document_p054_c00217 [h3]
A-96/128 Ultra
Apple 96 〜 128GB (M2/M3/M4 Max/Ultra) [93, 94]
32B 〜 70B (4bit)
可 ( 重い )
構成次第 高品質 RAG 等
“ 高品質ローカル ” の現実ライン。 70B 級( 4bit で約 40GB )をロードしても、まだ 50GB 以上の余裕がある圧倒的なメモリ空間。 M3/M4 で最大 128GB 構成が公式"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 55 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:39.0px;top:27.8px;width:690.7px;height:200.2px;border-color:#f1c40f;" title="document_p055_c00218 [h3]
Windows Tier (完全版表 1 ) W-CPU1 / W-CPU2 / G-8 / G-12 の現実ライン
Tier 想定ハード 現実的な LLM 規模 常駐可否 同時実行 設計指針
W-CPU1 CPU-only 8C/16T 級 + 32GB RAM
3B 〜 7B (4bit) 条件付き 負荷高
厳しい オンデマンド推奨
CPU 最適化命令( AVX-512/VNNI/AMX 等)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:39.0px;top:267.2px;width:685.7px;height:18.2px;border-color:#f1c40f;" title="document_p055_c00219 [h3]
G-8 NVIDIA GPU VRAM 8GB
7B 中心 (4bit) 可 短文に限る
軽量なら可 画像生成 SDXL 等は設定依存で厳しいため ベンチ。 7B 級 LLM 単体なら快適に動作。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:324.7px;width:703.1px;height:100.4px;border-color:#f1c40f;" title="document_p055_c00220 [h3]
G-12 NVIDIA GPU VRAM 12GB
7B 〜 14B (4bit) 可 現実化 14B 級が視野に入る。 VLM 2B 〜 7B 級や GPU 版を同時に載せやすく、実用性が高い 。
最適化のポイント: Windows の CPU 推論は Intel 拡張( IPEX )や OpenVINO 等の最適化が効く場合があります。 GPU ( G-8/12 )では、 VRAM 不足時にシ"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 56 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:28.8px;top:27.8px;width:698.6px;height:397.4px;border-color:#f1c40f;" title="document_p056_c00221 [h3]
Windows Tier (完全版表 2 ) 高品質・実務用: G-16 (16GB) / G-24 (24GB) 定義
Tier 想定ハード 現実的な LLM 規模 常駐可否 / 同時実行 設計指針
G-16 VRAM 16GB GeForce 4080 Laptop RTX 4070 Ti Super 等
14B 〜 27B (4bit 量子化 )
常駐 : 可 同時 : 高品質寄り成立 AS"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 57 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:19.4px;width:687.3px;height:110.5px;border-color:#f1c40f;" title="document_p057_c00222 [h3]
補足: AMD/Intel GPU の現実ライン 公式最適化とエコシステムの現状
結論: AMD/Intel 環境でも公式最適化による選択肢が存在しますが、運用は実装・ドライバ依存となり、 個別検証が必要です。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-2" style="left:51.0px;top:194.3px;width:309.2px;height:225.1px;border-color:#e67e22;" title="document_p057_c00223 [h2]
AMD Radeon GPU (ROCm)
ROCm HIP SDK ZLUDA (Deprecated)
ROCm の Windows サポートが進展しており、一部の WSL2 環境やネイティ ブ Windows での動作が可能になりつつあります。 [97]
llama.cpp の HIP BLAS バックエンドや、 MLC LLM などが AMD GPU をサポー ト。 VRAM 容量あたりの"><span class="lbl" style="background:#e67e22;">h2</span></div><div class="bbox level-3" style="left:420.2px;top:194.3px;width:310.0px;height:209.7px;border-color:#f1c40f;" title="document_p057_c00224 [h3]
Intel Arc / iGPU (OpenVINO/SYCL)
OpenVINO IPEX SYCL / oneAPI
PyTorch 拡張として XPU ( Intel GPU )サポートを提供。 Arc GPU での推論 加速が可能。 [13]
llama.cpp の SYCL バックエンドや OpenVINO 最適化により、 iGPU ( Core Ultra 等)を含めた幅広いハードウェ"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 58 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:24.0px;top:15.7px;width:712.2px;height:409.4px;border-color:#f1c40f;" title="document_p058_c00225 [h3]
常駐運用 vs オンデマンド運用 TTFT (初速)、安定性、リソース効率のトレードオフ
基本運用モデルの比較
常駐( Always-on ) 推奨 : チャット Bot
モデルをメモリに保持し、 Prompt Cache を維持。
利点: TTFT が最短、応答が安定。
欠点:メモリを常時占有(他アプリと競合)。
オンデマンド( On-demand ) 推奨 : 翻訳 / 要約
リクエスト時のみ"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 59 (10 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:5.8px;width:695.1px;height:164.4px;border-color:#f1c40f;" title="document_p059_c00226 [h3]
ユースケース別推奨スタック:音声メモ → 整文化 → タスク抽出
用途に応じて「低コスト/高品質/ハード制約」の 3 構 成を選択
結論: ASR 精度と LLM 推論能力のバランスで構成を決定します。 ※誤転記防止には VAD (無音除去)が必須。タスク抽出には JSON スキーマ固定 が有効です。
低コスト構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:48.1px;top:198.3px;width:192.5px;height:41.3px;border-color:#f1c40f;" title="document_p059_c00227 [h3]
Target HW
A-16 / W-CPU1 / G-8 想定 (Apple 16GB / CPU / VRAM
8GB)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:54.6px;top:258.7px;width:176.8px;height:96.6px;border-color:#f1c40f;" title="document_p059_c00228 [h3]
VAD &amp; ASR
Silero VAD ( 無音除去 ) faster-whisper (CPU/INT8)
LLM 整形・抽出
7B 級 (Qwen2.5-7B 等 ) 4bit (GGUF 系 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:44.4px;top:158.0px;width:382.4px;height:257.7px;border-color:#f1c40f;" title="document_p059_c00229 [h3]
Output Interface
JSON (タスク配列) Ollama/LM Studio (OpenAI 互換 )
高品質構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:283.3px;top:198.3px;width:201.9px;height:36.2px;border-color:#f1c40f;" title="document_p059_c00230 [h3]
Target HW
A-64+ / G-16+ / G-24 想定 (Apple 64GB+ / VRAM 16GB+)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:295.5px;top:258.7px;width:177.2px;height:96.6px;border-color:#f1c40f;" title="document_p059_c00231 [h3]
VAD &amp; ASR
faster-whisper (GPU) 必要なら大型 Whisper 系列
LLM 整形・抽出
14B 〜 32B 級 (Phi-4/Qwen 上位 ) 4bit / GPU 実行"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:287.9px;top:158.0px;width:392.0px;height:257.7px;border-color:#f1c40f;" title="document_p059_c00232 [h3]
Output Interface
Function Calling 前提 Schema 固定で JSON 破壊率低減
ハード制約構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:526.7px;top:198.3px;width:197.1px;height:41.3px;border-color:#f1c40f;" title="document_p059_c00233 [h3]
Target HW
W-CPU1 / A-16 ( ギリギリ ) ( 低スペック PC / メモリ不
足 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:526.0px;top:258.7px;width:198.3px;height:96.6px;border-color:#f1c40f;" title="document_p059_c00234 [h3]
VAD &amp; ASR
短音声限定 or 夜間バッチ処理リアルタイム性は犠
牲にする
LLM 整形・抽出
3B 〜 4B 級 (Phi-3 mini 等 ) 「抽出のみ」に機能限定"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:536.3px;top:379.5px;width:177.8px;height:35.3px;border-color:#f1c40f;" title="document_p059_c00235 [h3]
Output Interface
抽出テンプレート固定自由生成を極力減らす"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 60 (6 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:50.2px;top:19.4px;width:685.4px;height:104.9px;border-color:#f1c40f;" title="document_p060_c00236 [h3]
音声メモ → 整文化 → タスク抽出(失敗モードと回避策) ユースケース別推奨スタック
結論:誤転記・要点漏れ・推論不足は「設計」で抑え込む。 VAD による区間分割、逐次要約、テンプレ固定が鍵となります。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:55.6px;top:180.1px;width:191.8px;height:59.5px;border-color:#f1c40f;" title="document_p060_c00237 [h3]
ASR 誤転記 → 抽出ミス
失敗モード
ASR が固有名詞や数値を誤認識し、そのままタスクと して抽出されてしまう。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:48.5px;top:180.1px;width:434.6px;height:169.1px;border-color:#f1c40f;" title="document_p060_c00238 [h3]
回避策( Design )
1. VAD で区間分割:無音除去で認識精度向上。 2. 重要箇所再確認 UI :抽出されたタスクの元音声を ワンクリック再生できる UI を提供。
Silero VAD Playback UI
長文での要点漏れ
失敗モード
コンテキスト長超過や Attention の分散により、後半 の重要事項が無視される。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:292.1px;top:257.6px;width:195.9px;height:91.6px;border-color:#f1c40f;" title="document_p060_c00239 [h3]
回避策( Design )
1. 逐次要約( Rolling Summary ):一定区間ごとに要 約し、次区間の入力に含める。
2. KV 節約:量子化 KV やウインドウ制限でメモリ枯渇 を防ぐ。
Rolling Context Quantized KV"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:539.1px;top:180.1px;width:189.8px;height:59.5px;border-color:#f1c40f;" title="document_p060_c00240 [h3]
LLM の推論不足
失敗モード
小型モデル( 3B-7B )が指示に従わず、タスク以外の 雑談や幻覚を出力する。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:60.0px;top:257.6px;width:670.7px;height:164.4px;border-color:#f1c40f;" title="document_p060_c00241 [h3]
回避策( Design )
1. テンプレ固定:自由生成を禁止し、抽出テンプレ ート( JSON Schema 等)を強制。
2. Few-shot 提示:プロンプトに抽出成功例を含める 。
JSON Schema Few-shot
システム連携のポイント
Ollama/LM Studio の OpenAI 互換 API を活用して業務アプリと接続する場合、受け取った JSON 出力の型チェック("><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 61 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:28.8px;top:17.3px;width:709.2px;height:408.1px;border-color:#5dade2;" title="document_p061_c00242 [body]
ユースケース:会議議事録(話者分離+要約+アクション) 推奨スタック 3 構成( Tier 別)
結論:規模と要件に応じて 3 構成を選択。無音除去( VAD )と発話ターン統合ルールが品質下限を決定します。 ※評価は p50/p95 、 RTF 、意味改変率で監視することを推奨します。
低コスト構成
前処理 (VAD)
Silero VAD [82] 無音区間を確実に除去
[82]
ASR ( "><span class="lbl" style="background:#5dade2;">body</span></div></div></div><div class="page-block"><h2>Page 62 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:19.4px;width:689.6px;height:103.9px;border-color:#f1c40f;" title="document_p062_c00243 [h3]
会議議事録:失敗モードと回避策 ユースケース詳細分析
結論:話者誤割当・ ASR 幻覚・長時間処理の負荷を設計で抑止。 VAD (無音除去)の厳格化とタイムスタンプベースの検証が品質の防波堤となります。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:59.2px;top:182.4px;width:306.4px;height:178.4px;border-color:#f1c40f;" title="document_p062_c00244 [h3]
主な失敗モード( Failure Modes )
話者誤割当( Diarization Error ) 高頻度
発話の切れ目が不明確で、 A さんの発言が B さんとして記録される。特に割り 込み発話で多発。
ASR 幻覚( Hallucination ) 致命的
無音区間やノイズに対して、存在しない文章(「ご視聴ありがとうございま した」等)を生成してしまう現象。
長文による要点漏れ
コンテキス"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:57.0px;top:182.4px;width:667.3px;height:250.9px;border-color:#f1c40f;" title="document_p062_c00245 [h3]
回避策と設計( Mitigation )
短い切れ目のマージ+手動マッピング 設計
極端に短い発話区間を前後の発話者に統合するルールを適用。参加者名と ID の紐付け UI を用意。
VAD 厳格化+無音区間破棄 前処理
Silero VAD 等の閾値を調整し、確実に音声がある区間のみ ASR へ渡す。「要審 査フラグ」で怪しい出力をマーク。
逐次要約( Rolling Summary )+ KV"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 63 (10 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:17.0px;width:681.3px;height:153.7px;border-color:#f1c40f;" title="document_p063_c00246 [h3]
ユースケース:文書 RAG PDF/ スキャン → OCR → 検索 → 回答
結論: RAG 品質は Embedding で上限が決まるため、「 Embedding で recall 確保 →Reranker で precision 向上」の二段構えが基本戦略で す。
低コスト構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:198.9px;width:181.4px;height:34.1px;border-color:#f1c40f;" title="document_p063_c00247 [h3]
OCR / Pre-process
Tesseract OCR ※スキャン品質が高い場合"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:255.1px;width:180.1px;height:34.1px;border-color:#f1c40f;" title="document_p063_c00248 [h3]
Embedding / Reranker
BGE-M3 ( 多用途 ) bge-reranker-base ( 軽量 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:158.6px;width:385.3px;height:235.6px;border-color:#f1c40f;" title="document_p063_c00249 [h3]
LLM Runtime
7B 級 4bit (CPU) llama.cpp / Ollama
テキスト中心の標準 PDF
高品質構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:198.9px;width:190.6px;height:45.5px;border-color:#f1c40f;" title="document_p063_c00250 [h3]
OCR / DocAI
PaddleOCR ( 構造化 ) Donut (OCR-free 情報抽出 )
LayoutLMv3 ( レイアウト保持 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:267.7px;width:174.3px;height:34.1px;border-color:#f1c40f;" title="document_p063_c00251 [h3]
Embedding / Reranker
BGE-M3 + 大型 reranker (GPU 推論推奨 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:158.6px;width:397.6px;height:234.8px;border-color:#f1c40f;" title="document_p063_c00252 [h3]
LLM Runtime
14B 〜 32B 級 (GPU) 図表・レイアウト情報を加味
図表・帳票を含む文書
ハード制約構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:198.9px;width:192.1px;height:34.1px;border-color:#f1c40f;" title="document_p063_c00253 [h3]
Processing Strategy
夜間バッチ処理 OCR ・ Embedding を夜間に生成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:255.1px;width:194.1px;height:34.1px;border-color:#f1c40f;" title="document_p063_c00254 [h3]
Daytime Operation
検索と短い回答のみ実行 LLM 推論負荷を最小化"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:311.3px;width:709.2px;height:127.4px;border-color:#f1c40f;" title="document_p063_c00255 [h3]
LLM Runtime
3B 〜 7B 級 (4bit/INT8) コンテキスト長を制限
リソース極小環境
Local AI Technical Survey Report 2026 63 / 80"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 64 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:19.4px;width:682.6px;height:388.2px;border-color:#f1c40f;" title="document_p064_c00256 [h3]
文書 RAG :失敗モードと回避策 OCR 誤読・検索失敗・幻覚への対策設計
結論:文書 RAG の失敗は「 OCR 誤読」「検索精度不足」「幻覚引用」に大別され、 前処理の標準化 と 原文スニペット引用 の UX 要件化で回避します。
主な失敗モード( Failure Modes )
スキャン品質低下や傾きにより、固有名詞や数値が誤認識され、正しい文書が ヒットしない。 Recall 低下
表組"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 65 (13 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:17.0px;width:684.1px;height:153.7px;border-color:#f1c40f;" title="document_p065_c00257 [h3]
ユースケース:画像理解( VLM ) — 3 構成 スクショ / 写真の説明・抽出:推奨スタック比較
結論: 2B 〜 7B 級 VLM がローカルの現実ライン。 72B 級は上位機前提となります。 ※視覚トークン肥大による速度 / メモリ急落を防ぐため、解像度・ max-pixels の制御が必須です。
低コスト構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:198.9px;width:150.7px;height:27.8px;border-color:#f1c40f;" title="document_p065_c00258 [h3]
Target HW
Apple 16GB / VRAM 8GB 級"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:242.5px;width:170.8px;height:34.1px;border-color:#f1c40f;" title="document_p065_c00259 [h3]
VLM Model
Qwen2-VL 2B 軽量・高速な視覚理解"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:298.7px;width:181.6px;height:39.2px;border-color:#f1c40f;" title="document_p065_c00260 [h3]
Runtime
mac: MLX-VLM ( 公式例あり ) Win: GPU 推論
(transformers 等 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:158.6px;width:385.3px;height:266.5px;border-color:#f1c40f;" title="document_p065_c00261 [h3]
Key Point
解像度を制限しメモリ圧迫を回避
スクショ説明・簡易 OCR
高品質構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:198.9px;width:159.0px;height:27.2px;border-color:#f1c40f;" title="document_p065_c00262 [h3]
Target HW
Apple 32GB+ / VRAM 16-24GB+"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:242.5px;width:199.5px;height:34.1px;border-color:#f1c40f;" title="document_p065_c00263 [h3]
VLM Model
Qwen2-VL 7B / InternVL2 ( 必要に応じて上位モデル )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:298.7px;width:202.6px;height:39.2px;border-color:#f1c40f;" title="document_p065_c00264 [h3]
Strategy
画像 → テキスト抽出 →Embedding →RAG 融合 ( ハイブ
リッド検索 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:158.6px;width:397.6px;height:265.7px;border-color:#f1c40f;" title="document_p065_c00265 [h3]
Key Point
OCR-free の図表理解能力を活用
図表理解・情報抽出
ハード制約構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:198.9px;width:151.8px;height:27.8px;border-color:#f1c40f;" title="document_p065_c00266 [h3]
Target HW
VRAM 不足 / メモリ制約大"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:242.5px;width:202.0px;height:38.3px;border-color:#f1c40f;" title="document_p065_c00267 [h3]
Substitute Strategy
OCR (PaddleOCR 等 ) + テキスト LLM VLM モデルを使用
しない"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:298.7px;width:201.7px;height:38.3px;border-color:#f1c40f;" title="document_p065_c00268 [h3]
Operation
OCR でテキスト化し、 LLM で整形画像入力自体を避
ける"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:354.8px;width:199.6px;height:69.4px;border-color:#f1c40f;" title="document_p065_c00269 [h3]
Compromise
空間認識・文脈理解を諦め文字情報の抽出・整理
に限定
文字主体の処理"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 66 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:46.9px;top:19.4px;width:687.0px;height:389.8px;border-color:#5dade2;" title="document_p066_c00270 [body]
画像理解:失敗モードと回避策 ユースケース別推奨スタック(詳細)
結論: UI 文字の誤読と視覚トークン肥大による速度低下を、 OCR 併用 と 解像度・ max-pixels 制御 で回避します。
主な失敗モード( Failure Modes )
2B/7B 級 VLM では、スクリーンショット内の小さなフォントや密集した 情報を正確に読み取れないケースが頻発。
高解像度画像をそのまま入力すると"><span class="lbl" style="background:#5dade2;">body</span></div></div></div><div class="page-block"><h2>Page 67 (10 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:46.9px;top:17.0px;width:680.7px;height:153.7px;border-color:#5dade2;" title="document_p067_c00271 [body]
ユースケース:コーディング補助 推奨スタック 3 パターン( IDE 支援・リポジトリ理解)
結論:補完は小型モデル、設計レビューや長距離依存解決は中〜大型モデルで役割分担します。 ※生成コードは自動コンパイル / テスト実行を “ ツール ” 化し、検証ループに組み込むことが重要です。
低コスト構成"><span class="lbl" style="background:#5dade2;">body</span></div><div class="bbox level-3" style="left:41.4px;top:198.9px;width:185.3px;height:34.1px;border-color:#f1c40f;" title="document_p067_c00272 [h3]
Code LLM
Qwen2.5-Coder 7B サイズ展開が豊富で軽量"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:255.1px;width:197.4px;height:39.2px;border-color:#f1c40f;" title="document_p067_c00273 [h3]
RAG / Embedding
リポジトリ Embedding (BGE/E5) 関連ファイル抽出
→LLM 回答"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:41.4px;top:158.6px;width:385.3px;height:223.0px;border-color:#f1c40f;" title="document_p067_c00274 [h3]
Connection
Ollama / LM Studio OpenAI 互換 API で IDE 拡張接続
一般的な IDE 補完・ Q&amp;A
高品質構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:198.9px;width:179.3px;height:34.1px;border-color:#f1c40f;" title="document_p067_c00275 [h3]
Code LLM
StarCoder2 (7B/15B) + 32B 級汎用 LLM 併用"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:255.1px;width:202.2px;height:38.3px;border-color:#f1c40f;" title="document_p067_c00276 [h3]
Optimization
prefix caching (vLLM 等 ) 長文コンテキストでの遅延抑
制"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:282.3px;top:158.6px;width:397.6px;height:222.1px;border-color:#f1c40f;" title="document_p067_c00277 [h3]
Capability
設計レビュー / 長距離依存複雑なリファクタリング
提案
大規模開発・設計支援
ハード制約構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:198.9px;width:179.8px;height:34.1px;border-color:#f1c40f;" title="document_p067_c00278 [h3]
Target HW
メモリ 8GB / エントリー機古い開発環境"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:523.3px;top:255.1px;width:173.0px;height:34.1px;border-color:#f1c40f;" title="document_p067_c00279 [h3]
Code LLM
3B 級モデル Phi-3 mini / StarCoder2-3B"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:311.3px;width:709.2px;height:117.6px;border-color:#f1c40f;" title="document_p067_c00280 [h3]
Strategy
短い支援に限定「行補完」+「関数リファクタ提
案」
スニペット補完のみ
Local AI Technical Survey Report 2026 67 / 80"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 68 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:50.2px;top:19.4px;width:679.1px;height:420.4px;border-color:#5dade2;" title="document_p068_c00281 [body]
コーディング支援:失敗モードと回避策 コーディング補助( IDE 支援・リポジトリ理解)
結論:幻覚 API 生成と長文遅延を「検証ループ」と「キャッシュ活用」で抑止し、 コード品質とセキュリティを自動化プロセスで担保します。
主な失敗モード( Failure Modes )
ライブラリのバージョン不一致や、もっともらしいが実在しない関数 ( Hallucination )を生成。
リポジトリ全体"><span class="lbl" style="background:#5dade2;">body</span></div></div></div><div class="page-block"><h2>Page 69 (13 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:46.9px;top:17.0px;width:684.7px;height:154.9px;border-color:#f1c40f;" title="document_p069_c00282 [h3]
ユースケース:画像生成( Diffusion ) — 3 構成 推奨スタックと VRAM 要件の最適化
結論: SDXL を軸に ComfyUI でワークフローを資産化。 FLUX 等はライセンス精査が必須です。 ※ VRAM 要件は解像度・バッチ・ステップ数に依存するため、プロファイル固定が重要です。
低コスト構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:49.9px;top:201.3px;width:72.2px;height:39.5px;border-color:#f1c40f;" title="document_p069_c00283 [h3]
Target VRAM
VRAM 8GB 〜 12GB (Apple Silicon 16GB)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:49.9px;top:264.1px;width:102.7px;height:39.6px;border-color:#f1c40f;" title="document_p069_c00284 [h3]
Model / UI
SDXL base 1.0 + Refiner なし ComfyUI (Win/Linux/macOS)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:49.9px;top:326.9px;width:84.9px;height:39.5px;border-color:#f1c40f;" title="document_p069_c00285 [h3]
Optimization
Attention Slicing 有効化 FP16 / Tiled VAE 活用"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:49.9px;top:159.8px;width:378.1px;height:269.4px;border-color:#f1c40f;" title="document_p069_c00286 [h3]
Resolution
1024x1024 固定 Batch size = 1
高品質構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:290.7px;top:201.3px;width:76.8px;height:39.5px;border-color:#f1c40f;" title="document_p069_c00287 [h3]
Target VRAM
VRAM 24GB 以上 (Apple Silicon 64GB+)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:290.7px;top:264.1px;width:117.4px;height:39.6px;border-color:#f1c40f;" title="document_p069_c00288 [h3]
Model / UI
FLUX.1 [dev/pro] 等の上位系 ComfyUI ( ワークフロー資産化 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:290.7px;top:326.9px;width:87.8px;height:39.5px;border-color:#f1c40f;" title="document_p069_c00289 [h3]
Optimization
Seed 固定で再現性担保 Refiner / LoRA 多重適用"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:290.7px;top:159.8px;width:390.4px;height:268.5px;border-color:#f1c40f;" title="document_p069_c00290 [h3]
License Check
FLUX.1-dev 等の商用条件 ライセンス精査が必須
ハード制約構成"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:531.7px;top:201.3px;width:84.6px;height:39.5px;border-color:#f1c40f;" title="document_p069_c00291 [h3]
Target VRAM
VRAM 8GB 未満 ( メインメモリ共有等 )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:531.7px;top:264.1px;width:110.2px;height:39.6px;border-color:#f1c40f;" title="document_p069_c00292 [h3]
Model / UI
SD 1.5 系 / LCM-LoRA ( 高速化 ) Stable Diffusion WebUI (Forge)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:531.7px;top:326.9px;width:128.7px;height:39.5px;border-color:#f1c40f;" title="document_p069_c00293 [h3]
Offloading strategy
夜間バッチ処理 または LAN 内別筐体へオフロード"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:531.7px;top:389.7px;width:103.0px;height:39.5px;border-color:#f1c40f;" title="document_p069_c00294 [h3]
Limitation
解像度 512x512 等に制限 日中は LLM にリソース集中"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 70 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:50.2px;top:19.4px;width:681.9px;height:415.6px;border-color:#5dade2;" title="document_p070_c00295 [body]
画像生成:失敗モードと回避策 VRAM 制約と品質・ライセンス管理
結論: VRAM 不足と生成結果のブレを 「標準プロファイル」 で抑制し、 ライセンス違反リスクを 「事前精査と監査」 で排除します。
主な失敗モード( Failure Modes )
高解像度や大バッチ指定時にプロセスがクラッシュ。特に SDXL/FLUX 等 の大型モデルで頻発。
同じプロンプトでも Seed や設定の違いで出"><span class="lbl" style="background:#5dade2;">body</span></div></div></div><div class="page-block"><h2>Page 71 (3 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-1" style="left:48.0px;top:91.6px;width:79.9px;height:80.4px;border-color:#e74c3c;" title="document_p071_c00296 [h1]
10"><span class="lbl" style="background:#e74c3c;">h1</span></div><div class="bbox level-3" style="left:48.0px;top:211.4px;width:198.7px;height:118.0px;border-color:#f1c40f;" title="document_p071_c00297 [h3]
測定と比較の 方法
ベンチマーク設計と 再現性の担保"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:393.8px;top:67.0px;width:309.8px;height:295.3px;border-color:#f1c40f;" title="document_p071_c00298 [h3]
KE Y TAKEAWAYS
本章の要点
指標の厳密定義:
p50/p95 レイテンシ、 TTFT 、 tok/s 、 RTF 等を定義し、 JSON 破 壊率や意味改変率も定量化します。
測定手順の標準化:
条件固定(温度・量子化等)、ウォームアップ分離、 KV 影 響の切り分け(短文 / 長文)を徹底します。
再現性のコア要素:
ASR/TTS の前処理固定と、最小テストセット(短文 / 長文"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 72 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:47.3px;top:16.3px;width:679.2px;height:418.2px;border-color:#5dade2;" title="document_p072_c00299 [body]
測定指標の定義 p50/p95 ・ TTFT ・ tok/s ・ RTF ・品質指標
結論: LLM の「初速( TTFT )」と「生成速度( tok/s )」を分離して計測し、 品質( JSON 破壊率等)とリソース消費( RAM/VRAM )を定量化します。
速度・リソース指標( Latency &amp; Resource )
TTFT ( Time To First Token )と生成完了時間を"><span class="lbl" style="background:#5dade2;">body</span></div></div></div><div class="page-block"><h2>Page 73 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:24.0px;top:15.7px;width:714.0px;height:409.7px;border-color:#f1c40f;" title="document_p073_c00300 [h3]
測定手順(条件固定・ウォームアップ・ KV 切り分け) 再現可能なベンチマーク設計の要件
再現条件の固定
生成パラメータの統一
同一プロンプト、最大トークン数、温度( temperature )、 top_p を固 定し、ランダム性を排除または制御します。
モデル環境の固定
同一の量子化形式( GGUF Q4_K_M など)、コンテキスト長設定を使 用します。
音声・画像系の前提
ASR/TTS "><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 74 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:46.9px;top:15.5px;width:693.8px;height:409.7px;border-color:#5dade2;" title="document_p074_c00301 [body]
リスクとコンプライアンス ライセンス管理・プライバシー・安全性対策
結論:「コードの OSS ライセンス」と「モデル重みの利用条件」を分離管理し、ローカル完結の利点を活かしつつ、検証ループ による安全性確保が必須です。
ライセンス管理
コードと重みの分離
ランタイム (MIT/Apache) とモデル重み (Community/ 非商用 ) は別 条件。利用範囲を台帳化する。
個別確認の義務化
Q"><span class="lbl" style="background:#5dade2;">body</span></div></div></div><div class="page-block"><h2>Page 75 (8 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-0" style="left:57.6px;top:15.5px;width:680.4px;height:16.7px;border-color:#5dade2;" title="document_p075_c00302 [body]
まとめ(意思決定フロー) ローカル AI 導入の 5 ステップと判断基準"><span class="lbl" style="background:#5dade2;">body</span></div><div class="bbox level-3" style="left:39.0px;top:77.2px;width:101.1px;height:212.8px;border-color:#f1c40f;" title="document_p075_c00303 [h3]
1 要件定義 Requirements Requirements
業務課題から技術要件へ変換す る始点。
判断基準 (Criteria)
品質 : 日本語、 JSON 、専門性
速度 : p95 レイテンシ、 tok/s
機能 : Tool-use 、 RAG 有無
アクション
許容レイテンシ目標(例 : 10 tok/s )と JSON スキーマの厳格 さを決定。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:186.0px;top:77.2px;width:100.8px;height:201.0px;border-color:#f1c40f;" title="document_p075_c00304 [h3]
2 Tier 選定 Hardware Tier Hardware Tier
メモリ制約に基づく現実ライン の把握。
判断基準 (Criteria)
Apple: 16 〜 128GB (UMA)
Windows: VRAM 8 〜 24GB
CPU: AVX/RAM 容量
アクション
「重み 4bit + KV キャッシュ」 の理論値で Tier (A-16 〜 G-24) を特定。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:332.9px;top:77.2px;width:105.2px;height:189.3px;border-color:#f1c40f;" title="document_p075_c00305 [h3]
3 モデル選定 Model Select Model Select
Tier 内で動く最適モデルの選定。
判断基準 (Criteria)
規模 : 7B-14B ( 汎用 ), 32B+ ( 高品 質 )
形式 : GGUF, EXL2, AWQ
権利 : ライセンス条件
アクション
Llama/Qwen/Phi 等からサイズ 適合候補を選び、ライセンス を確認。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:479.9px;top:77.2px;width:100.8px;height:78.9px;border-color:#f1c40f;" title="document_p075_c00306 [h3]
4 ランタイム Runtime Runtime
ハードウェア性能を引き出す実 行環境。
判断基準 (Criteria)"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:485.3px;top:162.2px;width:64.4px;height:7.5px;border-color:#f1c40f;" title="document_p075_c00307 [h3]
Mac: MLX / llama.cpp"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:485.3px;top:174.6px;width:94.7px;height:91.9px;border-color:#f1c40f;" title="document_p075_c00308 [h3]
Win: Ollama / LM Studio / vLLM
API: OpenAI 互換性
アクション
OS とモデル形式に合ったラン タイムを選択。 Server 機能の 有無を確認。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:77.2px;width:709.2px;height:350.7px;border-color:#f1c40f;" title="document_p075_c00309 [h3]
5 検証・最適化 Verify Verify
実機ベンチによる実用性の確定 。
判断基準 (Criteria)
指標 : TTFT, p95, JSON 破壊率
負荷 : メモリピーク , KV 推移
品質 : 幻覚 , 誤転記
アクション
条件固定ベンチで測定。 KV 量 子化やプロンプトキャッシュ で調整。
意思決定の要点 (Key Takeaways)
「ランタイム先行」ではなく「要件 → "><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 76 (8 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:50.2px;top:20.3px;width:685.0px;height:220.7px;border-color:#f1c40f;" title="document_p076_c00310 [h3]
総括(実務への適用) 実装の原則と次のステップ
結論:ローカル AI は「 4bit 量子化+ KV 最適化」を前提に、 失敗モードを設計で抑え込むことで実務運用が可能です。
実装の原則( Principles )
重みは 4bit ( GGUF/AWQ/GPTQ )でメモリ理論値を計算し、 Tier に合わせ る。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:55.9px;top:241.1px;width:312.0px;height:56.2px;border-color:#f1c40f;" title="document_p076_c00311 [h3]
Cost-Efficiency
長文・多同時接続時は、 paged KV 、 KV 量子化、 prompt cache を活用してメ モリ爆発を防ぐ。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:55.9px;top:297.2px;width:307.5px;height:56.2px;border-color:#f1c40f;" title="document_p076_c00312 [h3]
Scalability
幻覚・誤転記・ JSON 破壊は「起きるもの」とし、 UI 確認・検証ループ・ 再生成でカバーする。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:55.9px;top:177.3px;width:528.2px;height:232.2px;border-color:#f1c40f;" title="document_p076_c00313 [h3]
Robustness
p50/p95 レイテンシと品質( tok/s, RTF, JSON 破壊率)を定量ベンチマーク で測定し確定させる。
次のステップ( Action Items )"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:421.2px;top:213.9px;width:296.2px;height:30.6px;border-color:#f1c40f;" title="document_p076_c00314 [h3]
1
Tier の確定: 手持ちハードウェア( Apple/Win )と Tier 表を照合し、現実的なモデル規模を把 握。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:421.2px;top:274.0px;width:295.3px;height:30.6px;border-color:#f1c40f;" title="document_p076_c00315 [h3]
2
パイロット構築: 推奨スタック(低コスト / 高品質)に基づき、 llama.cpp/Ollama/faster-whisper 等 でプロトタイプ作成。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:421.2px;top:334.1px;width:296.3px;height:21.7px;border-color:#f1c40f;" title="document_p076_c00316 [h3]
3
ベンチマーク実施: 条件固定(温度 / トークン数)でログを取り、 p95 レイテンシと実用性を計測。"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:394.2px;width:709.2px;height:31.2px;border-color:#f1c40f;" title="document_p076_c00317 [h3]
4
リスク監査: ライセンス(特に画像生成 /TTS )とプライバシー要件(外部送信なし)を最終 確認。 Local AI Technical Survey Report 2026 76 / 80"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 77 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:28.8px;top:27.8px;width:690.8px;height:397.4px;border-color:#f1c40f;" title="document_p077_c00318 [h3]
References ( 1/4 ) Technical Research Report Sources [1-34]
[1] https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/
[2] https://docs.openhands.dev/openhands/usage/llms/local-llms
[3] https://lm"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 78 (2 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:36.6px;top:27.8px;width:670.1px;height:175.1px;border-color:#f1c40f;" title="document_p078_c00319 [h3]
References (参考文献リスト 2/4 ) [35] ~ [68] Llama / Qwen / Gemma / Phi / VLM / Whisper / TTS / OCR
NO. S OU RC E U R L / D ES CR IPTION
[35] G em m a 2 M odel C ard https://ai.google.dev/gemma/docs/core/mod"><span class="lbl" style="background:#f1c40f;">h3</span></div><div class="bbox level-3" style="left:28.8px;top:99.7px;width:702.9px;height:340.1px;border-color:#f1c40f;" title="document_p078_c00320 [h3]
[41]
Op en AI C ook bo ok (R un locally LM S tu dio) https://developers.openai.com/cookbook/articles/gpt-oss/run-locally- lmstudio/
[42] Qw en2- VL 72B https://huggingface.co/Qwen/Qwen2-VL-72B
[43] Ph"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 79 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:28.8px;top:27.8px;width:705.7px;height:397.4px;border-color:#f1c40f;" title="document_p079_c00321 [h3]
References ( 3/4 ) 参考文献リスト [69] - [102]
N o . R eference D etails (Layout / Im ag e Gen / Voice / H W )
[69] LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking https://arxiv."><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div><div class="page-block"><h2>Page 80 (1 chunks)</h2><div class="canvas" style="width:476px;height:674px;"><div class="bbox level-3" style="left:28.8px;top:27.8px;width:705.4px;height:409.9px;border-color:#f1c40f;" title="document_p080_c00322 [h3]
References (参考文献リスト 4/4 ) 音声周辺・ベンチマーク・最適化技術・ハードウェア関連 [103] - [136]
[103
]
Phi-3-m ini-128k-instruct M odel C ard https://huggingface.co/microsoft/Phi-3-mini-128k-instruct
[104
]
vLLM: Qua ntized KV Ca"><span class="lbl" style="background:#f1c40f;">h3</span></div></div></div></body></html>
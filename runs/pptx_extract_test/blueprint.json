{
  "schema_version": "0.1",
  "document_id": "2",
  "theme_id": "theme_default",
  "slides": [
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "2026年2月15日 | Generated by Genspark AI Slides",
      "slots": {
        "TITLE": "2026年2月15日 | Generated by Genspark AI Slides",
        "BULLETS": [
          "Technical Research Report",
          "ローカルAI 技術調査レポート",
          "オンデバイス／ローカル推論の現実解と推奨スタック",
          "LLM",
          "VLM",
          "ASR",
          "TTS",
          "RAG"
        ]
      },
      "citations": [
        {
          "mark": "※1",
          "page": 1,
          "chunk_id": "s001_sh004"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "このスライドの読み方",
      "slots": {
        "TITLE": "このスライドの読み方",
        "BULLETS": [
          "概要→詳細の二層構造ガイド",
          "構造：二層設計",
          "1. 概要スライド（Executive Summary）",
          "意思決定に必要な「結論」と「要点」を最初に提示します。時間がない場合はここだけ読めば全体像が掴めます。",
          "2. 完全版スライド（Full Detail）",
          "レポート内の表、数値、グラフ、注釈を省略せずに掲載します。エンジニアや実装担当者が参照するための詳細情報です。",
          "相互リンクと参照",
          "概要から詳細へ、詳細から参考文献へ、論理的に接続されています。"
        ]
      },
      "citations": [
        {
          "mark": "※2",
          "page": 2,
          "chunk_id": "s002_sh004"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Local AI Technical Report",
      "slots": {
        "TITLE": "Local AI Technical Report",
        "BULLETS": [
          "目次（Agenda）",
          "Total 13 Chapters",
          "01",
          "タイトル／目的／読み方",
          "レポートの目的と、概要→詳細の二層構造の活用方法。",
          "02",
          "エグゼクティブサマリ",
          "実務要点、中核技術、推奨スタックの概要を提示。"
        ]
      },
      "citations": [
        {
          "mark": "※3",
          "page": 3,
          "chunk_id": "s003_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "エグゼクティブサマリ（実務要点）",
      "slots": {
        "TITLE": "エグゼクティブサマリ（実務要点）",
        "BULLETS": [
          "ローカルAI導入における意思決定の重要ポイント",
          "結論：ローカルAIは「4bit量子化＋KV管理」を前提に、\n GGUF/llama.cpp系とOllama/LM Studio/MLXで実務化可能です。",
          "定義とスコープ",
          "推論が端末orローカルLAN内で完結",
          "入力データは外部へ送信されない",
          "LANサーブ含む（LM Studio, Ollama）",
          "中核技術",
          "Weight-only 4bit (AWQ等)"
        ]
      },
      "citations": [
        {
          "mark": "※4",
          "page": 4,
          "chunk_id": "s004_sh004"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "エグゼクティブサマリ（詳細1）",
      "slots": {
        "TITLE": "エグゼクティブサマリ（詳細1）",
        "BULLETS": [
          "ローカルAIの定義と中核技術",
          "結論：ローカルAIは「推論がユーザー管理下で完結する構成」と定義され、 \n重み量子化（4bit）とKVキャッシュ最適化が実運用の技術的基盤です。",
          "ローカルAIの定義と射程",
          "ユーザー端末（オンデバイス）、PC、またはローカルLAN内サーバで完結。",
          "入力データ（文書・音声・画像）がデフォルトでクラウドへ送出されない構成。",
          "LM StudioやOllamaの「localhost/network公開」機能を含みます。",
          "[1][2]",
          "Localhost"
        ]
      },
      "citations": [
        {
          "mark": "※5",
          "page": 5,
          "chunk_id": "s005_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "エグゼクティブサマリ（詳細2）",
      "slots": {
        "TITLE": "エグゼクティブサマリ（詳細2）",
        "BULLETS": [
          "実運用ランタイムとハードウェア実用ライン",
          "結論：ランタイムはllama.cpp/GGUF系・MLXが第一選択となり、ハードウェアはメモリ容量でTier化されます（長文はKV支配）。",
          "実運用ランタイム（第一選択）",
          "クロスプラットフォーム標準：",
          "llama.cpp",
          "GGUF形式必須。最小セットアップでCPU/GPU推論が可能。",
          "軽量・汎用で多くのフロントエンドの基盤。[8]",
          "UI＋ローカルAPI："
        ]
      },
      "citations": [
        {
          "mark": "※6",
          "page": 6,
          "chunk_id": "s006_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "エグゼクティブサマリ（詳細3）",
      "slots": {
        "TITLE": "エグゼクティブサマリ（詳細3）",
        "BULLETS": [
          "推奨スタック3パターン（Tier別構成案）",
          "結論：用途とリソースに応じて「低コスト／高品質／ハード制約」の3構成を選択します。\n※各構成の具体的な失敗モードと回避策は、後述の「ユースケース別推奨スタック」章で詳細に展開します。",
          "低コスト構成",
          "Target HW",
          "CPUのみ / エントリーGPU / Apple 16GB",
          "LLM Runtime",
          "llama.cpp (GGUF Q4/K) Ollama / LM Studio",
          "ASR / TTS"
        ]
      },
      "citations": [
        {
          "mark": "※7",
          "page": 7,
          "chunk_id": "s007_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ローカルAIの定義とスコープ",
      "slots": {
        "TITLE": "ローカルAIの定義とスコープ",
        "BULLETS": [
          "Definition & Scope",
          "DEFINITION",
          "本資料の「ローカルAI」とは、推論がユーザー管理下（端末／ローカルPC／ローカルLAN）で完結し、入力データが外部へ送信されない構成を指します。",
          "スコープの射程（範囲）",
          "ローカルAPIサーバを含む:\nlocalhostだけでなく、LAN公開（network）された推論サーバも対象。",
          "LM Studio / Ollama",
          "実装例:\nWindows/macOS/Linux上で動作するOllama、LM Studio等のOpenAI互換APIサーバ。",
          "非対象:\n推論リクエストがインターネット経由で外部クラウドAPI（OpenAI, Anthropic等）へ飛ぶ構成。"
        ]
      },
      "citations": [
        {
          "mark": "※8",
          "page": 8,
          "chunk_id": "s008_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "未指定事項の扱い（前提条件）",
      "slots": {
        "TITLE": "未指定事項の扱い（前提条件）",
        "BULLETS": [
          "オフライン要件・対象OS・ライセンス",
          "結論：「完全オフライン」は推論実行時のみを必須要件とし、 \n 導入・更新時のネットワーク利用は許容する現実的な設計を前提とします。",
          "オフライン要件と対象OS",
          "“完全オフライン”の定義：",
          "「モデル推論自体はオフラインで成立する」レベルを基本とします。モデルの初回ダウンロードやRAG文書の取り込みプロセスにはネットワーク接続が必要になり得る点を前提とします。",
          "[19]",
          "対象OSのスコープ：",
          "macOS Ventura以降およびWindows 10/11を主要ターゲットとします。Linuxは「Windows上でWSL2を利用」または「別筐体サーバ」の選択肢としてのみ扱います。"
        ]
      },
      "citations": [
        {
          "mark": "※9",
          "page": 9,
          "chunk_id": "s009_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "04",
      "slots": {
        "TITLE": "04",
        "BULLETS": [
          "ローカルAI\nランドスケープ",
          "オンデバイス実行環境の\n全体像と技術スタック",
          "KEY TAKEAWAYS",
          "本章の要点",
          "テキストLLMの二大潮流：\n GGUF（llama.cpp系）とGPU量子化形式（GPTQ/AWQ/EXL2等）に大別されます。",
          "プラットフォームの最適解：\n macOSはMetal/Unified Memory、WindowsはOllama/LM Studioが実務的選択肢です。",
          "構成要素の多様性：\n LLMだけでなく、ASR/TTS/VLM/RAG/Agentまで網羅的にマップ化します。",
          "カテゴリ網羅図"
        ]
      },
      "citations": [
        {
          "mark": "※10",
          "page": 10,
          "chunk_id": "s010_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ローカルAIランドスケープ",
      "slots": {
        "TITLE": "ローカルAIランドスケープ",
        "BULLETS": [
          "カテゴリ網羅・代表モデルとランタイムの関係図",
          "Local AI Technical Survey Report 2026",
          "11 / 80",
          "結論：ローカルAIは9カテゴリ（LLM/VLM/ASR/TTS/Embedding/OCR/画像生成/Agent/音声前処理）で構成され、\n モデル系とランタイム系が実務上の結節点となります。",
          "Core (AI Category)",
          "Model Examples",
          "Runtime / Stack"
        ]
      },
      "citations": [
        {
          "mark": "※11",
          "page": 11,
          "chunk_id": "s011_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ローカルAIランドスケープ（詳細）",
      "slots": {
        "TITLE": "ローカルAIランドスケープ（詳細）",
        "BULLETS": [
          "カテゴリ別代表モデル・ランタイム・主用途",
          "結論：実務上の収束点は「LLMはGGUF (llama.cpp)かGPU量子化系」「macOSはMLX/Unified Memory最適化」「WindowsはOllama/LM Studio + 必要に応じWSL2」。",
          "テキストLLM",
          "形式:",
          "GGUF (llama.cpp必須), GPTQ/AWQ/EXL2 (GPU向け)",
          "[21]",
          "代表:",
          "Llama 3.1, Qwen2.5, Gemma 2, Phi-3/4"
        ]
      },
      "citations": [
        {
          "mark": "※12",
          "page": 12,
          "chunk_id": "s012_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "05",
      "slots": {
        "TITLE": "05",
        "BULLETS": [
          "特徴軸（評価軸）\nの定義",
          "評価の共通物差しと\n測定基準",
          "KEY TAKEAWAYS",
          "本章の要点",
          "13の評価軸：\n 指示追従、JSON堅牢性、長文耐性、速度、メモリなど、多角的な視点でモデルを比較評価します。",
          "KVキャッシュ最適化：\n paged/quantized KVやreuse機能が、長文コンテキストや多同時リクエスト処理の鍵となります。",
          "JSON制約の重要性：\n ツール利用においてJSON出力の安定性は必須ですが、機能はランタイムに強く依存します。",
          "13の評価軸定義"
        ]
      },
      "citations": [
        {
          "mark": "※13",
          "page": 13,
          "chunk_id": "s013_sh007"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Evaluation Metrics Overview",
      "slots": {
        "TITLE": "Evaluation Metrics Overview",
        "BULLETS": [
          "特徴軸（評価軸）の定義",
          "Total 13 Metrics",
          "01",
          "指示追従 (Instruction Following)",
          "プロンプトの制約・禁止事項・形式指定（箇条書き禁止等）を遵守する能力。",
          "02",
          "幻覚耐性 (Hallucination Resistance)",
          "根拠のない固有名詞や数値を捏造せず、不確実性を提示できる性質。"
        ]
      },
      "citations": [
        {
          "mark": "※14",
          "page": 14,
          "chunk_id": "s014_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "特徴軸（詳細）：測定と設計含意",
      "slots": {
        "TITLE": "特徴軸（詳細）：測定と設計含意",
        "BULLETS": [
          "13の評価軸における計測手順と技術的背景",
          "機能・性能評価のコア",
          "JSON堅牢性と指示追従",
          "ツール呼び出し等の業務連携ではスキーマ破壊が致命的。Ollama/LM StudioのOpenAI互換APIを用いて同一条件で比較検証を行う。llama-cpp-python等のランタイム依存機能（response_format）も活用。",
          "[24]",
          "[25]",
          "長文耐性とメモリ管理",
          "コンテキスト長に応じてKVキャッシュメモリは線形に増加し、支配的要因となる。単純なトークン数だけでなく、KVキャッシュの最適化技術（Paged Attention等）の有無が安定性を左右する。"
        ]
      },
      "citations": [
        {
          "mark": "※15",
          "page": 15,
          "chunk_id": "s015_sh004"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "06",
      "slots": {
        "TITLE": "06",
        "BULLETS": [
          "カテゴリ別\nモデルカタログ",
          "主要9カテゴリの\n詳細データと採用判断",
          "SECTION OVERVIEW",
          "提示方針：三層構造",
          "概要（Overview）：\n カテゴリごとの主要トレンドと代表モデルの要点を概説。",
          "完全版表（Full Data）：\n 規模・量子化・得意不得意・ランタイム相性・リスク・参考文献を列落ちなく全掲載。",
          "採用判断（Decision）：\n 「どう選ぶか」の意思決定基準と失敗回避策を提示。",
          "テキストLLM"
        ]
      },
      "citations": [
        {
          "mark": "※16",
          "page": 16,
          "chunk_id": "s016_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "テキストLLM（汎用）概要",
      "slots": {
        "TITLE": "テキストLLM（汎用）概要",
        "BULLETS": [
          "主要7モデルと実運用環境",
          "結論：ローカル実務のボリュームゾーンは7B〜14B（4bit）。 \n 形式はGGUF（llama.cpp系）かGPU量子化（GPTQ/AWQ/EXL2）の二択が現実解です。",
          "代表モデル群（要点）",
          "Llama 3.1 Instruct: 8B/70B/405B系。多言語対話最適化を明記。",
          "[29-31]",
          "Qwen2.5 Instruct: 0.5〜72B。幅広いサイズ展開と高性能。",
          "[32-33]",
          "Gemma 2: 2B/9B/27B。9Bがローカル向き。責任ある利用を明示。"
        ]
      },
      "citations": [
        {
          "mark": "※17",
          "page": 17,
          "chunk_id": "s017_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "テキストLLM（完全版表1）",
      "slots": {
        "TITLE": "テキストLLM（完全版表1）",
        "BULLETS": [
          "主要モデル詳細比較：Llama 3.1 / Qwen2.5 / Gemma 2 / Mixtral",
          "採用判断の要点：ローカル実務のボリュームゾーンは7B〜14B級（4bit）です。3B級はハード制約時、32B以上は高品質要件時（メモリ増設前提）に選択します。ライセンスはファミリー内でも異なる場合があるため、必ず最新のモデルカードを確認してください。",
          "Page 18 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [29-37]"
        ]
      },
      "citations": [
        {
          "mark": "※18",
          "page": 18,
          "chunk_id": "s018_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "テキストLLM（完全版表2）",
      "slots": {
        "TITLE": "テキストLLM（完全版表2）",
        "BULLETS": [
          "主要モデル詳細比較：Phi-3 Mini / Phi-4 / gpt-oss",
          "採用判断の要点：14Bクラス（Phi-4等）はローカル運用の「上位実用ライン」であり、32GB以上のメモリ環境が推奨されます。長文コンテキスト（128K等）を活用する場合は、モデルサイズだけでなくKVキャッシュのメモリ消費が支配的になるため、メモリ設計（KV量子化やコンテキスト長制限）が不可欠です。",
          "Page 19 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [38-41]"
        ]
      },
      "citations": [
        {
          "mark": "※19",
          "page": 19,
          "chunk_id": "s019_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "採用判断基準（テキストLLM）",
      "slots": {
        "TITLE": "採用判断基準（テキストLLM）",
        "BULLETS": [
          "選定フローと実務上の重要ポイント",
          "結論：モデル選定は「必要品質→許容レイテンシ→許容メモリ→形式→ランタイム」の順に行うのが、実務上最も破綻しにくいフローです。",
          "実務の現実ライン",
          "7B〜14Bがボリュームゾーン\n 一般業務やRAGにおいて、品質と速度のバランスが良いスイートスポット。",
          "3B級はハード制約モード\n メモリ不足時のフォールバックや、抽出特化タスクで使い分ける。",
          "形式とライセンス",
          "形式先行は詰まりやすい\n ランタイム（例: LM Studio使いたい）から入ると形式制約で選択肢が狭まるため逆順推奨。",
          "ライセンスの個別確認\n Qwen2.5等、同ファミリー内でもサイズにより条件が異なる例外あり。"
        ]
      },
      "citations": [
        {
          "mark": "※20",
          "page": 20,
          "chunk_id": "s020_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "VLM/LMM 概要（画像理解・マルチモーダル）",
      "slots": {
        "TITLE": "VLM/LMM 概要（画像理解・マルチモーダル）",
        "BULLETS": [
          "カテゴリ別モデルカタログ",
          "結論：ローカル運用の現実ラインは2B/7B級が中心。 \n 画像トークン化の前処理依存が強く、ランタイムの明示サポートが重要です。",
          "代表モデルと特徴",
          "Qwen2-VL (2B/7B/72B)",
          "[42]",
          "ローカル現実ラインの主力。2B/7Bが実用的。72Bは上位ハード（96GB+）前提。",
          "Phi-3-Vision (128K)",
          "[43]"
        ]
      },
      "citations": [
        {
          "mark": "※21",
          "page": 21,
          "chunk_id": "s021_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "VLM/LMM（完全版表）",
      "slots": {
        "TITLE": "VLM/LMM（完全版表）",
        "BULLETS": [
          "Qwen2-VL / Phi-3-Vision / InternVL2 / LLaVA 詳細比較",
          "採用判断の要点：画像理解はテキストLLMより前処理・プロセッサ依存が強いため、「ランタイムがそのモデルを明示サポートしているか」を一次確認してください（例：MLX-VLMがQwen2-VLのコマンド提示）。72B級はローカルでは上位機（96GB+等）が必要で、現実は2B/7B級で設計します。",
          "Page 22 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [28, 42-47]"
        ]
      },
      "citations": [
        {
          "mark": "※22",
          "page": 22,
          "chunk_id": "s022_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "採用判断基準（VLM/LMM）",
      "slots": {
        "TITLE": "採用判断基準（VLM/LMM）",
        "BULLETS": [
          "ランタイム適合性と運用設計",
          "結論：画像理解は前処理・プロセッサ依存が強いため、「ランタイムの明示サポート確認」が最優先です。2B/7B級を基本とし、不足分をRAG/OCRで補うのが安全策です。",
          "ランタイム適合性",
          "一次情報の確認\n ランタイムがモデルを明示サポートしているか確認が安全（例: MLX-VLMのQwen2-VL対応コマンド提示）。",
          "プロセッサ依存性\n テキストLLMより前処理（画像トークン化）の依存が強く、変換ミスが起きやすい。",
          "運用設計（2B/7B級）",
          "現実的なライン\n 72B級は96GB+メモリや複数GPUが必要なため、実務では2B/7B級での運用設計に寄せる。",
          "視覚トークン制御\n 長文・多画像時はKV支配とトークン肥大に注意。max-pixels/マルチ画像数を制御する。"
        ]
      },
      "citations": [
        {
          "mark": "※23",
          "page": 23,
          "chunk_id": "s023_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ASR（音声認識）概要",
      "slots": {
        "TITLE": "ASR（音声認識）概要",
        "BULLETS": [
          "カテゴリ別モデルカタログ",
          "結論：ローカルASRは「RTF・メモリ・誤転記」の同時最適化が必須。\nfaster-whisperを基準実装とし、速度と精度（幻覚抑制）のバランスを図ります。",
          "Whisper系実装の比較",
          "主な技術要素:",
          "CTranslate2",
          "VAD Integration",
          "INT8 Quantization",
          "Beam Search"
        ]
      },
      "citations": [
        {
          "mark": "※24",
          "page": 24,
          "chunk_id": "s024_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ASR（完全版表）",
      "slots": {
        "TITLE": "ASR（完全版表）",
        "BULLETS": [
          "主要モデル・方式比較：Whisper / faster-whisper / whisper.cpp",
          "採用判断の要点：ローカルASRは「RTF（実時間比）」「メモリ」「誤転記リスク」の3要素を同時に満たす必要があります。faster-whisperは速度・メモリ面での利点を明示しているため、まずはこれを基準実装として検証し、要件に応じてGPU化やモデルサイズの調整を行うのが合理的です。",
          "Page 25 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [48-51]"
        ]
      },
      "citations": [
        {
          "mark": "※25",
          "page": 25,
          "chunk_id": "s025_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "採用判断基準（ASR）",
      "slots": {
        "TITLE": "採用判断基準（ASR）",
        "BULLETS": [
          "RTF要件と誤転記リスクに基づく選定フロー",
          "結論：faster-whisperを基準実装とし、RTF要件・誤転記リスクに応じてGPU化やモデル拡大を検討する段階的最適化が合理的です。",
          "前処理の固定",
          "VAD設定の統一\n 無音区間の除去は認識精度と速度に直結するため、Silero VAD等の設定を固定し再現性を担保する。",
          "ベンチマーク条件\n 同一音声・同一前処理での比較が必須（後章のベンチ設計参照）。",
          "会議用途の要点",
          "話者分離が品質の土台\n pyannote.audio等の話者分離（Diarization）精度が議事録の質を左右する。",
          "無音除去の設計\n 幻覚（無音区間にテキストが入る現象）を防ぐため、VADによる無音区間の破棄を徹底する。"
        ]
      },
      "citations": [
        {
          "mark": "※26",
          "page": 26,
          "chunk_id": "s026_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "TTS（音声合成）概要",
      "slots": {
        "TITLE": "TTS（音声合成）概要",
        "BULLETS": [
          "読み上げと音声クローンの分離設計",
          "結論：「読み上げ」は軽量モデル（Piper/Kokoro）で常駐化し、 \n 「音声クローン」は高品質モデル（XTTS/StyleTTS2）で分離設計します。",
          "代表システムと特徴",
          "Piper: \"fast, local neural TTS\"を標榜。省リソース・常駐向き。",
          "[52]",
          "Kokoro: 82Mパラメータで高品質・高効率。",
          "[53]",
          "XTTS-v2: 数秒の参照音声で多言語クローンが可能。"
        ]
      },
      "citations": [
        {
          "mark": "※27",
          "page": 27,
          "chunk_id": "s027_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "TTS（完全版表）",
      "slots": {
        "TITLE": "TTS（完全版表）",
        "BULLETS": [
          "主要モデル詳細比較：Piper / Kokoro / XTTS-v2 / StyleTTS2",
          "採用判断の要点：「読み上げ（通知・要約）」と「クローン（本人声）」は別物として分離設計します。前者はPiper/Kokoroのような軽量系、後者はXTTS等で、法務・倫理リスク管理を別レイヤに置きます。",
          "[56]",
          "Page 28 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [52-56]"
        ]
      },
      "citations": [
        {
          "mark": "※28",
          "page": 28,
          "chunk_id": "s028_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "採用判断基準（TTS）",
      "slots": {
        "TITLE": "採用判断基準（TTS）",
        "BULLETS": [
          "品質・コスト・リスクの分離管理",
          "結論：「読み上げ（通知・要約）」と「クローン（本人声）」は別レイヤで運用すべきです。前者は軽量化し、後者は法務・倫理リスクを管理します。",
          "分離運用（Layered）",
          "軽量TTSの常駐\n 通知・要約読み上げはPiper/Kokoro等で省リソース化し、システムに常駐させる。",
          "クローンは別系統\n XTTS等はGPUリソースを消費するため、必要な時のみ呼び出す別サービスとして切り出す。",
          "リスク・コンプライアンス",
          "権利同意の必須化\n 音声クローンは法務・倫理リスクが高い。業務利用時は対象者の書面同意をプロセス化する。",
          "ログ監査\n 「誰が・いつ・何を」生成したかのログを保存し、不正利用を追跡可能にする。"
        ]
      },
      "citations": [
        {
          "mark": "※29",
          "page": 29,
          "chunk_id": "s029_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Embedding / Reranker 概要",
      "slots": {
        "TITLE": "Embedding / Reranker 概要",
        "BULLETS": [
          "検索・RAG基盤モデル",
          "結論：RAGシステムの品質上限はEmbeddingの検索精度で決まります。 \n まずBGE-M3等でRecallを確保し、RerankerでPrecisionを向上させる二段構えが定石です。",
          "Embedding (ベクトル検索)",
          "Retrieval",
          "Vector DB",
          "Multi-Functionality / Multi-Linguality / Multi-Granularityを特徴とし、多言語・多用途で強力なベースラインとなります。",
          "[58]",
          "多言語検索の定番モデル。日本語を含む多言語環境での実績が豊富です。"
        ]
      },
      "citations": [
        {
          "mark": "※30",
          "page": 30,
          "chunk_id": "s030_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Embedding/Reranker（完全版表）",
      "slots": {
        "TITLE": "Embedding/Reranker（完全版表）",
        "BULLETS": [
          "検索・RAG基盤：BGE-M3 / multilingual-e5 / bge-reranker",
          "採用判断の要点：RAGの失敗の多くは「検索外れ」「上位が弱い」「文脈過多」です。EmbeddingでRecallを確保し、rerankerでPrecisionを上げる二段構えが実務的推奨です。ローカル運用では、Embeddingの前計算（夜間バッチ）により実時間の負荷をLLM応答に集中させる設計が有効です。",
          "Page 31 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [58-62]"
        ]
      },
      "citations": [
        {
          "mark": "※31",
          "page": 31,
          "chunk_id": "s031_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "採用判断基準（Embedding / Reranker）",
      "slots": {
        "TITLE": "採用判断基準（Embedding / Reranker）",
        "BULLETS": [
          "検索精度と運用安定化のための二段構え設計",
          "結論：Embeddingで網羅性(Recall)を確保し、Rerankerで精度(Precision)を上げる「二段構え」が実務的な最適解です。",
          "運用の安定化",
          "前計算（夜間バッチ）\n 検索対象文書のEmbedding化やOCR処理は夜間にバッチ実行し、日中の計算資源をLLMの応答生成に集中させる。",
          "インデックス更新\n 頻繁な更新が必要ない場合は、静的インデックスとして管理し、運用負荷を下げる。",
          "失敗パターンと回避策",
          "検索が外れる（Recall不足）\n 回避策：文書の前処理（チャンク分割）を見直す、またはHybrid検索（キーワード検索併用）を導入する。",
          "上位文書の関連度が弱い\n 回避策：Rerankerを導入して上位の並び順を補正し、LLMに渡すノイズを減らす。"
        ]
      },
      "citations": [
        {
          "mark": "※32",
          "page": 32,
          "chunk_id": "s032_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "OCR / Document AI 概要",
      "slots": {
        "TITLE": "OCR / Document AI 概要",
        "BULLETS": [
          "スキャン品質と構造化要件に基づく手法選択",
          "結論：スキャン品質やレイアウトの複雑度に応じて、 \n古典的OCRとOCR-free（Doc理解）を使い分けるハイブリッド戦略が推奨されます。",
          "代表的なシステム・モデル",
          "Tesseract OCR",
          "[64]",
          "古典的OCR＋LSTM。単純なテキスト化のデファクトスタンダード。",
          "PaddleOCR",
          "[66]"
        ]
      },
      "citations": [
        {
          "mark": "※33",
          "page": 33,
          "chunk_id": "s033_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "OCR / Document AI（完全版表）",
      "slots": {
        "TITLE": "OCR / Document AI（完全版表）",
        "BULLETS": [
          "主要モデル詳細比較：Tesseract / PaddleOCR / Donut / LayoutLMv3",
          "レイアウト保持の重要性：RAGにおいて、単なるテキスト化では図表やレイアウト構造に含まれる情報が欠落しがちです。PaddleOCRの構造化出力や、Donut/LayoutLMv3のようなDocument AIモデルを活用し、レイアウト情報を保持することが検索精度の向上に直結します。",
          "Page 34 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [64, 66, 68, 69]"
        ]
      },
      "citations": [
        {
          "mark": "※34",
          "page": 34,
          "chunk_id": "s034_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "OCR/Document AI採用判断基準",
      "slots": {
        "TITLE": "OCR/Document AI採用判断基準",
        "BULLETS": [
          "リスク緩和のためのUX要件とハイブリッド構成",
          "結論：誤読リスクをゼロにはできない前提で、前処理の標準化と「原文引用」による人間系確認のUXを要件化します。",
          "前処理の標準化",
          "品質の下限を担保\n 傾き補正、ノイズ除去、二値化をOCR前段に固定的に組み込むことで、認識精度のベースラインを確保。",
          "画像の正規化\n 解像度やコントラストのバラつきを抑え、モデルの得意な入力形式に合わせる。",
          "高品質方針：ハイブリッド",
          "構造化とレイアウト保持\n PaddleOCRでテキスト構造化を行い、Donut/LayoutLMv3でレイアウト情報を保持してRAG連携精度を高める。",
          "図表・UIへの対応\n 複雑な図表やUIスクショはVLMとOCRを併用し、視覚情報と文字情報を相互補完させて堅牢化する。"
        ]
      },
      "citations": [
        {
          "mark": "※35",
          "page": 35,
          "chunk_id": "s035_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "画像生成（Diffusion）概要",
      "slots": {
        "TITLE": "画像生成（Diffusion）概要",
        "BULLETS": [
          "モデル・ランタイム・実務ポイント",
          "結論：画像生成はSDXLを軸に、ComfyUI/SD WebUIで運用。\n FLUX等はライセンス精査が必須であり、VRAM要件はワークフローに依存します。",
          "主要モデルとUI（Runtime）",
          "高品質生成の標準モデル。ライセンス確認の上、低コスト構成の軸に。",
          "[71]",
          "ノードベースUI。Windows/Linux/macOS対応を明示し、ワークフローの再利用性が高い。",
          "[72]",
          "豊富なプラグインエコシステムを持つ定番UI。"
        ]
      },
      "citations": [
        {
          "mark": "※36",
          "page": 36,
          "chunk_id": "s036_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "画像生成（完全版表）",
      "slots": {
        "TITLE": "画像生成（完全版表）",
        "BULLETS": [
          "主要モデル・UI詳細比較：SDXL / ComfyUI / SD WebUI / FLUX",
          "採用判断の要点：VRAM容量が最大の制約となります。解像度、バッチサイズ、ステップ数を固定したプロファイルを作成し、VRAM不足を防ぐ運用設計が重要です。特にFLUX等の最新・上位モデルを採用する場合は、ライセンス条件（商用利用制限など）を事前に入念に確認してください。",
          "Page 37 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [71-76]"
        ]
      },
      "citations": [
        {
          "mark": "※37",
          "page": 37,
          "chunk_id": "s037_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "\\\\n\\\\n採用判断基準（画像生成）\\\\n",
      "slots": {
        "TITLE": "\\\\n\\\\n採用判断基準（画像生成）\\\\n",
        "BULLETS": [
          "VRAM制約と再現性の担保",
          "\\\\n 結論：VRAM制約に合わせて解像度/バッチ/ステップを固定し、ワークフローを資産化して再現性を担保します。\\\\n",
          "1",
          "VRAM制約",
          "ハードウェア上限を\nまず確認",
          "2",
          "解像度固定",
          "VRAMに収まる\n最大サイズを決定"
        ]
      },
      "citations": [
        {
          "mark": "※38",
          "page": 38,
          "chunk_id": "s038_sh001"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Agent / Tool-use 概要",
      "slots": {
        "TITLE": "Agent / Tool-use 概要",
        "BULLETS": [
          "ローカル環境でのエージェント構築と実行",
          "結論：Agentは「LLM＋ツール呼び出し（関数）＋実行環境」で構成され、Ollama/LM StudioのOpenAI互換APIを用いることで、ローカルでの実務的な構築が現実化しています。",
          "構成要素とAPI基盤",
          "エージェントの基本構成。LLMが判断し、定義されたツール（関数）を呼び出し、ローカル環境で実行して結果を返すループ構造。",
          "OllamaやLM Studioは、ローカルで動作しながらOpenAI互換のエンドポイントを提供。これにより、既存のAgentフレームワークやクライアントツールをそのまま利用可能。",
          "[23][24][41]",
          "OpenHands等の自律エージェントツールでも、ローカルLLMを利用するガイドが整備されています。",
          "[2]"
        ]
      },
      "citations": [
        {
          "mark": "※39",
          "page": 39,
          "chunk_id": "s039_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Agent/Tool-use（完全版表）",
      "slots": {
        "TITLE": "Agent/Tool-use（完全版表）",
        "BULLETS": [
          "OpenAI互換API / Function Calling / 実装基盤の詳細比較",
          "実装のポイント：安定運用の鍵は「JSONスキーマの固定」と「パラメータ（温度/top_p）の固定」です。幻覚（Hallucination）による不正な関数呼び出しを防ぐため、実行前に許可された関数リストと照合するホワイトリスト方式を推奨します。また、トラブルシューティング用にAPIの要求・応答ログを必ず保存してください。",
          "Page 40 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [2,3,19,23,24,27,41]"
        ]
      },
      "citations": [
        {
          "mark": "※40",
          "page": 40,
          "chunk_id": "s040_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "採用判断基準（Agent / Tool-use）",
      "slots": {
        "TITLE": "採用判断基準（Agent / Tool-use）",
        "BULLETS": [
          "JSONスキーマ固定と検証ループによる安全運用",
          "結論：JSONスキーマ固定＋検証ループ（自動チェック）による安全運用を基本とし、Ollama/LM StudioのOpenAI互換APIで実装を統一します。",
          "スキーマと検証",
          "JSONスキーマの厳格化 必須引数、型定義、Enum制約を厳密に記述し、モデルの構造化出力能力を最大限に活用。破壊時は即時再生成へ。",
          "検証ループの実装 「コード生成→コンパイル/テスト実行」自体をツールとして組み込み、LLM自身に結果をフィードバックするループを構築。",
          "実装基盤の統一",
          "OpenAI互換APIで統一 Ollama/LM Studio等の互換APIを採用し、クライアントコードを標準化。移行性・保守性を確保。",
          "[24,41]"
        ]
      },
      "citations": [
        {
          "mark": "※41",
          "page": 41,
          "chunk_id": "s041_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "音声周辺（VAD/話者分離/ウェイクワード/ノイズ除去）概要",
      "slots": {
        "TITLE": "音声周辺（VAD/話者分離/ウェイクワード/ノイズ除去）概要",
        "BULLETS": [
          "カテゴリ別モデルカタログ",
          "結論：前処理（VAD/話者分離/ウェイクワード/ノイズ除去）が音声UXの品質下限を決定します。\nSilero VADによる無音除去とpyannote.audioによる話者分離が実務上の標準構成です。",
          "主要コンポーネント（Primary）",
          "VAD（Voice Activity Detection）：Silero VAD",
          "[82]",
          "無音・雑音区間を事前除去し、ASRの幻覚（Hallucination）を抑制。軽量・高精度でデファクトスタンダード。",
          "話者分離（Diarization）：pyannote.audio",
          "[84]"
        ]
      },
      "citations": [
        {
          "mark": "※42",
          "page": 42,
          "chunk_id": "s042_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "音声周辺（完全版表）",
      "slots": {
        "TITLE": "音声周辺（完全版表）",
        "BULLETS": [
          "Silero VAD / pyannote / openWakeWord / RNNoise 詳細比較",
          "設計の推奨：会議系ワークフローでは「VAD → ASR → 要約」の直列処理を基本とし、負荷の高い話者分離（pyannote等）は必要時のみ追加する設計が実用的です。すべての音声に一律で話者分離を適用すると、処理時間（RTF）が大幅に悪化する可能性があります。",
          "Page 43 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [14, 82, 84, 85]"
        ]
      },
      "citations": [
        {
          "mark": "※43",
          "page": 43,
          "chunk_id": "s043_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "採用判断基準（音声周辺）",
      "slots": {
        "TITLE": "採用判断基準（音声周辺）",
        "BULLETS": [
          "前処理パイプライン標準化と品質管理",
          "結論：前処理パイプラインを標準化し、閾値・モデルバージョン・RTF目標を固定することで、UXの品質下限を担保します。",
          "パイプラインの標準化",
          "基準設定のプリセット化\n VAD閾値、話者分離の有無、ノイズ除去の有無をユースケースごとに固定セットとして定義。",
          "再現性の担保\n モデルバージョンとパラメータをコードで固定し、環境による挙動差を排除。",
          "ログ管理・監査",
          "パフォーマンス監視\n RTF（実時間係数）とエラー率をログに保存し、処理遅延や異常を検知。",
          "長時間処理のオフロード\n 長時間の音声処理は夜間バッチ化し、日中の計算資源をASRや要約などの対話的タスクに集中。"
        ]
      },
      "citations": [
        {
          "mark": "※44",
          "page": 44,
          "chunk_id": "s044_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "07",
      "slots": {
        "TITLE": "07",
        "BULLETS": [
          "メモリ設計の\nコア",
          "重み量子化・KVキャッシュ管理と\n最適化技術",
          "KEY TAKEAWAYS",
          "本章の要点",
          "重みメモリの現実解：\n 4bit量子化（weight-only）が基本。理論値＋10%オーバーヘッドで設計します。",
          "KVキャッシュの支配性：\n 長文コンテキストではKVがメモリを圧迫。vLLMのpaged/quantized KV[5,6]やTensorRT-LLMのKV reuse[7]が必須です。",
          "最適化技術の活用：\n llama.cppのprompt cache[88,90]等でTTFTを短縮し、実用性を高めます。",
          "重み理論値計算"
        ]
      },
      "citations": [
        {
          "mark": "※45",
          "page": 45,
          "chunk_id": "s045_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "メモリ設計のコア（概要）",
      "slots": {
        "TITLE": "メモリ設計のコア（概要）",
        "BULLETS": [
          "重み量子化とKVキャッシュの支配性",
          "結論：ローカルAIのメモリ制約は、「重み4bit量子化」と「KVキャッシュ最適化」の2点によって決定される支配的な要因です。",
          "重みメモリ（Weights）",
          "パラメータ数とbit幅で物理的な下限が決まります。",
          "メモリ ≈ パラメータ数 × bit幅 ÷ 8 （+約10% ランタイム/CUDA オーバーヘッド）",
          "FP16（16bit）と比較して約1/4のサイズで、品質劣化を最小限に抑えつつコンシューマGPUに載せるための必須技術です。",
          "GGUF Q4_K_M",
          "AWQ 4bit"
        ]
      },
      "citations": [
        {
          "mark": "※46",
          "page": 46,
          "chunk_id": "s046_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "重みメモリ理論値（完全版表1：0.5B〜14B）",
      "slots": {
        "TITLE": "重みメモリ理論値（完全版表1：0.5B〜14B）",
        "BULLETS": [
          "モデル規模別 推奨メモリ容量（+10%オーバーヘッド込み概算）",
          "計算根拠：上記数値は「パラメータ数 × bit幅 ÷ 8」に、ランタイムオーバヘッドとして約10%を加算した概算理論値です。これらは重みのみのメモリ消費であり、実運用ではこれに加えてKVキャッシュ（コンテキスト長に比例）が必要となります。特に7B〜14Bモデルは、8GB/16GBメモリ環境での動作可否の境界線となるため、4bit量子化の活用が実務上必須となります。",
          "Page 47 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report (Calculated Estimates)"
        ]
      },
      "citations": [
        {
          "mark": "※47",
          "page": 47,
          "chunk_id": "s047_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "重みメモリ理論値（完全版表2：27B〜70B）",
      "slots": {
        "TITLE": "重みメモリ理論値（完全版表2：27B〜70B）",
        "BULLETS": [
          "大規模モデル（4bit/INT8/FP16）のメモリ要件",
          "計算前提： パラメータ数 × bit幅 ÷ 8 で基本容量を算出後、実運用におけるランタイムオーバヘッド等を考慮して約+10%を加算した概算値です。",
          "設計含意： 30B級モデルの実運用には、4bit量子化でも約16-18GiBのVRAM/統合メモリが必要です。70B級では4bitでも約36GiBを消費するため、Apple Silicon 64GB/96GBや、VRAM 24GB×2枚構成などの上位ハードウェア構成が現実的なラインとなります。",
          "Page 48 | ローカルAI 技術調査レポート 2026",
          "Source: Technical Survey Report (Calculated Values based on Weight Quantization)"
        ]
      },
      "citations": [
        {
          "mark": "※48",
          "page": 48,
          "chunk_id": "s048_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "KVキャッシュの支配性（完全版表1）",
      "slots": {
        "TITLE": "KVキャッシュの支配性（完全版表1）",
        "BULLETS": [
          "2k〜32k tokensにおけるメモリ消費量比較（Llama3 8B相当）",
          "設計含意：KVキャッシュはコンテキスト長に比例して線形増加します。32kトークンなどの長文コンテキストでは、FP16のままでは4GBものVRAMを消費し、モデル重み（8B 4bitで約4.5GB）と合わせると8GB VRAMの限界を超えます。FP8/INT4量子化やPaged KV Cacheの導入が、長文運用成立の鍵となります。",
          "[5-7]",
          "※計算前提: Llama3 8B (n_layer=32, n_head_kv=8, head_dim=128), GQA有効",
          "Page 49 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [5-7]"
        ]
      },
      "citations": [
        {
          "mark": "※49",
          "page": 49,
          "chunk_id": "s049_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "KVキャッシュの支配性（完全版表2：超長文域）",
      "slots": {
        "TITLE": "KVキャッシュの支配性（完全版表2：超長文域）",
        "BULLETS": [
          "コンテキスト長131k tokensにおけるメモリ消費量比較（Llama3 8B相当）",
          "メモリ内訳の逆転現象",
          "超長文（128k等）では、KVキャッシュのメモリ消費量がモデル本体（重み）のメモリ消費量を上回る現象が発生します。\n 例：Llama3 8B（4bit重み≒4.1GiB）に対し、131k tokensのKV（FP16）は16.00GiBに達し、総メモリの約80%をKVが占有します。",
          "設計による回避策（Design Implications）",
          "要約・圧縮：Rolling Summary等でコンテキストを常に一定長以下に保つ。",
          "分割処理：長文を一括入力せず、チャンク分割して処理（Map-Reduce等）。",
          "RAG活用：全文をコンテキストに入れず、Embedding検索で必要箇所のみ抽出。",
          "KV量子化：vLLM等のpaged/quantized KV機能を積極的に利用[5,6]。"
        ]
      },
      "citations": [
        {
          "mark": "※50",
          "page": 50,
          "chunk_id": "s050_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "メモリ最適化技術",
      "slots": {
        "TITLE": "メモリ最適化技術",
        "BULLETS": [
          "paged KV／quantized KV／KV reuse／prompt cache",
          "KV構造と管理の最適化",
          "Paged KV Cache (vLLM)",
          "[5]",
          "メモリを固定サイズのページ単位で管理し、断片化を抑制する技術。OSの仮想メモリと同様の仕組みで、GPUメモリの利用効率を劇的に向上させ、スループットを高めます。",
          "Quantized KV Cache",
          "[6]",
          "KVキャッシュを標準のFP16からFP8やINT4へ量子化。精度劣化を最小限に抑えつつメモリフットプリントを50〜75%削減し、より長いコンテキストや大きなバッチサイズでの推論を可能にします。"
        ]
      },
      "citations": [
        {
          "mark": "※51",
          "page": 51,
          "chunk_id": "s051_sh004"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Tier定義と現実ライン（概要）",
      "slots": {
        "TITLE": "Tier定義と現実ライン（概要）",
        "BULLETS": [
          "ハードウェア別の実用ライン定義",
          "結論：Tierは「重み（4bit）＋KV（4k〜8k）＋オーバーヘッド」を前提に定義。\n理論計算値をベースとしつつ、最終判断は実機再現ベンチで行う設計です。",
          "Apple Silicon (UMA)",
          "16GB (A-16) 3B 〜 7/8B (4bit)",
          "24-32GB (A-24/32) 7B 〜 14B (4bit)",
          "64GB (A-64) 14B 〜 32B (4bit)",
          "96-128GB (A-96+) 32B 〜 70B (4bit)",
          "Windows GPU (VRAM)"
        ]
      },
      "citations": [
        {
          "mark": "※52",
          "page": 52,
          "chunk_id": "s052_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Apple Silicon Tier（完全版表1）",
      "slots": {
        "TITLE": "Apple Silicon Tier（完全版表1）",
        "BULLETS": [
          "実用ライン定義：A-16 / A-24/32 (Unified Memory Architecture)",
          "UMA (Unified Memory Architecture) の特性： CPUとGPUが同一のメモリプールを共有するため、データ転送のオーバーヘッドが極小化されます。Metal Performance Shaders (MPS) バックエンドを使用するMLXやPyTorchの実装では、この統合メモリを効率的に利用できますが、画面表示やOS自身のメモリ消費（数GB）を差し引いた残量が実効VRAMとなる点に注意が必要です。",
          "[1,132]",
          "Page 53 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [1,10,11,92,93,132]"
        ]
      },
      "citations": [
        {
          "mark": "※53",
          "page": 53,
          "chunk_id": "s053_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Apple Silicon Tier（完全版表2）",
      "slots": {
        "TITLE": "Apple Silicon Tier（完全版表2）",
        "BULLETS": [
          "A-64 / A-96/128：高品質ローカルの到達点と運用指針",
          "到達点の意味： A-64以上は「妥協のないローカルAI」を実現する領域です。特にUnified Memory Architecture (UMA) の恩恵により、同等VRAMを持つディスクリートGPU構成よりも低コストかつ省電力に大規模モデルを扱えます。ただし、推論速度（スループット）は専用GPUに劣る場合があるため、レイテンシ要件が厳しい場合は量子化レベルの調整やプロンプトキャッシュの活用が重要になります。",
          "Page 54 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [1, 92, 93, 94]"
        ]
      },
      "citations": [
        {
          "mark": "※54",
          "page": 54,
          "chunk_id": "s054_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Windows Tier（完全版表1）",
      "slots": {
        "TITLE": "Windows Tier（完全版表1）",
        "BULLETS": [
          "W-CPU1 / W-CPU2 / G-8 / G-12 の現実ライン",
          "最適化のポイント：WindowsのCPU推論はIntel拡張（IPEX）やOpenVINO等の最適化が効く場合があります。GPU（G-8/12）では、VRAM不足時にシステムRAMへ溢れると劇的に遅くなるため、タスクマネージャー等でVRAM使用率を厳密に監視してください。",
          "[13,135,136]",
          "Page 55 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report [13, 135, 136]"
        ]
      },
      "citations": [
        {
          "mark": "※55",
          "page": 55,
          "chunk_id": "s055_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "Windows Tier（完全版表2）",
      "slots": {
        "TITLE": "Windows Tier（完全版表2）",
        "BULLETS": [
          "高品質・実務用：G-16 (16GB) / G-24 (24GB) 定義",
          "KVキャッシュに関する重要な注意点：\n VRAMに余裕があっても、長文コンテキスト（32k〜128k）を扱うとKVキャッシュがVRAMを大量に消費し、OOM（Out Of Memory）の原因となります。\n G-16/G-24であっても、長文を扱う際は「KV量子化（FP8/INT4）」や「コンテキスト設計（要約・分割）」によるメモリ管理が必須です。",
          "Page 56 | ローカルAI 技術調査レポート 2026",
          "Based on Weight Memory Theory + Reproduction Benchmarks"
        ]
      },
      "citations": [
        {
          "mark": "※56",
          "page": 56,
          "chunk_id": "s056_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "補足：AMD/Intel GPUの現実ライン",
      "slots": {
        "TITLE": "補足：AMD/Intel GPUの現実ライン",
        "BULLETS": [
          "公式最適化とエコシステムの現状",
          "結論：AMD/Intel環境でも公式最適化による選択肢が存在しますが、運用は実装・ドライバ依存となり、個別検証が必要です。",
          "AMD Radeon GPU (ROCm)",
          "ROCm",
          "HIP SDK",
          "ZLUDA (Deprecated)",
          "ROCmのWindowsサポートが進展しており、一部のWSL2環境やネイティブWindowsでの動作が可能になりつつあります。",
          "[97]"
        ]
      },
      "citations": [
        {
          "mark": "※57",
          "page": 57,
          "chunk_id": "s057_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "常駐運用 vs オンデマンド運用",
      "slots": {
        "TITLE": "常駐運用 vs オンデマンド運用",
        "BULLETS": [
          "TTFT（初速）、安定性、リソース効率のトレードオフ",
          "基本運用モデルの比較",
          "常駐（Always-on）",
          "推奨: チャットBot",
          "モデルをメモリに保持し、Prompt Cacheを維持。",
          "利点：TTFTが最短、応答が安定。",
          "欠点：メモリを常時占有（他アプリと競合）。",
          "オンデマンド（On-demand）"
        ]
      },
      "citations": [
        {
          "mark": "※58",
          "page": 58,
          "chunk_id": "s058_sh004"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ユースケース別推奨スタック：音声メモ→整文化→タスク抽出",
      "slots": {
        "TITLE": "ユースケース別推奨スタック：音声メモ→整文化→タスク抽出",
        "BULLETS": [
          "用途に応じて「低コスト／高品質／ハード制約」の3構成を選択",
          "結論：ASR精度とLLM推論能力のバランスで構成を決定します。 ※誤転記防止にはVAD（無音除去）が必須。タスク抽出にはJSONスキーマ固定が有効です。",
          "低コスト構成",
          "Target HW",
          "A-16 / W-CPU1 / G-8想定 (Apple 16GB / CPU / VRAM 8GB)",
          "VAD & ASR",
          "Silero VAD (無音除去) faster-whisper (CPU/INT8)",
          "LLM 整形・抽出"
        ]
      },
      "citations": [
        {
          "mark": "※59",
          "page": 59,
          "chunk_id": "s059_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "音声メモ→整文化→タスク抽出（失敗モードと回避策）",
      "slots": {
        "TITLE": "音声メモ→整文化→タスク抽出（失敗モードと回避策）",
        "BULLETS": [
          "ユースケース別推奨スタック",
          "結論：誤転記・要点漏れ・推論不足は「設計」で抑え込む。\n VADによる区間分割、逐次要約、テンプレ固定が鍵となります。",
          "ASR誤転記→抽出ミス",
          "失敗モード",
          "ASRが固有名詞や数値を誤認識し、そのままタスクとして抽出されてしまう。",
          "回避策（Design）",
          "1. VADで区間分割：無音除去で認識精度向上。\n 2. 重要箇所再確認UI：抽出されたタスクの元音声をワンクリック再生できるUIを提供。",
          "Silero VAD"
        ]
      },
      "citations": [
        {
          "mark": "※60",
          "page": 60,
          "chunk_id": "s060_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ユースケース：会議議事録（話者分離＋要約＋アクション）",
      "slots": {
        "TITLE": "ユースケース：会議議事録（話者分離＋要約＋アクション）",
        "BULLETS": [
          "推奨スタック3構成（Tier別）",
          "結論：規模と要件に応じて3構成を選択。無音除去（VAD）と発話ターン統合ルールが品質下限を決定します。\n※評価はp50/p95、RTF、意味改変率で監視することを推奨します。",
          "低コスト構成",
          "前処理 (VAD)",
          "Silero VAD [82] 無音区間を確実に除去",
          "[82]",
          "ASR (音声認識)",
          "faster-whisper [50] (CPU / INT8量子化)"
        ]
      },
      "citations": [
        {
          "mark": "※61",
          "page": 61,
          "chunk_id": "s061_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "会議議事録：失敗モードと回避策",
      "slots": {
        "TITLE": "会議議事録：失敗モードと回避策",
        "BULLETS": [
          "ユースケース詳細分析",
          "結論：話者誤割当・ASR幻覚・長時間処理の負荷を設計で抑止。\nVAD（無音除去）の厳格化とタイムスタンプベースの検証が品質の防波堤となります。",
          "主な失敗モード（Failure Modes）",
          "話者誤割当（Diarization Error）",
          "高頻度",
          "発話の切れ目が不明確で、Aさんの発言がBさんとして記録される。特に割り込み発話で多発。",
          "ASR幻覚（Hallucination）",
          "致命的"
        ]
      },
      "citations": [
        {
          "mark": "※62",
          "page": 62,
          "chunk_id": "s062_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ユースケース：文書RAG",
      "slots": {
        "TITLE": "ユースケース：文書RAG",
        "BULLETS": [
          "PDF/スキャン → OCR → 検索 → 回答",
          "結論：RAG品質はEmbeddingで上限が決まるため、「Embeddingでrecall確保→Rerankerでprecision向上」の二段構えが基本戦略です。",
          "低コスト構成",
          "OCR / Pre-process",
          "Tesseract OCR ※スキャン品質が高い場合",
          "Embedding / Reranker",
          "BGE-M3 (多用途) bge-reranker-base (軽量)",
          "LLM Runtime"
        ]
      },
      "citations": [
        {
          "mark": "※63",
          "page": 63,
          "chunk_id": "s063_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "文書RAG：失敗モードと回避策",
      "slots": {
        "TITLE": "文書RAG：失敗モードと回避策",
        "BULLETS": [
          "OCR誤読・検索失敗・幻覚への対策設計",
          "結論：文書RAGの失敗は「OCR誤読」「検索精度不足」「幻覚引用」に大別され、 \n前処理の標準化と原文スニペット引用のUX要件化で回避します。",
          "主な失敗モード（Failure Modes）",
          "スキャン品質低下や傾きにより、固有名詞や数値が誤認識され、正しい文書がヒットしない。",
          "Recall低下",
          "表組みや段組みが崩れてテキスト化され、文脈が断絶。LLMが意味を誤解釈する。",
          "Context喪失",
          "検索結果に含まれない情報を、さも引用したかのように回答する。"
        ]
      },
      "citations": [
        {
          "mark": "※64",
          "page": 64,
          "chunk_id": "s064_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ユースケース：画像理解（VLM）— 3構成",
      "slots": {
        "TITLE": "ユースケース：画像理解（VLM）— 3構成",
        "BULLETS": [
          "スクショ/写真の説明・抽出：推奨スタック比較",
          "結論：2B〜7B級VLMがローカルの現実ライン。72B級は上位機前提となります。\n※視覚トークン肥大による速度/メモリ急落を防ぐため、解像度・max-pixelsの制御が必須です。",
          "低コスト構成",
          "Target HW",
          "Apple 16GB / VRAM 8GB級",
          "VLM Model",
          "Qwen2-VL 2B 軽量・高速な視覚理解",
          "Runtime"
        ]
      },
      "citations": [
        {
          "mark": "※65",
          "page": 65,
          "chunk_id": "s065_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "画像理解：失敗モードと回避策",
      "slots": {
        "TITLE": "画像理解：失敗モードと回避策",
        "BULLETS": [
          "ユースケース別推奨スタック（詳細）",
          "結論：UI文字の誤読と視覚トークン肥大による速度低下を、\nOCR併用と解像度・max-pixels制御で回避します。",
          "主な失敗モード（Failure Modes）",
          "2B/7B級VLMでは、スクリーンショット内の小さなフォントや密集した情報を正確に読み取れないケースが頻発。",
          "高解像度画像をそのまま入力すると、視覚トークン数が数千に達し、推論速度（TTFT/生成）が急激に悪化。",
          "複雑な表組みやグラフの空間関係を誤認し、存在しない数値や関係性を捏造する。",
          "回避策・設計（Mitigation）",
          "文字情報はPaddleOCR等で別経路からテキストとして供給し、VLMは「状況説明」に専念させる。"
        ]
      },
      "citations": [
        {
          "mark": "※66",
          "page": 66,
          "chunk_id": "s066_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ユースケース：コーディング補助",
      "slots": {
        "TITLE": "ユースケース：コーディング補助",
        "BULLETS": [
          "推奨スタック3パターン（IDE支援・リポジトリ理解）",
          "結論：補完は小型モデル、設計レビューや長距離依存解決は中〜大型モデルで役割分担します。\n※生成コードは自動コンパイル/テスト実行を“ツール”化し、検証ループに組み込むことが重要です。",
          "低コスト構成",
          "Code LLM",
          "Qwen2.5-Coder 7B サイズ展開が豊富で軽量",
          "RAG / Embedding",
          "リポジトリEmbedding (BGE/E5) 関連ファイル抽出→LLM回答",
          "Connection"
        ]
      },
      "citations": [
        {
          "mark": "※67",
          "page": 67,
          "chunk_id": "s067_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "コーディング支援：失敗モードと回避策",
      "slots": {
        "TITLE": "コーディング支援：失敗モードと回避策",
        "BULLETS": [
          "コーディング補助（IDE支援・リポジトリ理解）",
          "結論：幻覚API生成と長文遅延を「検証ループ」と「キャッシュ活用」で抑止し、 \n コード品質とセキュリティを自動化プロセスで担保します。",
          "主な失敗モード（Failure Modes）",
          "ライブラリのバージョン不一致や、もっともらしいが実在しない関数（Hallucination）を生成。",
          "リポジトリ全体を読み込むとKVキャッシュが肥大化し、補完のレスポンスが低下する。",
          "古い構文や廃止されたメソッドを使用し、コンパイルエラーやセキュリティリスクを招く。",
          "回避策・設計（Mitigation & Design）",
          "ビルド・単体テスト実行をfunction callingとしてLLMに提供。失敗時はエラーログをフィードバックして再生成させる。"
        ]
      },
      "citations": [
        {
          "mark": "※68",
          "page": 68,
          "chunk_id": "s068_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "ユースケース：画像生成（Diffusion）— 3構成",
      "slots": {
        "TITLE": "ユースケース：画像生成（Diffusion）— 3構成",
        "BULLETS": [
          "推奨スタックとVRAM要件の最適化",
          "結論：SDXLを軸にComfyUIでワークフローを資産化。FLUX等はライセンス精査が必須です。\n※VRAM要件は解像度・バッチ・ステップ数に依存するため、プロファイル固定が重要です。",
          "低コスト構成",
          "Target VRAM",
          "VRAM 8GB〜12GB\n(Apple Silicon 16GB)",
          "Model / UI",
          "SDXL base 1.0 + Refinerなし\nComfyUI (Win/Linux/macOS)",
          "Optimization"
        ]
      },
      "citations": [
        {
          "mark": "※69",
          "page": 69,
          "chunk_id": "s069_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "画像生成：失敗モードと回避策",
      "slots": {
        "TITLE": "画像生成：失敗モードと回避策",
        "BULLETS": [
          "VRAM制約と品質・ライセンス管理",
          "結論：VRAM不足と生成結果のブレを「標準プロファイル」で抑制し、 \n ライセンス違反リスクを「事前精査と監査」で排除します。",
          "主な失敗モード（Failure Modes）",
          "高解像度や大バッチ指定時にプロセスがクラッシュ。特にSDXL/FLUX等の大型モデルで頻発。",
          "同じプロンプトでもSeedや設定の違いで出力が変動し、UIモック等の修正サイクルが回らない。",
          "FLUX.1-dev等の非商用/開発用ライセンスを、誤って商用プロダクトや社内資料に利用してしまう。",
          "回避策と設計（Mitigation Strategies）",
          "VRAM容量に応じた上限テーブルを作成。Attention SlicingやVAE Tiling等の最適化を強制適用。"
        ]
      },
      "citations": [
        {
          "mark": "※70",
          "page": 70,
          "chunk_id": "s070_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "10",
      "slots": {
        "TITLE": "10",
        "BULLETS": [
          "測定と比較の\n方法",
          "ベンチマーク設計と\n再現性の担保",
          "KEY TAKEAWAYS",
          "本章の要点",
          "指標の厳密定義：\n p50/p95レイテンシ、TTFT、tok/s、RTF等を定義し、JSON破壊率や意味改変率も定量化します。",
          "測定手順の標準化：\n 条件固定（温度・量子化等）、ウォームアップ分離、KV影響の切り分け（短文/長文）を徹底します。",
          "再現性のコア要素：\n ASR/TTSの前処理固定と、最小テストセット（短文/長文/構造化/音声）による比較を実施します。",
          "指標定義"
        ]
      },
      "citations": [
        {
          "mark": "※71",
          "page": 71,
          "chunk_id": "s071_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "測定指標の定義",
      "slots": {
        "TITLE": "測定指標の定義",
        "BULLETS": [
          "p50/p95・TTFT・tok/s・RTF・品質指標",
          "結論：LLMの「初速（TTFT）」と「生成速度（tok/s）」を分離して計測し、\n 品質（JSON破壊率等）とリソース消費（RAM/VRAM）を定量化します。",
          "速度・リソース指標（Latency & Resource）",
          "TTFT（Time To First Token）と生成完了時間を分離して計測。KV reuseやprompt cacheの効果はTTFTに現れるため分離が必須。",
          "生成トークン数 ÷ 生成時間。ユーザーの体感速度やバッチ処理能力の指標。",
          "ASR/TTS用指標。処理時間 ÷ 音声長。",
          "RTF < 1.0 なら実時間より高速",
          "OS監視またはプロセス単位（nvidia-smi等）で計測。Apple Silicon等の統合メモリ環境ではシステム全体の圧迫度も注視。"
        ]
      },
      "citations": [
        {
          "mark": "※72",
          "page": 72,
          "chunk_id": "s072_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "測定手順（条件固定・ウォームアップ・KV切り分け）",
      "slots": {
        "TITLE": "測定手順（条件固定・ウォームアップ・KV切り分け）",
        "BULLETS": [
          "再現可能なベンチマーク設計の要件",
          "再現条件の固定",
          "生成パラメータの統一",
          "同一プロンプト、最大トークン数、温度（temperature）、top_p を固定し、ランダム性を排除または制御します。",
          "モデル環境の固定",
          "同一の量子化形式（GGUF Q4_K_Mなど）、コンテキスト長設定を使用します。",
          "音声・画像系の前提",
          "ASR/TTSは同一音声・同一前処理（VAD有無）、同一話者設定で品質と速度を分離測定します。"
        ]
      },
      "citations": [
        {
          "mark": "※73",
          "page": 73,
          "chunk_id": "s073_sh004"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "リスクとコンプライアンス",
      "slots": {
        "TITLE": "リスクとコンプライアンス",
        "BULLETS": [
          "ライセンス管理・プライバシー・安全性対策",
          "結論：「コードのOSSライセンス」と「モデル重みの利用条件」を分離管理し、ローカル完結の利点を活かしつつ、検証ループによる安全性確保が必須です。",
          "ライセンス管理",
          "コードと重みの分離\n ランタイム(MIT/Apache)とモデル重み(Community/非商用)は別条件。利用範囲を台帳化する。",
          "個別確認の義務化\n Qwen2.5のファミリー内例外や、FLUX.1-dev等の派生モデル条件を一次ソースで確認する。",
          "[33]",
          "プライバシー・安全",
          "ローカル完結の徹底\n 入力データがデフォルトで外部送信されない構成を物理/論理的に担保。LAN内APIも管理対象。"
        ]
      },
      "citations": [
        {
          "mark": "※74",
          "page": 74,
          "chunk_id": "s074_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "まとめ（意思決定フロー）",
      "slots": {
        "TITLE": "まとめ（意思決定フロー）",
        "BULLETS": [
          "ローカルAI導入の5ステップと判断基準",
          "1",
          "要件定義\nRequirements",
          "Requirements",
          "業務課題から技術要件へ変換する始点。",
          "判断基準 (Criteria)",
          "品質: 日本語、JSON、専門性",
          "速度: p95レイテンシ、tok/s"
        ]
      },
      "citations": [
        {
          "mark": "※75",
          "page": 75,
          "chunk_id": "s075_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "総括（実務への適用）",
      "slots": {
        "TITLE": "総括（実務への適用）",
        "BULLETS": [
          "実装の原則と次のステップ",
          "結論：ローカルAIは「4bit量子化＋KV最適化」を前提に、 \n 失敗モードを設計で抑え込むことで実務運用が可能です。",
          "実装の原則（Principles）",
          "重みは4bit（GGUF/AWQ/GPTQ）でメモリ理論値を計算し、Tierに合わせる。",
          "Cost-Efficiency",
          "長文・多同時接続時は、paged KV、KV量子化、prompt cacheを活用してメモリ爆発を防ぐ。",
          "Scalability",
          "幻覚・誤転記・JSON破壊は「起きるもの」とし、UI確認・検証ループ・再生成でカバーする。"
        ]
      },
      "citations": [
        {
          "mark": "※76",
          "page": 76,
          "chunk_id": "s076_sh006"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "References（1/4）",
      "slots": {
        "TITLE": "References（1/4）",
        "BULLETS": [
          "Technical Research Report Sources [1-34]",
          "[1]",
          "https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/",
          "[2]",
          "https://docs.openhands.dev/openhands/usage/llms/local-llms",
          "[3]",
          "https://lmstudio.ai/docs/developer/core/server",
          "[4]"
        ]
      },
      "citations": [
        {
          "mark": "※77",
          "page": 77,
          "chunk_id": "s077_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "References（参考文献リスト 2/4）",
      "slots": {
        "TITLE": "References（参考文献リスト 2/4）",
        "BULLETS": [
          "[35]～[68] Llama / Qwen / Gemma / Phi / VLM / Whisper / TTS / OCR",
          "Page 78 | ローカルAI 技術調査レポート 2026",
          "Reference List [35-68]"
        ]
      },
      "citations": [
        {
          "mark": "※78",
          "page": 78,
          "chunk_id": "s078_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "References（3/4）",
      "slots": {
        "TITLE": "References（3/4）",
        "BULLETS": [
          "参考文献リスト [69] - [102]",
          "Page 79 | ローカルAI 技術調査レポート 2026",
          "Data Source: Technical Survey Report"
        ]
      },
      "citations": [
        {
          "mark": "※79",
          "page": 79,
          "chunk_id": "s079_sh005"
        }
      ]
    },
    {
      "slide_no": null,
      "component_id": "comp_title_bullets",
      "message": "References（参考文献リスト4/4）",
      "slots": {
        "TITLE": "References（参考文献リスト4/4）",
        "BULLETS": [
          "音声周辺・ベンチマーク・最適化技術・ハードウェア関連 [103] - [136]",
          "完",
          "Page 80 | ローカルAI 技術調査レポート 2026",
          "Technical Survey Completed."
        ]
      },
      "citations": [
        {
          "mark": "※80",
          "page": 80,
          "chunk_id": "s080_sh005"
        }
      ]
    }
  ],
  "toc": [
    {
      "title": "2026年2月15日 | Generated by Genspark AI Slides",
      "level": 1,
      "slide_index": 1
    },
    {
      "title": "このスライドの読み方",
      "level": 1,
      "slide_index": 2
    },
    {
      "title": "Local AI Technical Report",
      "level": 1,
      "slide_index": 3
    },
    {
      "title": "エグゼクティブサマリ（実務要点）",
      "level": 1,
      "slide_index": 4
    },
    {
      "title": "エグゼクティブサマリ（詳細1）",
      "level": 1,
      "slide_index": 5
    },
    {
      "title": "エグゼクティブサマリ（詳細2）",
      "level": 1,
      "slide_index": 6
    },
    {
      "title": "エグゼクティブサマリ（詳細3）",
      "level": 1,
      "slide_index": 7
    },
    {
      "title": "ローカルAIの定義とスコープ",
      "level": 1,
      "slide_index": 8
    },
    {
      "title": "未指定事項の扱い（前提条件）",
      "level": 1,
      "slide_index": 9
    },
    {
      "title": "04",
      "level": 1,
      "slide_index": 10
    },
    {
      "title": "ローカルAIランドスケープ",
      "level": 1,
      "slide_index": 11
    },
    {
      "title": "ローカルAIランドスケープ（詳細）",
      "level": 1,
      "slide_index": 12
    },
    {
      "title": "05",
      "level": 1,
      "slide_index": 13
    },
    {
      "title": "Evaluation Metrics Overview",
      "level": 1,
      "slide_index": 14
    },
    {
      "title": "特徴軸（詳細）：測定と設計含意",
      "level": 1,
      "slide_index": 15
    },
    {
      "title": "06",
      "level": 1,
      "slide_index": 16
    },
    {
      "title": "テキストLLM（汎用）概要",
      "level": 1,
      "slide_index": 17
    },
    {
      "title": "テキストLLM（完全版表1）",
      "level": 1,
      "slide_index": 18
    },
    {
      "title": "テキストLLM（完全版表2）",
      "level": 1,
      "slide_index": 19
    },
    {
      "title": "採用判断基準（テキストLLM）",
      "level": 1,
      "slide_index": 20
    },
    {
      "title": "VLM/LMM 概要（画像理解・マルチモーダル）",
      "level": 1,
      "slide_index": 21
    },
    {
      "title": "VLM/LMM（完全版表）",
      "level": 1,
      "slide_index": 22
    },
    {
      "title": "採用判断基準（VLM/LMM）",
      "level": 1,
      "slide_index": 23
    },
    {
      "title": "ASR（音声認識）概要",
      "level": 1,
      "slide_index": 24
    },
    {
      "title": "ASR（完全版表）",
      "level": 1,
      "slide_index": 25
    },
    {
      "title": "採用判断基準（ASR）",
      "level": 1,
      "slide_index": 26
    },
    {
      "title": "TTS（音声合成）概要",
      "level": 1,
      "slide_index": 27
    },
    {
      "title": "TTS（完全版表）",
      "level": 1,
      "slide_index": 28
    },
    {
      "title": "採用判断基準（TTS）",
      "level": 1,
      "slide_index": 29
    },
    {
      "title": "Embedding / Reranker 概要",
      "level": 1,
      "slide_index": 30
    },
    {
      "title": "Embedding/Reranker（完全版表）",
      "level": 1,
      "slide_index": 31
    },
    {
      "title": "採用判断基準（Embedding / Reranker）",
      "level": 1,
      "slide_index": 32
    },
    {
      "title": "OCR / Document AI 概要",
      "level": 1,
      "slide_index": 33
    },
    {
      "title": "OCR / Document AI（完全版表）",
      "level": 1,
      "slide_index": 34
    },
    {
      "title": "OCR/Document AI採用判断基準",
      "level": 1,
      "slide_index": 35
    },
    {
      "title": "画像生成（Diffusion）概要",
      "level": 1,
      "slide_index": 36
    },
    {
      "title": "画像生成（完全版表）",
      "level": 1,
      "slide_index": 37
    },
    {
      "title": "\\\\n\\\\n採用判断基準（画像生成）\\\\n",
      "level": 1,
      "slide_index": 38
    },
    {
      "title": "Agent / Tool-use 概要",
      "level": 1,
      "slide_index": 39
    },
    {
      "title": "Agent/Tool-use（完全版表）",
      "level": 1,
      "slide_index": 40
    },
    {
      "title": "採用判断基準（Agent / Tool-use）",
      "level": 1,
      "slide_index": 41
    },
    {
      "title": "音声周辺（VAD/話者分離/ウェイクワード/ノイズ除去）概要",
      "level": 1,
      "slide_index": 42
    },
    {
      "title": "音声周辺（完全版表）",
      "level": 1,
      "slide_index": 43
    },
    {
      "title": "採用判断基準（音声周辺）",
      "level": 1,
      "slide_index": 44
    },
    {
      "title": "07",
      "level": 1,
      "slide_index": 45
    },
    {
      "title": "メモリ設計のコア（概要）",
      "level": 1,
      "slide_index": 46
    },
    {
      "title": "重みメモリ理論値（完全版表1：0.5B〜14B）",
      "level": 1,
      "slide_index": 47
    },
    {
      "title": "重みメモリ理論値（完全版表2：27B〜70B）",
      "level": 1,
      "slide_index": 48
    },
    {
      "title": "KVキャッシュの支配性（完全版表1）",
      "level": 1,
      "slide_index": 49
    },
    {
      "title": "KVキャッシュの支配性（完全版表2：超長文域）",
      "level": 1,
      "slide_index": 50
    },
    {
      "title": "メモリ最適化技術",
      "level": 1,
      "slide_index": 51
    },
    {
      "title": "Tier定義と現実ライン（概要）",
      "level": 1,
      "slide_index": 52
    },
    {
      "title": "Apple Silicon Tier（完全版表1）",
      "level": 1,
      "slide_index": 53
    },
    {
      "title": "Apple Silicon Tier（完全版表2）",
      "level": 1,
      "slide_index": 54
    },
    {
      "title": "Windows Tier（完全版表1）",
      "level": 1,
      "slide_index": 55
    },
    {
      "title": "Windows Tier（完全版表2）",
      "level": 1,
      "slide_index": 56
    },
    {
      "title": "補足：AMD/Intel GPUの現実ライン",
      "level": 1,
      "slide_index": 57
    },
    {
      "title": "常駐運用 vs オンデマンド運用",
      "level": 1,
      "slide_index": 58
    },
    {
      "title": "ユースケース別推奨スタック：音声メモ→整文化→タスク抽出",
      "level": 1,
      "slide_index": 59
    },
    {
      "title": "音声メモ→整文化→タスク抽出（失敗モードと回避策）",
      "level": 1,
      "slide_index": 60
    },
    {
      "title": "ユースケース：会議議事録（話者分離＋要約＋アクション）",
      "level": 1,
      "slide_index": 61
    },
    {
      "title": "会議議事録：失敗モードと回避策",
      "level": 1,
      "slide_index": 62
    },
    {
      "title": "ユースケース：文書RAG",
      "level": 1,
      "slide_index": 63
    },
    {
      "title": "文書RAG：失敗モードと回避策",
      "level": 1,
      "slide_index": 64
    },
    {
      "title": "ユースケース：画像理解（VLM）— 3構成",
      "level": 1,
      "slide_index": 65
    },
    {
      "title": "画像理解：失敗モードと回避策",
      "level": 1,
      "slide_index": 66
    },
    {
      "title": "ユースケース：コーディング補助",
      "level": 1,
      "slide_index": 67
    },
    {
      "title": "コーディング支援：失敗モードと回避策",
      "level": 1,
      "slide_index": 68
    },
    {
      "title": "ユースケース：画像生成（Diffusion）— 3構成",
      "level": 1,
      "slide_index": 69
    },
    {
      "title": "画像生成：失敗モードと回避策",
      "level": 1,
      "slide_index": 70
    },
    {
      "title": "10",
      "level": 1,
      "slide_index": 71
    },
    {
      "title": "測定指標の定義",
      "level": 1,
      "slide_index": 72
    },
    {
      "title": "測定手順（条件固定・ウォームアップ・KV切り分け）",
      "level": 1,
      "slide_index": 73
    },
    {
      "title": "リスクとコンプライアンス",
      "level": 1,
      "slide_index": 74
    },
    {
      "title": "まとめ（意思決定フロー）",
      "level": 1,
      "slide_index": 75
    },
    {
      "title": "総括（実務への適用）",
      "level": 1,
      "slide_index": 76
    },
    {
      "title": "References（1/4）",
      "level": 1,
      "slide_index": 77
    },
    {
      "title": "References（参考文献リスト 2/4）",
      "level": 1,
      "slide_index": 78
    },
    {
      "title": "References（3/4）",
      "level": 1,
      "slide_index": 79
    },
    {
      "title": "References（参考文献リスト4/4）",
      "level": 1,
      "slide_index": 80
    }
  ]
}